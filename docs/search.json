[
  {
    "objectID": "posts/2025-08-10-CBOW/cbow.html",
    "href": "posts/2025-08-10-CBOW/cbow.html",
    "title": "Comprehensive deep dive and implementation of Continuous Bag-of-Words",
    "section": "",
    "text": "# For text processing\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords') # download stopwords\nfrom torchtext.data import get_tokenizer\nfrom collections import Counter\n\n# For Neural Network\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import StepLR, LambdaLR\nfrom datasets import load_dataset\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(device)\n\n# For time measurement\nimport time\n\n# For visualization\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\mgupta70.ASURITE\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\ncuda\n\n\nPaper title: Efficient Estimation of Word Representations in Vector Space\nThis paper proposed 4 methods to learn word vectors.\n\nNLLM\nRNNLM\nCBOW\nSkip-gram\n\nSome details about them as mentioned in the paper:\n\nNLLM - a feedforward NN with following layers: i/p, projection, hidden and o/p\nRNNLM - removes the need of projection layer. Thus, i/p, hidden and o/p\nCBOW - predict current word from R previous and R future words (context words)\nSkip-gram - predict context words from given current word (i.e. predict 2R words) (Multi label multi-class classification)\n\nBefore going into details of CBOW method, let’s load some data.\n\ndataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\ntrain_texts = dataset['train']['text']\nval_texts = dataset['validation']['text']\nprint(\"Train sample: \", train_texts[1] )\nprint(\"Val sample: \", val_texts[1])\nprint(\"Train: Val = \", len(train_texts), \":\", len(val_texts))\n\ntrain_texts and val_texts are basically lists of strings. Let’s join all the strings into a single large string called train_text and val_text\n\ntrain_text = \"\"\nfor text_paragraph in train_texts:\n    train_text = train_text + \" \" + text_paragraph\n\nval_text = \"\"\nfor text_paragraph in val_texts:\n    val_text = val_text + \" \" + text_paragraph\n\nprint(\"Number of words in the corpus (in Millions):\", len(train_text.split())/1000000)\nprint(\"Number of words in the validation corpus (in Millions):\", len(val_text.split())/1000000)\n\nNumber of words in the corpus (in Millions): 2.05191\nNumber of words in the validation corpus (in Millions): 0.213886\n\n\n\n# preprocess the text: tokenize, lowercase, remove punctuation, etc.\ndef tokenize_text(text, make_lower=True, remove_stopwords=False):\n    # lowercase\n    if make_lower:\n        text = text.lower()\n    # remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    # tokenize: assume that words are separated by spaces\n    tokenizer = get_tokenizer(\"basic_english\")\n    words = tokenizer(text)\n    # remove stopwords\n    if remove_stopwords:\n        stop_words = set(stopwords.words('english'))\n        words = [word for word in words if word not in stop_words]\n    return words\n\n\n# preprocess the train text\ntrain_words = tokenize_text(train_text, remove_stopwords=False)\nvocab = set(train_words)\nV = len(vocab)\nprint(\"--------------------------------\")\nprint(\"Number of tokens in the corpus (in Millions):\", len(train_words)/1000000)\nprint(\"Vocabulary size, V:\", V)\n\n# preprocess the validation text\nval_words = tokenize_text(val_text, remove_stopwords=False)\n\n--------------------------------\nNumber of tokens in the corpus (in Millions): 1.749369\nVocabulary size, V: 66102\n\n\n\n# let's see some of the train words by frequency in descending order\ntrain_word_counts = Counter(train_words)\ntrain_word_counts.most_common()\n\n[('the', 130769),\n ('of', 57032),\n ('and', 50735),\n ('in', 45016),\n ('to', 39522),\n ('a', 36453),\n ('was', 21008),\n ('on', 15141),\n ('as', 15062),\n ('s', 14488),\n ('that', 14351),\n ('for', 13794),\n ('with', 13012),\n ('by', 12718),\n ('is', 11692),\n ('it', 9276),\n ('from', 9229),\n ('at', 9071),\n ('his', 9020),\n ('he', 8708),\n ('were', 7334),\n ('an', 6251),\n ('had', 5707),\n ('which', 5546),\n ('be', 4860),\n ('are', 4714),\n ('this', 4560),\n ('their', 4290),\n ('first', 4245),\n ('but', 4233),\n ('not', 4006),\n ('one', 3914),\n ('they', 3894),\n ('its', 3878),\n ('also', 3842),\n ('after', 3749),\n ('her', 3670),\n ('or', 3657),\n ('two', 3565),\n ('have', 3470),\n ('has', 3325),\n ('been', 3263),\n ('who', 3029),\n ('she', 2884),\n ('new', 2767),\n ('other', 2729),\n ('during', 2690),\n ('when', 2655),\n ('time', 2607),\n ('all', 2557),\n ('into', 2443),\n ('more', 2402),\n ('would', 2332),\n ('1', 2254),\n ('i', 2179),\n ('over', 2137),\n ('while', 2127),\n ('game', 2077),\n ('only', 2061),\n ('most', 2027),\n ('2', 1982),\n ('three', 1976),\n ('later', 1928),\n ('up', 1919),\n ('about', 1914),\n ('may', 1878),\n ('between', 1871),\n ('him', 1844),\n ('song', 1828),\n ('there', 1801),\n ('some', 1771),\n ('than', 1765),\n ('out', 1760),\n ('no', 1702),\n ('season', 1692),\n ('year', 1669),\n ('made', 1642),\n ('city', 1609),\n ('3', 1601),\n ('such', 1584),\n ('before', 1557),\n ('where', 1524),\n ('used', 1508),\n ('series', 1498),\n ('them', 1488),\n ('second', 1469),\n ('world', 1462),\n ('being', 1454),\n ('years', 1451),\n ('both', 1448),\n ('many', 1447),\n ('000', 1447),\n ('these', 1445),\n ('film', 1405),\n ('however', 1397),\n ('album', 1383),\n ('south', 1373),\n ('war', 1370),\n ('through', 1364),\n ('5', 1362),\n ('north', 1357),\n ('then', 1329),\n ('part', 1320),\n ('can', 1320),\n ('early', 1300),\n ('several', 1287),\n ('4', 1279),\n ('number', 1278),\n ('state', 1274),\n ('including', 1273),\n ('against', 1272),\n ('well', 1262),\n ('known', 1259),\n ('became', 1255),\n ('four', 1238),\n ('united', 1216),\n ('under', 1198),\n ('although', 1194),\n ('m', 1180),\n ('century', 1178),\n ('day', 1165),\n ('following', 1152),\n ('music', 1134),\n ('us', 1119),\n ('began', 1102),\n ('because', 1090),\n ('so', 1085),\n ('work', 1083),\n ('like', 1075),\n ('end', 1071),\n ('called', 1065),\n ('episode', 1064),\n ('until', 1040),\n ('found', 1039),\n ('said', 1038),\n ('area', 1034),\n ('could', 1033),\n ('states', 1019),\n ('american', 1018),\n ('people', 1015),\n ('6', 1011),\n ('since', 995),\n ('british', 989),\n ('each', 984),\n ('released', 979),\n ('same', 979),\n ('team', 974),\n ('church', 968),\n ('around', 966),\n ('10', 963),\n ('long', 960),\n ('did', 958),\n ('along', 957),\n ('million', 940),\n ('life', 939),\n ('five', 938),\n ('national', 916),\n ('0', 914),\n ('john', 910),\n ('back', 910),\n ('high', 904),\n ('company', 900),\n ('t', 897),\n ('another', 896),\n ('best', 891),\n ('use', 888),\n ('you', 878),\n ('if', 872),\n ('final', 871),\n ('september', 869),\n ('august', 865),\n ('river', 861),\n ('large', 856),\n ('what', 848),\n ('west', 843),\n ('8', 837),\n ('km', 836),\n ('off', 835),\n ('down', 831),\n ('7', 825),\n ('due', 819),\n ('games', 818),\n ('june', 810),\n ('history', 809),\n ('line', 809),\n ('will', 803),\n ('name', 799),\n ('now', 791),\n ('any', 786),\n ('storm', 786),\n ('received', 785),\n ('home', 784),\n ('9', 777),\n ('government', 774),\n ('described', 774),\n ('six', 773),\n ('within', 770),\n ('species', 770),\n ('much', 769),\n ('group', 764),\n ('family', 758),\n ('october', 757),\n ('played', 755),\n ('east', 748),\n ('league', 747),\n ('general', 744),\n ('took', 744),\n ('set', 744),\n ('major', 737),\n ('road', 737),\n ('late', 735),\n ('july', 735),\n ('wrote', 733),\n ('single', 731),\n ('won', 731),\n ('system', 729),\n ('play', 726),\n ('video', 725),\n ('times', 724),\n ('according', 723),\n ('record', 718),\n ('third', 716),\n ('based', 713),\n ('april', 711),\n ('man', 707),\n ('included', 702),\n ('just', 700),\n ('march', 699),\n ('book', 697),\n ('january', 695),\n ('those', 694),\n ('show', 693),\n ('named', 691),\n ('very', 690),\n ('even', 690),\n ('england', 689),\n ('main', 687),\n ('white', 687),\n ('left', 686),\n ('york', 685),\n ('men', 684),\n ('small', 681),\n ('school', 681),\n ('though', 678),\n ('division', 672),\n ('club', 671),\n ('way', 670),\n ('old', 670),\n ('original', 667),\n ('near', 666),\n ('last', 665),\n ('12', 663),\n ('november', 661),\n ('water', 657),\n ('death', 655),\n ('place', 654),\n ('20', 653),\n ('15', 651),\n ('tropical', 650),\n ('december', 647),\n ('built', 646),\n ('own', 644),\n ('we', 642),\n ('character', 641),\n ('songs', 639),\n ('top', 633),\n ('de', 632),\n ('form', 631),\n ('30', 631),\n ('player', 630),\n ('do', 626),\n ('black', 625),\n ('king', 625),\n ('public', 623),\n ('island', 619),\n ('german', 619),\n ('next', 617),\n ('2009', 616),\n ('make', 613),\n ('still', 612),\n ('2008', 612),\n ('2010', 611),\n ('role', 598),\n ('led', 598),\n ('again', 595),\n ('ii', 590),\n ('moved', 590),\n ('career', 589),\n ('university', 589),\n ('without', 587),\n ('love', 585),\n ('often', 584),\n ('among', 582),\n ('recorded', 581),\n ('further', 578),\n ('hurricane', 578),\n ('military', 576),\n ('period', 575),\n ('star', 575),\n ('local', 574),\n ('considered', 573),\n ('army', 570),\n ('production', 570),\n ('release', 569),\n ('side', 568),\n ('2007', 567),\n ('great', 565),\n ('house', 557),\n ('came', 556),\n ('published', 554),\n ('written', 552),\n ('100', 551),\n ('continued', 550),\n ('power', 549),\n ('english', 548),\n ('town', 547),\n ('story', 545),\n ('forces', 542),\n ('days', 542),\n ('run', 542),\n ('route', 540),\n ('held', 540),\n ('french', 539),\n ('support', 535),\n ('14', 535),\n ('16', 532),\n ('11', 530),\n ('18', 528),\n ('force', 527),\n ('half', 527),\n ('take', 526),\n ('few', 526),\n ('international', 525),\n ('having', 524),\n ('25', 524),\n ('county', 523),\n ('land', 522),\n ('throughout', 521),\n ('2011', 519),\n ('point', 518),\n ('become', 516),\n ('order', 515),\n ('children', 515),\n ('2006', 515),\n ('light', 513),\n ('version', 513),\n ('title', 511),\n ('former', 509),\n ('lost', 507),\n ('track', 507),\n ('different', 506),\n ('development', 505),\n ('field', 504),\n ('ship', 503),\n ('similar', 502),\n ('despite', 499),\n ('live', 498),\n ('common', 497),\n ('members', 496),\n ('park', 494),\n ('february', 492),\n ('13', 491),\n ('gave', 488),\n ('produced', 488),\n ('short', 487),\n ('southern', 487),\n ('little', 485),\n ('dylan', 485),\n ('site', 482),\n ('once', 480),\n ('2012', 480),\n ('television', 480),\n ('writing', 480),\n ('given', 479),\n ('central', 478),\n ('control', 476),\n ('total', 476),\n ('country', 475),\n ('band', 475),\n ('service', 472),\n ('northern', 471),\n ('re', 469),\n ('include', 466),\n ('young', 464),\n ('position', 464),\n ('fire', 463),\n ('battalion', 460),\n ('making', 457),\n ('never', 457),\n ('seven', 456),\n ('away', 456),\n ('tour', 455),\n ('lead', 454),\n ('air', 454),\n ('age', 454),\n ('2013', 452),\n ('how', 451),\n ('reported', 451),\n ('open', 450),\n ('seen', 450),\n ('battle', 449),\n ('highway', 449),\n ('western', 448),\n ('good', 448),\n ('eastern', 448),\n ('st', 447),\n ('stated', 446),\n ('attack', 445),\n ('red', 445),\n ('god', 445),\n ('match', 444),\n ('returned', 443),\n ('across', 443),\n ('body', 442),\n ('instead', 442),\n ('ships', 441),\n ('established', 440),\n ('using', 438),\n ('ft', 438),\n ('population', 435),\n ('modern', 434),\n ('construction', 434),\n ('week', 434),\n ('america', 434),\n ('noted', 432),\n ('my', 431),\n ('less', 431),\n ('royal', 431),\n ('head', 430),\n ('c', 430),\n ('reached', 430),\n ('developed', 429),\n ('building', 429),\n ('eight', 429),\n ('rock', 428),\n ('players', 427),\n ('h', 427),\n ('ireland', 427),\n ('brigade', 426),\n ('president', 424),\n ('result', 424),\n ('thought', 422),\n ('right', 421),\n ('performance', 420),\n ('miles', 418),\n ('london', 418),\n ('himself', 417),\n ('father', 416),\n ('per', 415),\n ('important', 415),\n ('style', 413),\n ('performed', 412),\n ('felt', 411),\n ('various', 410),\n ('australia', 410),\n ('full', 409),\n ('17', 409),\n ('feet', 408),\n ('areas', 408),\n ('previous', 407),\n ('win', 407),\n ('low', 406),\n ('events', 406),\n ('b', 405),\n ('died', 401),\n ('kingdom', 401),\n ('guitar', 400),\n ('football', 399),\n ('too', 398),\n ('art', 398),\n ('others', 398),\n ('went', 398),\n ('originally', 397),\n ('project', 397),\n ('mm', 397),\n ('human', 396),\n ('upon', 395),\n ('23', 395),\n ('level', 395),\n ('works', 394),\n ('range', 394),\n ('started', 392),\n ('formed', 391),\n ('characters', 390),\n ('james', 390),\n ('political', 390),\n ('women', 388),\n ('should', 387),\n ('cup', 386),\n ('50', 384),\n ('port', 384),\n ('caused', 383),\n ('eventually', 382),\n ('located', 382),\n ('21', 382),\n ('19', 380),\n ('28', 379),\n ('created', 378),\n ('24', 378),\n ('stars', 378),\n ('critics', 377),\n ('sent', 377),\n ('me', 377),\n ('ground', 377),\n ('able', 376),\n ('2004', 374),\n ('class', 373),\n ('2005', 373),\n ('chart', 371),\n ('night', 371),\n ('born', 369),\n ('region', 369),\n ('street', 367),\n ('together', 366),\n ('design', 366),\n ('center', 366),\n ('court', 366),\n ('director', 365),\n ('present', 364),\n ('popular', 364),\n ('strong', 364),\n ('every', 363),\n ('award', 363),\n ('return', 361),\n ('son', 361),\n ('remained', 360),\n ('hero', 360),\n ('see', 359),\n ('novel', 358),\n ('completed', 358),\n ('guns', 358),\n ('scored', 357),\n ('announced', 355),\n ('australian', 355),\n ('grand', 354),\n ('almost', 353),\n ('fourth', 353),\n ('22', 353),\n ('behind', 351),\n ('least', 350),\n ('damage', 350),\n ('26', 349),\n ('added', 348),\n ('brown', 348),\n ('ten', 348),\n ('party', 348),\n ('heavy', 347),\n ('killed', 345),\n ('months', 345),\n ('followed', 345),\n ('wife', 344),\n ('appeared', 344),\n ('addition', 343),\n ('d', 343),\n ('playing', 342),\n ('does', 342),\n ('success', 342),\n ('list', 340),\n ('awards', 340),\n ('features', 338),\n ('aircraft', 335),\n ('coast', 334),\n ('sea', 334),\n ('2003', 334),\n ('taken', 333),\n ('david', 331),\n ('2015', 331),\n ('leading', 330),\n ('action', 329),\n ('championship', 329),\n ('europe', 328),\n ('france', 328),\n ('either', 327),\n ('served', 327),\n ('front', 327),\n ('recording', 327),\n ('towards', 326),\n ('operations', 325),\n ('campaign', 325),\n ('gold', 324),\n ('mother', 323),\n ('put', 323),\n ('elements', 322),\n ('decided', 322),\n ('records', 320),\n ('close', 320),\n ('generally', 319),\n ('magazine', 319),\n ('believed', 319),\n ('fleet', 319),\n ('move', 316),\n ('female', 316),\n ('post', 316),\n ('ever', 316),\n ('carey', 316),\n ('sold', 315),\n ('soon', 315),\n ('example', 315),\n ('poem', 315),\n ('goal', 314),\n ('points', 313),\n ('infantry', 313),\n ('significant', 313),\n ('fort', 312),\n ('weeks', 312),\n ('rather', 311),\n ('study', 311),\n ('european', 310),\n ('outside', 309),\n ('federer', 309),\n ('robert', 308),\n ('opened', 308),\n ('help', 307),\n ('finished', 307),\n ('directed', 307),\n ('brought', 307),\n ('case', 307),\n ('non', 306),\n ('william', 306),\n ('law', 306),\n ('go', 305),\n ('wales', 305),\n ('27', 304),\n ('earlier', 304),\n ('o', 304),\n ('featured', 303),\n ('victory', 302),\n ('get', 302),\n ('act', 300),\n ('successful', 300),\n ('manager', 300),\n ('gun', 299),\n ('start', 298),\n ('mid', 298),\n ('stage', 298),\n ('member', 297),\n ('provided', 297),\n ('association', 297),\n ('opening', 296),\n ('working', 296),\n ('appearance', 296),\n ('village', 296),\n ('mi', 296),\n ('wanted', 295),\n ('council', 295),\n ('particularly', 293),\n ('roman', 293),\n ('jordan', 293),\n ('troops', 292),\n ('2014', 291),\n ('initially', 291),\n ('depression', 291),\n ('atlantic', 291),\n ('40', 291),\n ('tech', 291),\n ('evidence', 290),\n ('29', 290),\n ('yard', 290),\n ('far', 290),\n ('office', 289),\n ('largest', 289),\n ('find', 289),\n ('dam', 289),\n ('review', 288),\n ('blue', 288),\n ('george', 288),\n ('attempt', 288),\n ('saw', 288),\n ('possible', 287),\n ('19th', 287),\n ('special', 286),\n ('type', 286),\n ('summer', 285),\n ('month', 285),\n ('above', 284),\n ('union', 284),\n ('yards', 284),\n ('rest', 283),\n ('florida', 283),\n ('allowed', 282),\n ('saying', 282),\n ('event', 281),\n ('winds', 281),\n ('race', 281),\n ('hours', 280),\n ('critical', 280),\n ('creek', 279),\n ('cross', 279),\n ('whom', 278),\n ('nine', 277),\n ('worked', 276),\n ('2001', 276),\n ('police', 276),\n ('society', 276),\n ('plan', 276),\n ('missouri', 276),\n ('designed', 275),\n ('reception', 275),\n ('500', 275),\n ('middle', 275),\n ('community', 275),\n ('previously', 274),\n ('free', 273),\n ('process', 273),\n ('forced', 272),\n ('real', 271),\n ('praised', 271),\n ('remains', 271),\n ('operation', 271),\n ('era', 271),\n ('radio', 271),\n ('200', 270),\n ('official', 269),\n ('research', 269),\n ('increased', 269),\n ('hall', 268),\n ('lower', 267),\n ('station', 267),\n ('parliament', 267),\n ('come', 266),\n ('michael', 266),\n ('relationship', 266),\n ('units', 265),\n ('command', 265),\n ('commander', 265),\n ('regiment', 265),\n ('studio', 265),\n ('hill', 265),\n ('taking', 264),\n ('base', 264),\n ('replaced', 264),\n ('parts', 263),\n ('writer', 263),\n ('navy', 262),\n ('ball', 262),\n ('industry', 262),\n ('social', 261),\n ('food', 260),\n ('highest', 259),\n ('college', 259),\n ('co', 259),\n ('bay', 259),\n ('reviews', 259),\n ('media', 259),\n ('beginning', 258),\n ('claimed', 258),\n ('estimated', 258),\n ('don', 258),\n ('museum', 257),\n ('mph', 257),\n ('canada', 257),\n ('section', 257),\n ('goals', 256),\n ('stone', 256),\n ('joined', 256),\n ('japanese', 255),\n ('placed', 255),\n ('religious', 255),\n ('commercial', 255),\n ('average', 255),\n ('stories', 254),\n ('involved', 254),\n ('signed', 254),\n ('training', 254),\n ('suggested', 254),\n ('oldham', 254),\n ('met', 253),\n ('shot', 253),\n ('introduced', 253),\n ('lines', 252),\n ('sometimes', 252),\n ('itself', 252),\n ('face', 251),\n ('31', 251),\n ('today', 251),\n ('background', 251),\n ('paul', 251),\n ('business', 251),\n ('olivier', 251),\n ('scene', 250),\n ('henry', 250),\n ('going', 250),\n ('complete', 250),\n ('structure', 249),\n ('mexico', 249),\n ('additional', 248),\n ('available', 248),\n ('thus', 248),\n ('give', 248),\n ('cast', 247),\n ('term', 247),\n ('loss', 247),\n ('language', 247),\n ('date', 247),\n ('horse', 247),\n ('whose', 246),\n ('nearly', 246),\n ('sound', 246),\n ('india', 246),\n ('thomas', 245),\n ('fifth', 245),\n ('past', 245),\n ('approximately', 244),\n ('indian', 244),\n ('shows', 244),\n ('must', 244),\n ('program', 244),\n ('irish', 244),\n ('told', 243),\n ('entire', 243),\n ('capital', 243),\n ('damaged', 243),\n ('already', 243),\n ('friends', 243),\n ('appointed', 243),\n ('native', 242),\n ('prior', 242),\n ('sun', 242),\n ('britain', 242),\n ('names', 241),\n ('issue', 241),\n ('probably', 241),\n ('turned', 241),\n ('mark', 241),\n ('forest', 241),\n ('male', 240),\n ('r', 240),\n ('winning', 239),\n ('passed', 239),\n ('change', 238),\n ('students', 238),\n ('our', 238),\n ('length', 237),\n ('size', 237),\n ('earth', 237),\n ('civil', 236),\n ('child', 236),\n ('especially', 236),\n ('woman', 235),\n ('christian', 235),\n ('enough', 235),\n ('notes', 235),\n ('overall', 234),\n ('chinese', 234),\n ('forward', 234),\n ('failed', 234),\n ('better', 234),\n ('running', 234),\n ('mixed', 234),\n ('captain', 233),\n ('ny', 233),\n ('limited', 232),\n ('2002', 232),\n ('iii', 231),\n ('minor', 231),\n ('ordered', 231),\n ('network', 231),\n ('l', 231),\n ('future', 231),\n ('wheeler', 231),\n ('regular', 230),\n ('remaining', 230),\n ('pacific', 230),\n ('spent', 230),\n ('changes', 230),\n ('moving', 230),\n ('might', 230),\n ('arrived', 230),\n ('includes', 230),\n ('larger', 229),\n ('education', 229),\n ('usually', 229),\n ('san', 229),\n ('canadian', 229),\n ('cathedral', 229),\n ('hand', 228),\n ('department', 228),\n ('hit', 228),\n ('birds', 228),\n ('required', 227),\n ('uk', 227),\n ('latter', 227),\n ('lake', 227),\n ('decision', 226),\n ('plot', 225),\n ('response', 225),\n ('africa', 225),\n ('wide', 225),\n ('debut', 225),\n ('space', 224),\n ('musical', 224),\n ('round', 224),\n ('voice', 224),\n ('2000', 224),\n ('crew', 223),\n ('appear', 223),\n ('related', 222),\n ('mounted', 222),\n ('groups', 222),\n ('territory', 221),\n ('view', 221),\n ('centre', 221),\n ('films', 221),\n ('supported', 221),\n ('saint', 221),\n ('rachel', 221),\n ('jin', 221),\n ('positive', 220),\n ('feature', 220),\n ('science', 220),\n ('nature', 220),\n ('billboard', 220),\n ('pressure', 220),\n ('ended', 220),\n ('00', 220),\n ('finally', 219),\n ('score', 219),\n ('squadron', 219),\n ('flight', 219),\n ('culture', 219),\n ('money', 218),\n ('becoming', 218),\n ('always', 217),\n ('particular', 217),\n ('smaller', 217),\n ('books', 217),\n ('provide', 217),\n ('shown', 217),\n ('charles', 217),\n ('private', 216),\n ('1995', 216),\n ('anti', 216),\n ('discovered', 216),\n ('shortly', 215),\n ('board', 215),\n ('minutes', 215),\n ('difficult', 215),\n ('person', 214),\n ('experience', 214),\n ('nations', 214),\n ('peter', 214),\n ('trade', 214),\n ('defeated', 214),\n ('mass', 214),\n ('temple', 214),\n ('staff', 213),\n ('big', 213),\n ('surface', 213),\n ('subsequently', 213),\n ('lack', 213),\n ('japan', 212),\n ('upper', 212),\n ('press', 212),\n ('living', 212),\n ('effects', 212),\n ('zealand', 212),\n ('fact', 211),\n ('word', 211),\n ('professional', 211),\n ('material', 210),\n ('greater', 210),\n ('mostly', 209),\n ('idea', 209),\n ('problems', 209),\n ('self', 209),\n ('intended', 209),\n ('room', 209),\n ('teams', 209),\n ('bridge', 209),\n ('numbers', 208),\n ('collection', 208),\n ('captured', 208),\n ('largely', 208),\n ('cut', 208),\n ('interest', 208),\n ('key', 208),\n ('gods', 208),\n ('referred', 207),\n ('themselves', 207),\n ('cover', 207),\n ('cm', 207),\n ('likely', 207),\n ('metres', 207),\n ('primary', 207),\n ('tv', 207),\n ('haifa', 207),\n ('mountain', 206),\n ('got', 206),\n ('2016', 206),\n ('60', 206),\n ('machine', 206),\n ('scientology', 206),\n ('news', 205),\n ('yet', 205),\n ('traffic', 205),\n ('xenon', 205),\n ('location', 204),\n ('tower', 204),\n ('entered', 204),\n ('contract', 204),\n ...]\n\n\nThe above approach for making vocabulary has following limitations: 1. size of vocabulary is very large. This means we need output layer with more number of neurons -&gt; more computation 2. Some words are less frequent occuring maybe only once or twice in the entire dataset. I found that in model training, when our dataset is small and we have large vocab and many words appear less frequently, it does not result in good model training. 3. Current vocabulary cannot handle out-of-vocabulary words that could appear in validation/ test settings. 4. Many stopwords like the, a are overrepresented in the dataset.\nIn order to solve 1 and 2, we filter out low frequency words by defining a hyperparameter MIN_FREQ\n(For 1 and 2, we can also use Heirarchical Softmax instead of just Softmax classifier.)\nFor 3, we assign token \"&lt;unk&gt;\" to new tokens (or words) and to less frequent ones which we filtered out using MIN_FREQ\nFor 4, we can use remove_stopwords = False while tokenizing the text. But when I removed stopwords, I witnessed decline in performance. Perhaps because stopwords are high in frequency and are essential to understand context in order to learn the current word emebeddings. Hence, I am keeping the stopwords.\n\n# filter low frequency words to create vocabulary (from train text)\nMIN_FREQ = 50\ntrain_word_counts = Counter(train_words)\nhigh_freq_words = [word for word in train_words if train_word_counts[word] &gt;= MIN_FREQ]\n\n# create a vocabulary\nvocab = list(set(high_freq_words))\nvocab.append(\"&lt;unk&gt;\")\nV = len(vocab)\nprint(\"Vocabulary size, V:\", V)\n\nVocabulary size, V: 4089\n\n\nThis has brought down vocab from over 66k to just about 4k.\n\nCBOW\nModel architecture as proposed\n\nCBOW: Predict the middle word based on R previous words and R future words\nSkip-gram: Predict context words based on middle word\n\nCBOW architecture has no hidden layer.\nIt is a feedforward NN with following layers: Input → Embedding → Output\n\nInput layer: one-hot encoded context words\nEmbedding layer: V x D matrix\nOutput layer: V x 1 vector\n\n“…where the non-linear hidden layer is removed…” -&gt; no hidden layer\n“…and the projection layer is shared for all words (not just the projection matrix); thus, all words get projected into the same position (their vectors are averaged)” -&gt; self.embeddings(context_indices).mean(dim=1)\n\n\nWhy was the non-linear hidden layer removed?\nAns: To make training faster. In other words, make model simpler. This was the main argument in the paper.\n\nclass CBOWModel(nn.Module):\n    def __init__(self, V, D):\n        \"\"\"\n        V: vocabulary size\n        D: embedding size\n        \"\"\"\n        super(CBOWModel, self).__init__()\n        self.projection = nn.Embedding(V, D)\n        self.output = nn.Linear(D, V)\n        self.init_weights()\n\n    def forward(self, inputs):\n        \"\"\"\n        inputs: context indices of shape [batch_size, 2R]\n        \"\"\"\n        embeddings = self.projection(inputs).mean(dim=1) # Average across context words shape: [batch_size, D]\n        logits = self.output(embeddings)\n        return logits\n    \n    # optional: weights initialization\n    def init_weights(self):\n        # xavier uniform initialization for embeddings and linear weights\n        nn.init.xavier_uniform_(self.projection.weight)\n        nn.init.xavier_uniform_(self.output.weight)\n        # zero bias initialization (optional but recommended)- valid for linear layer not for embedding layer\n        if self.output.bias is not None:\n            nn.init.zeros_(self.output.bias)\n\nAs per original paper (refer above image), authors: - used 4 history and 4 future word to predict current word. (R=4)\n\nR = 4 # context window size\n\n# create word to index and index to word mappings\nword_to_idx = {word: idx for idx, word in enumerate(vocab)}\nidx_to_word = {idx: word for word, idx in word_to_idx.items()}\n\ndef get_context_target(words, R, word_to_idx):\n    contexts = []\n    targets = []\n    for i in range(R, len(words) - R):\n        context_words = words[i-R:i] + words[i+1:i+R+1]\n        context_words = torch.tensor([word_to_idx[w] if w in word_to_idx else word_to_idx[\"&lt;unk&gt;\"] for w in context_words], dtype=torch.long)\n\n        target_word = words[i]\n        target_word = torch.tensor(word_to_idx[target_word] if target_word in word_to_idx else word_to_idx[\"&lt;unk&gt;\"], dtype=torch.long)\n\n        contexts.append(context_words)\n        targets.append(target_word)\n\n    X = torch.stack(contexts)\n    y = torch.stack(targets)\n    return X, y\n\nX_train, y_train = get_context_target(train_words, R, word_to_idx)\nX_val, y_val = get_context_target(val_words, R, word_to_idx)\n\nprint(\"Train data:\")\nprint(X_train.shape, y_train.shape)\nprint(\"Validation data:\")\nprint(X_val.shape, y_val.shape)\n\nTrain data:\ntorch.Size([1749361, 8]) torch.Size([1749361])\nValidation data:\ntorch.Size([183475, 8]) torch.Size([183475])\n\n\n\nprint(\"X_train sample:\")\nprint(X_train[:2,:])\nprint(\"y_train sample:\")\nprint(y_train[:2])\nprint(\"--------------------------------\")\nprint(\"X_val sample:\")\nprint(X_val[:2,:])\nprint(\"y_val sample:\")\nprint(y_val[:2])\n\nX_train sample:\ntensor([[ 581, 3372, 3297, 4088,  581,  274, 4088, 3372],\n        [3372, 3297, 4088, 1405,  274, 4088, 3372, 1964]])\ny_train sample:\ntensor([1405,  581])\n--------------------------------\nX_val sample:\ntensor([[4088, 4088, 4088, 4088,  240, 2041, 3147, 4088],\n        [4088, 4088, 4088, 3169, 2041, 3147, 4088, 2358]])\ny_val sample:\ntensor([3169,  240])\n\n\n\n\nHow CBOW is different from stansard Bag-of-Words? (Or, What does “as unlike standard bag-of-words model, it uses continuous distributed representation of the context” means?)\nAns:\nStandard Bag-of-Words: - Each word is represented as sparse vector containing 1’s at index corresponding to the context words and 0’s elsewhere. - Here, size of each word embedding vector = [1, V] where V is vocab size\nContinuous Bag-of-Words - Each word is represented via a projection matrix in which values are continuos and hence the vector does not necessarily consist of just 1’s and 0’s - Here, size of each word embedding vector = [1, D] where D is embedding_dimension a user chooses\n\nD = 300 # embedding size\nprint('Context window size, 2R:', 2*R)\nprint('Embedding size, D:', D)\nprint('Vocabulary size, V:', V)\n\n# create the model\nmodel = CBOWModel(V, D).to(device)\n# count the number of parameters\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint('Number of trainable parameters (Millions):', trainable_params/1e6)\n\nContext window size, 2R: 8\nEmbedding size, D: 300\nVocabulary size, V: 4089\nNumber of trainable parameters (Millions): 2.457489\n\n\n\nclass CBOWDataset(Dataset):\n    def __init__(self, contexts, targets):\n        self.contexts = contexts.to(device)\n        self.targets = targets.to(device)\n\n    def __len__(self):\n        return len(self.contexts)\n    \n    def __getitem__(self, idx):\n        return self.contexts[idx], self.targets[idx]\n\n\n# create the dataset\ntrain_ds = CBOWDataset(X_train, y_train)\nval_ds = CBOWDataset(X_val, y_val)\n\n# create the dataloader\nbatch_size = 128\ntrain_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\nval_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True)\n\n# Model training settings\ncriterion = nn.CrossEntropyLoss()\nepochs = 20\n# LambdaLR scheduler (start with lr=0.025 and linearly decay to 0.00005 (nearly 0) over user-defined epochs)\ninitial_lr = 0.025\nlr_lambda = lambda epoch: 1 - epoch / epochs\noptimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\nscheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n\nfor epoch in range(epochs):\n    total_loss = 0\n    model.train()\n    for batch_idx, (context_idx, target_idx) in enumerate(train_dl):\n        context_idx = context_idx.to(device)\n        target_idx = target_idx.to(device)\n\n        optimizer.zero_grad()\n        logits = model(context_idx)\n        loss = criterion(logits, target_idx)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        \n    current_lr = optimizer.param_groups[0]['lr']\n    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_dl):.4f}, LR: {current_lr:.4f}\")\n    scheduler.step()\n\n    model.eval()\n    with torch.no_grad():\n        total_loss = 0\n        for batch_idx, (context_idx, target_idx) in enumerate(val_dl):\n            context_idx = context_idx.to(device)\n            target_idx = target_idx.to(device)\n\n            logits = model(context_idx)\n            loss = criterion(logits, target_idx)\n            total_loss += loss.item()\n\n        print(f\"Validation Loss: {total_loss/len(val_dl):.4f}\")\n\nEpoch 1, Loss: 6.0743, LR: 0.0250\nValidation Loss: 6.0751\nEpoch 2, Loss: 5.9600, LR: 0.0238\nValidation Loss: 5.9654\nEpoch 3, Loss: 5.8597, LR: 0.0225\nValidation Loss: 5.9073\nEpoch 4, Loss: 5.7652, LR: 0.0213\nValidation Loss: 5.8122\nEpoch 5, Loss: 5.6731, LR: 0.0200\nValidation Loss: 5.7198\nEpoch 6, Loss: 5.5820, LR: 0.0188\nValidation Loss: 5.6467\nEpoch 7, Loss: 5.4927, LR: 0.0175\nValidation Loss: 5.5951\nEpoch 8, Loss: 5.4013, LR: 0.0163\nValidation Loss: 5.5274\nEpoch 9, Loss: 5.3099, LR: 0.0150\nValidation Loss: 5.4637\nEpoch 10, Loss: 5.2136, LR: 0.0138\nValidation Loss: 5.4086\nEpoch 11, Loss: 5.1117, LR: 0.0125\nValidation Loss: 5.3469\nEpoch 12, Loss: 5.0028, LR: 0.0112\nValidation Loss: 5.3002\nEpoch 13, Loss: 4.8867, LR: 0.0100\nValidation Loss: 5.2706\nEpoch 14, Loss: 4.7610, LR: 0.0087\nValidation Loss: 5.2455\nEpoch 15, Loss: 4.6259, LR: 0.0075\nValidation Loss: 5.2198\nEpoch 16, Loss: 4.4806, LR: 0.0063\nValidation Loss: 5.2170\nEpoch 17, Loss: 4.3247, LR: 0.0050\nValidation Loss: 5.2252\nEpoch 18, Loss: 4.1590, LR: 0.0038\nValidation Loss: 5.2454\nEpoch 19, Loss: 3.9821, LR: 0.0025\nValidation Loss: 5.2643\nEpoch 20, Loss: 3.7955, LR: 0.0013\nValidation Loss: 5.2792\n\n\nAbove, we used dynamic learning rate by using lr_scheduler from Pytorch. We reduced learning rate linearly because because original paper did this.\n\n\n# embedding from first model layer\nembeddings = list(model.parameters())[0]\nembeddings = embeddings.cpu().detach().numpy()\n\n# normalization\nnorms = (embeddings ** 2).sum(axis=1) ** (1 / 2)\nnorms = np.reshape(norms, (len(norms), 1))\nembeddings_norm = embeddings / norms\nembeddings_norm.shape\n\ndef get_top_similar(word: str, topN: int = 10):\n    word_id = word_to_idx[word]\n    word_vec = embeddings_norm[word_id]\n    word_vec = np.reshape(word_vec, (len(word_vec), 1))\n    dists = np.matmul(embeddings_norm, word_vec).flatten()\n    topN_ids = np.argsort(-dists)[1 : topN + 1]\n\n    topN_dict = {}\n    for sim_word_id in topN_ids:\n        sim_word = idx_to_word[sim_word_id]\n        topN_dict[sim_word] = dists[sim_word_id]\n    return topN_dict\n\n\nLet’s see some Semantic capabilities which our model has learned…\n\nprint(\"Top 5 similar words to 'man':\")\nfor word, sim in get_top_similar(\"man\", topN=5).items():\n    print(\"{}: {:.3f}\".format(word, sim))\n\nTop 5 similar words to 'man':\nperson: 0.287\nwoman: 0.272\ncougar: 0.270\ngirl: 0.246\npitcher: 0.224\n\n\n\n\nLet’s see with Syntactic capabilities which our model has learned\n\n\nemb1 = embeddings[word_to_idx[\"king\"]]\nemb2 = embeddings[word_to_idx[\"man\"]]\nemb3 = embeddings[word_to_idx[\"woman\"]]\n\nemb4 = emb1 - emb2 + emb3\nemb4_norm = (emb4 ** 2).sum() ** (1 / 2)\nemb4 = emb4 / emb4_norm\n\nemb4 = np.reshape(emb4, (len(emb4), 1))\ndists = np.matmul(embeddings_norm, emb4).flatten()\n\ntop5 = np.argsort(-dists)[:15]\nprint(\"\\nTop 15 similar words to 'king - man + woman':\")\nfor i,word_id in enumerate(top5):\n    print(\"{}. {}: {:.3f}\".format(i+1, idx_to_word[word_id], dists[word_id]))\n\n\nTop 15 similar words to 'king - man + woman':\n1. woman: 0.669\n2. king: 0.600\n3. emperor: 0.256\n4. lord: 0.245\n5. treaty: 0.234\n6. church: 0.224\n7. archbishop: 0.224\n8. monarch: 0.215\n9. scene: 0.212\n10. royal: 0.211\n11. regime: 0.207\n12. anglo: 0.202\n13. 10th: 0.200\n14. surviving: 0.200\n15. queen: 0.198\n\n\nWe see queen in top 15 words. To get better results: 1. train with larger sized datasets 2. Increase D 3. Train longer\nWhy am I saying this?\nAs per original paper trained the CBOW model on a large dataset of 6 Billion words. Even then their performance were not super-good until they started to increase the D.\n\nTheir results:\n\nFurther improvements: 1. Implement Heirarchical Softmax to speed up the computation 2. Implement Negative sampling"
  },
  {
    "objectID": "posts/2025-06-10-vit/vit_attention.html",
    "href": "posts/2025-06-10-vit/vit_attention.html",
    "title": "ViT Made Easy (Draft)",
    "section": "",
    "text": "From “Attention is All You Need”, attention is defined in terms of Queries, Keys, and Values matrices - calculated through a learnable linear layer.\nNote: It changes the length of the tokens from the token_len or dim (e.g. 49) to the channels or chan parameter (e.g. 64). Notice the bottom matrix of Q, K, V says “Projected Length of Tokens”\ni.e.\nBut we do it in steps:\n2.1\nStep-3:\nThis results in the following shape:\nStep-4: Skip Connection\nNote: Shape of new X can be different from input X. So, we use V for skip connection (by flattening it’s attention head dimension)\nimport torch\nimport torch.nn as nn\n\nclass Attention(nn.Module):\n    def __init__(self, \n                dim: int,\n                chan: int,\n                num_heads: int=1,\n                qkv_bias: bool=False,\n                qk_scale: float=None):\n\n        \"\"\" Attention Module\n\n            Args:\n                dim (int): input size of a single token\n                chan (int): resulting size of a single token (channels)\n                num_heads(int): number of attention heads in MSA\n                qkv_bias (bool): determines if the qkv layer learns an addative bias\n                qk_scale (NoneFloat): value to scale the queries and keys by; \n                                    if None, queries and keys are scaled by ``head_dim ** -0.5``\n        \"\"\"\n\n        super().__init__()\n\n        ## Define Constants\n        self.num_heads = num_heads\n        self.chan = chan\n        self.head_dim = self.chan // self.num_heads\n        self.scale = qk_scale or self.head_dim ** -0.5\n        assert self.chan % self.num_heads == 0, '\"Chan\" must be evenly divisible by \"num_heads\".'\n\n        ## Define Layers\n        self.qkv = nn.Linear(dim, chan * 3, bias=qkv_bias)\n        #### Each token gets projected from starting length (dim) to channel length (chan) 3 times (for each Q, K, V)\n        self.proj = nn.Linear(chan, chan)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        ## Dimensions: (batch, num_tokens, token_len)\n\n        ## Calcuate QKVs\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        #### Dimensions: (3, batch, heads, num_tokens, chan/num_heads = head_dim)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n\n        ## Calculate Attention\n        attn = (q * self.scale) @ k.transpose(-2, -1)\n        attn = attn.softmax(dim=-1)\n        #### Dimensions: (batch, heads, num_tokens, num_tokens)\n\n        ## Attention Layer\n        x = (attn @ v).transpose(1, 2).reshape(B, N, self.chan)\n        #### Dimensions: (batch, heads, num_tokens, chan)\n\n        ## Projection Layers\n        x = self.proj(x)\n\n        ## Skip Connection Layer\n        v = v.transpose(1, 2).reshape(B, N, self.chan)\n        x = v + x     \n        #### Because the original x has different size with current x, use v to do skip connection\n\n        return x\n\nc:\\Users\\mgupta70\\AppData\\Local\\anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree&gt;=0.13.0'`.\n  warnings.warn("
  },
  {
    "objectID": "posts/2025-06-10-vit/vit_attention.html#single-headed-attention",
    "href": "posts/2025-06-10-vit/vit_attention.html#single-headed-attention",
    "title": "ViT Made Easy (Draft)",
    "section": "Single-Headed Attention",
    "text": "Single-Headed Attention\n\ntoken_len = 49 # 7x7\nchannels = 64\nnum_tokens = 100\nbatch = 13\n\nx = torch.rand(batch, num_tokens, token_len)\nB, N, C = x.shape\nprint('Input dimensions are:', x.shape, '\\n\\t batchsize: ', x.shape[0], '\\n\\t num_tokens: ', x.shape[1], '\\n\\t token_len: ', x.shape[2])\n\n\n# Attention Module\nA = Attention(dim=token_len, chan=channels, num_heads=1, qkv_bias=False, qk_scale=None)\nA.eval() # ? Why??\n\nInput dimensions are: torch.Size([13, 100, 49]) \n     batchsize:  13 \n     num_tokens:  100 \n     token_len:  49\n\n\nAttention(\n  (qkv): Linear(in_features=49, out_features=192, bias=False)\n  (proj): Linear(in_features=64, out_features=64, bias=True)\n)\n\n\n\n# Step 1: Calculate Q, K, V\nqkv = A.qkv(x)\nprint(qkv.shape)\nprint('192 is basically 3x64, 3 is because of q, k, v')\nprint('')\n# Reshape qkv to get q, k, v\nprint('-- Reshaping --')\nqkv = qkv.reshape(B, N, 3, A.num_heads, A.head_dim).permute(2, 0, 3, 1, 4)\nprint(qkv.shape)\nprint('\\t3: for q, k, v\\n\\t13: batchsize\\n\\t1: num_heads\\n\\t100: num_tokens\\n\\t64: head_dim=(channels/num_heads) = 64/1 = 64')\n\ntorch.Size([13, 100, 192])\n192 is basically 3x64, 3 is because of q, k, v\n\n-- Reshaping --\ntorch.Size([3, 13, 1, 100, 64])\n    3: for q, k, v\n    13: batchsize\n    1: num_heads\n    100: num_tokens\n    64: head_dim=(channels/num_heads) = 64/1 = 64\n\n\n\nq, k, v = qkv[0], qkv[1], qkv[2]\nprint('See that the dimensions for queries, keys, and values are all the same:')\nprint('\\tShape of Q:', q.shape, '\\n\\tShape of K:', k.shape, '\\n\\tShape of V:', v.shape)\nprint('Dimensions for Queries are \\n\\tbatchsize:', q.shape[0], '\\n\\tattention heads:', q.shape[1], '\\n\\tnumber of tokens:', q.shape[2], '\\n\\tnew length of tokens:', q.shape[3])\n\n\nSee that the dimensions for queries, keys, and values are all the same:\n    Shape of Q: torch.Size([13, 1, 100, 64]) \n    Shape of K: torch.Size([13, 1, 100, 64]) \n    Shape of V: torch.Size([13, 1, 100, 64])\nDimensions for Queries are \n    batchsize: 13 \n    attention heads: 1 \n    number of tokens: 100 \n    new length of tokens: 64\n\n\n\n# Step 2: Calculate \"Attention\" values\n# Step- 2A: Scaling factor\nprint(f\"Scaling factor: 1/sqrt(head_dim)) = 1/({A.head_dim})**(0.5) = {A.scale}\")\n\n# Step- 2B: Calculate Attention\nattn = (q * A.scale) @ k.transpose(-2, -1)\nprint('Dimensions for Attn are:', attn.shape, '\\n\\tbatchsize:', attn.shape[0], '\\n\\tattention heads:', attn.shape[1], '\\n\\tnumber of tokens:', attn.shape[2], '\\n\\tnumber of tokens:', attn.shape[3])\n\n\n\nScaling factor: 1/sqrt(head_dim)) = 1/(64)**(0.5) = 0.125\nDimensions for Attn are: torch.Size([13, 1, 100, 100]) \n    batchsize: 13 \n    attention heads: 1 \n    number of tokens: 100 \n    number of tokens: 100\n\n\n\n# Step 3: Normalize Attention with Softmax\nattn = attn.softmax(dim=-1)\nprint('Dimensions for Attn after softmax are:', attn.shape, '\\n\\tbatchsize:', attn.shape[0], '\\n\\tattention heads:', attn.shape[1], '\\n\\tnumber of tokens:', attn.shape[2], '\\n\\tnumber of tokens:', attn.shape[3])\n\n# Step 4: Calculate \"Attention\" values\nx = attn @ v\nprint('Dimensions for Attn after softmax are:', x.shape, '\\n\\tbatchsize:', x.shape[0], '\\n\\tattention heads:', x.shape[1], '\\n\\tnumber of tokens:', x.shape[2], '\\n\\tlength of tokens:', x.shape[3])\n\n\n\nDimensions for Attn after softmax are: torch.Size([13, 1, 100, 100]) \n    batchsize: 13 \n    attention heads: 1 \n    number of tokens: 100 \n    number of tokens: 100\nDimensions for Attn after softmax are: torch.Size([13, 1, 100, 64]) \n    batchsize: 13 \n    attention heads: 1 \n    number of tokens: 100 \n    length of tokens: 64\n\n\n\nx = x.transpose(1, 2).reshape(B, N, A.chan)\nprint('Dimensions for x are', x.shape, '\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\tlength of tokens:', x.shape[2])\n\nDimensions for x are torch.Size([13, 100, 64]) \n    batchsize: 13 \n    number of tokens: 100 \n    length of tokens: 64\n\n\n\n# Step 4: Skip Connection\norig_shape = (batch, num_tokens, token_len)\nprint('Original shape:', orig_shape)\n\ncurr_shape = x.shape\nprint('Current shape:', curr_shape)\n\nprint('old v shape:', v.shape)\nv = v.transpose(1, 2).reshape(B, N, A.chan)\nprint('new v shape:', v.shape)\n\nx = x + v\nprint('After skip connection:', x.shape)\n\nOriginal shape: (13, 100, 49)\nCurrent shape: torch.Size([13, 100, 64])\nold v shape: torch.Size([13, 1, 100, 64])\nnew v shape: torch.Size([13, 100, 64])\nAfter skip connection: torch.Size([13, 100, 64])\n\n\nNotice: we flattened V along the head dimension i.e. 1 in [13, 1, 100, 64]"
  },
  {
    "objectID": "posts/2025-06-10-vit/vit_attention.html#multi-headed-attention",
    "href": "posts/2025-06-10-vit/vit_attention.html#multi-headed-attention",
    "title": "ViT Made Easy (Draft)",
    "section": "Multi-headed Attention",
    "text": "Multi-headed Attention\n\naka MSA i.e. Multi-headed Self Attention\n\n\ntoken_len = 49 # 7x7\nchannels = 64\nnum_tokens = 100\nbatch = 13\nnum_heads = 4\n\nx = torch.rand(batch, num_tokens, token_len)\nB, N, C = x.shape\nprint('Input dimensions are:', x.shape, '\\n\\t batchsize: ', x.shape[0], '\\n\\t num_tokens: ', x.shape[1], '\\n\\t token_len: ', x.shape[2])\n\n\n# Attention Module\nMSA = Attention(dim=token_len, chan=channels, num_heads=num_heads, qkv_bias=False, qk_scale=None)\nMSA.eval()\n\nInput dimensions are: torch.Size([13, 100, 49]) \n     batchsize:  13 \n     num_tokens:  100 \n     token_len:  49\n\n\nAttention(\n  (qkv): Linear(in_features=49, out_features=192, bias=False)\n  (proj): Linear(in_features=64, out_features=64, bias=True)\n)\n\n\nIn MSA: The total size of the Q, K, and V matrices have not changed; their contents are just distributed across the head dimension.\nThink about this as segmenting the single headed matrix for the multiple heads:\n\n\n# Step 1: Calculate Q, K, V\nqkv = MSA.qkv(x)\nprint(qkv.shape)\nprint('\\t192 is basically 3x4x16, 3 is because of q, k, v, 4 is because of num_heads, 16 is because of head_dim')\nprint('')\n\n# Reshape qkv to get q, k, v\nqkv = qkv.reshape(B, N, 3, MSA.num_heads, MSA.head_dim).permute(2, 0, 3, 1, 4)\nprint(qkv.shape)\nprint('\\t3: for q, k, v\\n\\t13: batchsize\\n\\t4: num_heads\\n\\t100: num_tokens\\n\\t16: head_dim=(channels/num_heads) = 64/4 = 16')\n\ntorch.Size([13, 100, 192])\n    192 is basically 3x4x16, 3 is because of q, k, v, 4 is because of num_heads, 16 is because of head_dim\n\ntorch.Size([3, 13, 4, 100, 16])\n    3: for q, k, v\n    13: batchsize\n    4: num_heads\n    100: num_tokens\n    16: head_dim=(channels/num_heads) = 64/4 = 16\n\n\n\nq, k, v = qkv[0], qkv[1], qkv[2]\n\nprint('See that the dimensions for queries, keys, and values are all the same:')\nprint('\\tShape of Q:', q.shape, '\\n\\tShape of K:', k.shape, '\\n\\tShape of V:', v.shape)\nprint('Dimensions for Queries are \\n\\tbatchsize:', q.shape[0], '\\n\\tattention heads:', q.shape[1], '\\n\\tnumber of tokens:', q.shape[2], '\\n\\tnew length of tokens:', q.shape[3])\n\n\nSee that the dimensions for queries, keys, and values are all the same:\n    Shape of Q: torch.Size([13, 4, 100, 16]) \n    Shape of K: torch.Size([13, 4, 100, 16]) \n    Shape of V: torch.Size([13, 4, 100, 16])\nDimensions for Queries are \n    batchsize: 13 \n    attention heads: 4 \n    number of tokens: 100 \n    new length of tokens: 16\n\n\nThe next step is to calculate attention for each head i as shown below:\n\n\n\nFor every head, i\n\n\n\n\n\nwhere dk is:\n\n\n\n\n\n# Step 2: Calculate Attention for each head\n\nattn = (q * MSA.scale) @ k.transpose(-2, -1)\nprint('Dimensions for Attn are:', attn.shape, '\\n\\tbatchsize:', attn.shape[0], '\\n\\tattention heads:', attn.shape[1], '\\n\\tnumber of tokens:', attn.shape[2], '\\n\\tnumber of tokens:', attn.shape[3])\n\n# Step 3: Normalize Attention with Softmax\nattn = attn.softmax(dim=-1)\nprint('Dimensions for Attn after softmax are:', attn.shape, '\\n\\tbatchsize:', attn.shape[0], '\\n\\tattention heads:', attn.shape[1], '\\n\\tnumber of tokens:', attn.shape[2], '\\n\\tnumber of tokens:', attn.shape[3])\n\n\nDimensions for Attn are: torch.Size([13, 4, 100, 100]) \n    batchsize: 13 \n    attention heads: 4 \n    number of tokens: 100 \n    number of tokens: 100\nDimensions for Attn after softmax are: torch.Size([13, 4, 100, 100]) \n    batchsize: 13 \n    attention heads: 4 \n    number of tokens: 100 \n    number of tokens: 100\n\n\n\n# Step 4: Calculate \"Attention\" values\nx = attn @ v\nprint('Dimensions for Attn after softmax are:', x.shape, '\\n\\tbatchsize:', x.shape[0], '\\n\\tattention heads:', x.shape[1], '\\n\\tnumber of tokens:', x.shape[2], '\\n\\tlength of tokens:', x.shape[3])\n\nDimensions for Attn after softmax are: torch.Size([13, 4, 100, 16]) \n    batchsize: 13 \n    attention heads: 4 \n    number of tokens: 100 \n    length of tokens: 16\n\n\nNow, we concatenate all x i’s :\n\n\nx = x.transpose(1, 2).reshape(B, N, MSA.chan)\nprint('Dimensions for x are:', x.shape, '\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\tlength of tokens:', x.shape[2])\n\nDimensions for x are: torch.Size([13, 100, 64]) \n    batchsize: 13 \n    number of tokens: 100 \n    length of tokens: 64\n\n\nNow, we got something similar as Single Headed Attention. Rest of the module remains same.\nFor skip connection, we still use V, but we have to reshape it to remove head dim\n\nx = MSA.proj(x)\nprint('Dimensions for x after projection are:', x.shape, '\\n\\tbatchsize:', x.shape[0], '\\n\\tnumber of tokens:', x.shape[1], '\\n\\tlength of tokens:', x.shape[2])\n\norig_shape = (batch, num_tokens, token_len)\nprint('Original shape:', orig_shape)\ncurr_shape = x.shape\nprint('Current shape:', curr_shape)\n\n# Skip Connection\n# flatten v\nprint('old v shape:', v.shape)\nv = v.transpose(1, 2).reshape(B, N, MSA.chan)\nprint('new v shape:', v.shape)\n\n# skip connection\nx = v + x\nprint('new x shape:', x.shape)\n\nDimensions for x after projection are: torch.Size([13, 100, 64]) \n    batchsize: 13 \n    number of tokens: 100 \n    length of tokens: 64\nOriginal shape: (13, 100, 49)\nCurrent shape: torch.Size([13, 100, 64])\nold v shape: torch.Size([13, 4, 100, 16])\nnew v shape: torch.Size([13, 100, 64])\nnew x shape: torch.Size([13, 100, 64])\n\n\nFinal Notes:\nThe learnable weights in an attention layer are found in the first projection from tokens to queries, keys, and values and in the final projection. The majority of the attention layer is deterministic matrix multiplication."
  },
  {
    "objectID": "posts/2025-06-08-transformersQA/transformers_QA.html",
    "href": "posts/2025-06-08-transformersQA/transformers_QA.html",
    "title": "Transformers Made Easy (Draft)",
    "section": "",
    "text": "Explain Attention in Transformers as laid out in “Attention is all you need” paper?\nAttention = Scaled Dot-Product Attention\n\nAttention = \nIn this, weights of each value = \nAttention is dot-product b/w \\(Q\\) and \\(K\\) normalized with \\(\\sqrt{d_k}\\) and then applying a Softmax function to get weights on each value. Weighted sum of values completes the attention calculation.\n\n\nExplain difference b/w Self-Attention vs Cross-Attention\nAttention mechanism is central to the success of Transformer models. Broadly speaking, attention enable the model to selectively focus on different parts of the input sequence.\nIn context of Transformer model proposed in “Attention is all you need”\nSelf-Attention - present in both Encoder and Decoder - allows the model to weigh the importance of each element in the single sequence to compute its own representation. - capture long-range dependencies and contextual information within the same sequence\n—– Self-Attention in “Attention is all you need” ——\nFor Encoder: In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\nFor Decoder: Self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections.\nCross-Attention - present only in Decoder - enables the model to attend to different parts of the input sequence while generating the output - enables interaction b/w input and output sequences. - In other words, allows the model to consider the relevant context from the encoder’s output during the generation of each element in the output sequence.\n—– Cross-Attention in “Attention is all you need” ——\nQueries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence.\n\n\nWhy is Cross-Attention important?\n\nHelpful in scenarios where information required to generate an element in output sequence is spread across the input sequence\nFor example, in language translation, understanding the meaning of a word in the target language may require considering the multiple words in the input language sentence.\nCross-attention allows the model to selectively attend to relevant parts of the source sequence during the generation process, generating meaningful translations.\n\n\n\nHow does Transformer parallelize computation?\nOR\n\n\nHow does Transformer avoid recurrence and convolutions?\nIt processes entire word sequence (or image patches) parallely. Transformers use the concept of Attention to consider all elements in the input sequence simultaneously, avoiding recurrence.\n\n\nWhy RNNs are slow or sequential?\nIn RNN or LSTMs, hidden stare \\(h_t\\) is a function of \\(h_{t-1}\\) and input at \\(t\\). This sequential nature precludes parallelization.\n\n\nWhy RNNs fails to capture long-term dependencies? How Transformers solved this?\n\nOne key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies\nAs mentioned above, \\(h_t\\) = func( \\(h_{t-1}\\) and \\(x_t\\)) in RNNs.\nThis means in a sequential network impact of \\(h_{10}\\) on \\(h_{11}\\) will definitely be more than \\(h_{2}\\). Due to this nature where dependency of two elements is dependent upon the distance between them + vanishing gradient problem, RNNs fail for long-contexts.\nTransformers solve this by processing the entire sequence simultaneously, disregarding the distance between two elements (by using attention mechanism to draw global dependencies).\na self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations.\n\n\n\nHow does Transformer enable capturing long-term dependencies?\nUsing Residual connections, Layer Norm - mitigating vanishing gradient problem. Procesing the entire sequence in parallel and using concept of self-attention to capture interaction of all elements of input sequence with one another. Reduces the path b/w first and last element of a sequence to O(1)\n\n\nWhat is the problem with convolution that Transformer solved?\nOR\n\n\nWhy Transformers is better than CNNs?\nThe problem with convolution is similar to that of recurrence or sequential nature of RNNs. Think from the perspective of convolutions in CNN. 2 things:\n\nCOnvolutional layers in start of the CNN capture very basic details like edges, etc. from a very small local portion of the image but conv layers towards the end of a CNN captures information regarding complex shapes (features) spread over a large portion of image area.\nIf we want to increase the receptive field, we need to add more layers and with more layers, the receptive field increases as we add more layers.\n\nBoth of these implies that in convolutional neural networks, the number of operations required to relate signals from two arbitrary input or output positions increase as distance between them increases. (This is similar to RNN where impact of \\(h_{10}\\) on \\(h_{11}\\) will definitely be more than \\(h_{2}\\)). This makes convolution and recurrence both unable to learn long-term dependency aka global context.\nBasically, in convolutional or recurrent models relation between two tokens depends on their distance (linearly/logarithmically). Transformers use self-attention, which lets every token directly attend to every other token — no matter how far apart. This means relating any two positions takes O(1) operations in terms of sequence distance — it’s distance-independent.\n\n\nBenefit of Transformer over previous sequence based methods\n\nCan handle variable-length sequences\nParallelizability, meaning more computationally efficient\nShorter training times\nCapability to capture long-term context\n\n\n\nExplain Encoder-Decoder Structure in Transformers\nEncoder - responsible for processing input sequence - stack of six identical layers where each layer had 2 sublayers - Multi-Head Self-Attention - FFN - LayerNorm + Residual connection around each sublayer - Encoder creates context-rich representation of input sequence\nDecoder - responsible for generating output sequence based on encoded information - stack of six identical layers where each layer had 2 sublayers - Masked Multi-Head Self-Attention - Cross-Attention (aka Multi-Head Attention on output of encoder stack) - FFN - LayerNorm + Residual connection around each sublayer - Decoder processes the output sequence step by step, attending to different parts of the input sequence as needed.\nNote: Though individual layers (either in Encoder or Decoder) were identical in terms of architecture but the weights were not shared across those layers.\n\n\nRole of FFN in Decoder or Encoder\n\n\nLayerNorm vs BatchNorm in Transformers?\nIn NLP tasks, the sentence length often varies – thus, if using batchnorm, it would be uncertain what would be the appropriate normalization constant (the total number of elements to divide by during normalization) to use. Different batches would have different normalization constants which leads to instability during the course of training.\nIn LayerNorm, for a (B, T, C) tensor, the mean and variance is computed across the channel/embedding (C) dimension for each position (T) and for each sample in batch (B). This results in (B * T) different means and variances. The normalization is applied independently to each sample across all the channels/embeddings (C). RMSNorm operates similarly to LayerNorm but only computes the root mean square (RMS) across the channel/embedding (C) dimension for each position (T) and for each sample in batch (B). This results in (B * T) different RMS values. The normalization is applied by dividing each sample’s activations by its RMS value, without subtracting the mean, making it computationally more efficient than LayerNorm.\nSince BatchNorm computes the mean and variance across the batch dimension and depends on batch size, it is not used in transformers due to variable sequence lengths in NLP. It requires storing the running mean and variance for each feature, which is memory-intensive for large models. Also, during distributed training, batch statistics need to be synced across multiple GPUs. LayerNorm is preferred not just in NLP but even in vision based transformers because it normalizes each sample independently, making it invariant to sequence length and batch size. RMSNorm operates in a very similar manner to LayerNorm but is more computationally efficient (since, unlike LayerNorm, mean subtraction is not performed and only RMS values are calculated) and can potentially lead to quicker convergence during training.\n\n\nWhy multiple layers were stacked? Why not use single large layer?\nStacking allows model to learn heirarchical features. Lower layers capturing local information and higher layers capturing more abstract and global information.\n\n\nQ: What is Dot-Product Attention vs Scaled Dot-Product Attention?\nDot-Product Attention is commonly called multiplicative attention. It is identical to Scaled Dot-product attention explained above except that it does not peform scaling with \\(\\sqrt{d_k}\\). In such cases, what happens is that for large dimension \\(d_k\\) values, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, they scaled the dot products by \\(1/\\sqrt{d_k}\\)\n\n\nQ: What is additive attention? Prior research suggests it works better than multiplicative attention. The why authors did not use it?\nAdditive attention computes the compatibility function using a feed-forward network witha single hidden layer. While the Additive and Multiplicative are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\nFYI, compatibility function in scaled dot-product attention = \nAlso, note that additive works better than multiplicative attention (vanilla w/o scaling) for large dimension \\(d_k\\) values only because of the probable small gradients in vanilla multiplicative attention (as explained in para above). Whereas for small \\(d_k\\) values, both additive and dot-product are equally well.\nNow, having said that researchers found that using scaling, the multiplicative attention could be significantly improved for large \\(d_k\\) and they called it scaled dot-product attention.\n\n\nExplain what is Multi-Head Self-Attention?\n OR \n\n\nWhy multi-head and why not single-head Attention was/is used?\n\nIn Attention mechanism, each token’s new representation is a weighted average of all other tokens’ representations.\nThis averaging blurs the fine-grained differences between individual token positions — i.e., “resolution” is reduced. (Think of it like blurring a high-res image by averaging pixel values — you get a smoother result, but lose detail.)\nTo address this blurring/reduced resolution, the Transformer uses Multi-Head Attention.\nInstead of computing a single attention, it computes multiple attention mechanisms in parallel, each with different parameter sets. This allows the model to capture diverse types of dependencies (e.g., short-range, long-range, syntactic, semantic), and combine them — restoring fine-grained details.\nIn other words, “Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this”\nMoreover, due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality. In other words, total computational cost with single head is same as multi-head (because we reduce the dimension of each head) and end up learning more and better features for the input.\n\n\n\nWhy Positional Encoding?\nSince the Transformer model when processing a sequence contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. - To this end, authors added “positional encodings” to the input embeddings at the bottoms of the encoder and decoder stacks.\n\n\nWhy Sine/ Cosine Positional Encoding?\n\nBecause for any fixed offset k, \\(PE_{pos+k}\\) can be represented as a linear function of \\(PE_{pos}\\) .\nsin/cos functions are shift-invariant — you can express sin(pos + k) as a linear combination of sin(pos) and cos(pos). That’s why it’s easier for the model to learn relative positions even though it’s only given absolute positions.\n\nsin(pos+k)=sin(pos)cos(k)+cos(pos)sin(k)\n\nThese frequencies form a geometric progression — i.e., they change exponentially.\n\nSmall i = long wavelength (slow variation → global info)\nLarge i = short wavelength (fast variation → local info)\n\n\n\n\nWhy fixed (i.e. sine/cosine) and not a learned positional Encoding?\n\nsinusodial positional encoding may allow for extrapolation when we encounter sequence of lengths greater than those encountered during training\n\n\n\nDo we use masking only for Decoder?\n\nNo. Masking can also be used for encoder.\nFor example: when processing multiple sentences of unequal length in batches then, it is common to use a [PAD] token. Here, to avoid attention to unnecessary padding tokens, we can use masking.\n'i hate this so much! [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]' is different from 'i hate this so much!'. In order to force NN to treat both as same, we provide masking to pad tokens.\n(Reference: HF NLP course Ch-2, Handling multiple sequences)\n\n\n\nWhat is the role of CLS token?\n\nA special token that marks the beginning of the sentence.\nIt can be used for encoders\nIt can be used in decoders where we can make CLS token the default token during the start of generation process.\n\n\n\nLoss for training Transformers?\n\n\nLoss for training a multi-modal model like SAM?\n\n\nSAM\n\n\nUsually input to model is a batch but how Transformers handle variable input sequence size?\n\n\nStrategies/Methods to both pretrain and finetune LLMs/ VLMs/Image Models\n\n\nMulti-task learnig? Why we do that?\n\n\nWhy multiple losses?\n\n\nPractices for xAI in multi-modal model?\n\n\nHow to tackle memory constraints in Transformers? (Amii)\n\n\nDifference between transduction and induction problems\nExamples of:\nTransduction problems - language modeling and machine translation\nInduction problems -\n GPT vs BERT vs BART \n\n\nResources\n\nhttps://medium.com/@abhinavbhartigoml/unraveling-transformers-a-deep-dive-into-self-attention-and-3e37dc875bea"
  },
  {
    "objectID": "posts/2025-06-04-yolo/yolo.html",
    "href": "posts/2025-06-04-yolo/yolo.html",
    "title": "YOLO Made Easy",
    "section": "",
    "text": "History of Object Detection\nTimeline\nYolo Models’ performance with time"
  },
  {
    "objectID": "posts/2025-06-04-yolo/yolo.html#yolo-v1-1",
    "href": "posts/2025-06-04-yolo/yolo.html#yolo-v1-1",
    "title": "YOLO Made Easy",
    "section": "YOLO V1",
    "text": "YOLO V1\nOfficial Paper\n\nA. Labeling the data:\nIn YOLO v1 the grid size is 7 x 7.\n\nA grid cell is labeled to contain an object only if the center of the bounding box is in it. If the grid cell contains a center, the “objectness” is labeled 1 and 0 otherwise.\n\n\nB. Predictions:\nFor 1 grid cell we predict: 5B + C paramters\nFor whole image divided into S x S grids, we predict: S X S X (5B + C) params\nwhere,\n5: 4 bbox values (x,y,w,h) + 1 (confidence score) : (Formally, confidence = Probab(Object) x IOU(truth,pred). If no object exists in a cell, the confidence scores should be zero. Otherwise confidence score will be equal to the IOU b/w predicted box and the ground truth.)\nB: number of bboxes predicted per grid cell, B=2 (author’s choice) (B is not related to the number of classes.)\nC: number of classes (class probabilities) : (conditional class probabilities, Pr(class_i|Object))\n\nIf we call P(object) the probability there is an object in a box b and P(c|object) the probability that the object is of class c, then the score for the class c in the box b in simply\n\\(P(c) = P(c|\\text{object})\\times P(\\text{object})\\)\n\n\nArchitecture\nFine-details: 1. To improve the speed of the network, they alternated convolutional layers with 3x3 kernel size and convolutional layers with 1x1 kernel size. How?\nAns:\nLet’s, we have a feature map of size 56 x 56 x 192 and we want to apply a convolution layer of 256 filters of kernel size 3 x 3. For each filter, we have:\n\\(56 \\times 56 \\times 192 \\times 3 \\times 3 =  5,419,008 \\text{ computations}\\)\nFor all the filters we have:\n\\(5,419,008 \\times 256 = \\sim 1.4 \\text{B computations}\\)\nvs\nlet’s apply a convolution layer of 128 filters of kernel size 1 x 1 first:\n\\(56 \\times 56 \\times 192 \\times 1 \\times 1 \\times 128 =  77,070,336 \\text{ computations}\\)\nand the resulting feature map is of size 56 x 56 x 128. Now, let’s apply our convolution layer of 256 filters with kernel size 3 x 3\n\\(56 \\times 56 \\times 128 \\times 3 \\times 3 \\times 256=  924,844,032 \\text{ computations}\\)\nAdding 77,070,336 +924,844,032=1\nTherefore applying a 1 x 1 convolution prior to the 3 x 3 convolution reduces the dimensionality of the tensors and saves ~ 0.4B computations.\n\nThe architecture uses Conv, Maxpool and Linear layers such that at the end the output should be 7x7x30. This was done because image was divided into 7x7 grids and for each grid, we are predicting 20 class probabs and 2 bboxes containing 5 values each (4 xywh + 1 conf). This was a customized setting applicable for PASCAL VOC dataset that had 20 classes. (Actualy the last layer is a FC layer not COnv layer. nn.Linear (4096, 7*7*30))\n\n\nTraining\nFor each image we prepare ground truth as 7x7x30 tensor as explained in the start. Model also outputs 7x7x30 tensor then, using the loss explained below the model was trained.\n\n\nLoss\nMean squared error loss for everything!!!\n\nLet’s break loss line by line\n\nThe identity function is 0 when there is no object or the current bounding box isn’t the responsible one. In other words, we only calculate the loss for the best bounding box. So the first line of the equation is the sum of squared differences between the predicted and actual midpoints of the objects in all grid cells which have an object in them and are the responsible bounding box.\nThe second line is the sum of the squared differences between the square roots of the predicted and actual widths and heights in all grid cells which have an object in them. These are square rooted for reasons explained earlier.\nThe third line is just the squared difference between the predicted class probabilities and the actual class probabilities in all cells which have an object.\nThe fourth line is the same but for all cells which don’t have an object in them.\nThe reason these two (3rd and 4th lines) are split up is so that we can multiply the fourth line by the noobj coefficient to punish the model less severely if it misclassifies when there is no object present.\nQ: Why square root used for bbox size loss and not normal dimension?\nA: Because it gives different weights to bboxes of different sizes. We want to make this error larger for smaller bboxes\nLet’s understand by example.\nCase-1 : Actual width = 100, Predicted width = 98\nCase-2 : Actual width = 4, Predicted width = 2\nIn both the case the deviation is same i.e. 2 but the quality of bbox for small bbox (Case-2) is very poor while quality of BBox predicted in Case-1 is really good.\nLoss with normal width\nCase-1 Loss: (100-98)**2 = 4\nCase-2 Loss: (4-2)**2 = 4\nLoss with square-root width\nCase-1 Loss: (10-9.899)**2 = 0.01\nCase-2 Loss: (2-1.414)**2 = 0.343\n\n\n\nNO\n\n\n\n\n\nYES\n\n\n\n\n\n\n\nNMS\n\nFind all the boxes with a high objectness score. This is the level of confidence of the model that there is an object in the grid cell. We typically have a confidence threshold, so any box with a lower score is not considered.\n\n\n\nWe then choose the box with the high confidence scores and remove the boxes that have high overlap i.e. IoU with the the highest confidence bbox. (Overlap is decided by IOU IoU)\n\n\nExact Algo\nSuppose we have B: bboxes preds, C_thresh: confidence thresh (objectness score) for each bbox\niou_thresh: iou overlap threshold\n\nFilter bboxes with confidence &gt; C_thresh. Suppose now B -&gt; updated B\nSort bboxes in B_new by confidence scores in descending order\nInitialize a new variable F_bboxes to store final bboxes\nwhile B_new is not empty:\n4.1 for b1 bbox in B_new\n4.2 Add b1 to F_bboxes\n4.3 Remove b1 from B_new\n4.4 for remaining b2 in B_new:\n4.4.1 if IoU(b1, b2) &gt;= iou_thresh -&gt; remove b2 from B_new\n\n# Suppose B contains BBoxes as list of list of 5 coordinates (x,y,w,h,c) : where c is confidence (objectness score)\n\nB = sorted(B, key=lambda x: x[4], reverse=True) # initial_bboxes\nF = [] # final_bboxes\n\nwhile len(B)!=0:\n    b1 = B.pop(0)\n    F.append(b1)\n\n    remaining_bboxes = []\n    for b2 in B:\n        if iou(b1, b2) &lt; iou_thresh:\n            remaining_bboxes.append(b2)\n    B = remaining_bboxes\n\n\nProblems:\n1.1 Did not account for this Edge case: In the case the grid cell contains 2 objects, suppose cat and a dog, we need to choose one of the classes as the label for the training data.\n1.2 YOLO can only predict a limited number of bounding boxes per grid cell, 2 in the original research paper. And though that number can be increased, only one class prediction can be made per cell, limiting the detections when multiple objects appear in a single grid cell. Thus, it struggles with bounding groups of small objects, such as flocks of birds, or multiple small objects of different classes.\n\nYOLO imposes strong spatial constraints on bounding box predictions since each grid cell only predicts two boxes and can only have one class. This spatial constraint limits the number of nearby objects that our model can predict.\nThe model struggled with small objects that appear in groups, such as flocks of birds\nTHeir loss function treats errors the same in small bounding boxes versus large bounding boxes. A small error in a large box is generally benign but a small error in a small box has a much greater effect on IOU\nDetections at multi-scale not supported."
  },
  {
    "objectID": "posts/2025-06-04-yolo/yolo.html#yolov2-aka-yolo9000",
    "href": "posts/2025-06-04-yolo/yolo.html#yolov2-aka-yolo9000",
    "title": "YOLO Made Easy",
    "section": "YOLOv2 (aka YOLO9000)",
    "text": "YOLOv2 (aka YOLO9000)\nOfficial Paper\n\nWhat’s different from YOLOv1?\nIntroduced Anchor Boxes: This allows the algorithm to handle a wider range of object sizes and aspect ratios. Used BatchNorm Expanded dataset: PASCAL VOC + COCO Flexible Image Size: 320x320 to 608x608: Multi-scale training. This helps to improve the detection performance of small objects.\nIn YOLOv2 (and beyond), instead of predicting arbitrary bounding boxes directly, the model predicts offsets from a fixed set of predefined boxes, called anchor boxes or prior boxes.\nThe different classes are this time passed through a Softmax activation function and the loss function is using cross-entropy instead of MSE.\nGrid size = 13x13 to ensure better detection of smaller objects\n\n\nAnchor Boxes\nAnchor Boxes: Sort of priors about object shapes; they guide the model in doing better predictions.\nInstead of directly predicting the x & y i.e. bbox centers, the model predicts tx and ty such that:\n\\(x = \\sigma(t_x)\\)\n\\(y = \\sigma(t_y)\\)\nwhere, \\(\\sigma\\) is sigmoid: \\(\\sigma(a) = \\frac{1}{1 + \\exp - a}\\)\nTo get width bw and bh for the bbox, the model predicts tw and th such that\n\\(b_w = p_w e^{t_w}\\)\n\\(b_h = p_h e^{t_h}\\)\nwhere, \\(p_w\\) and \\(p_h\\) are already known when we calculated anchor boxes (explained later).\n\nThe model predicts to (objectness score) also such that:\n\\(P(\\text{object})= \\sigma(t_o)\\)\nQ: Why \\(\\sigma\\)?\nA: Makes x and y bounded between 0 and 1 -&gt; center is always within the cell\n\nHow to calculate anchor boxes?\nK-means algorithm with K=5 (i.e. compute 5 anchors) over the trainig data, clustering together similar shapes.\nSteps:\n\nThey ran K-Means clustering on the bounding boxes in the training set.\nEach bounding box is treated as a 2D point: [w,h]\nTraditional K-Means uses Euclidean distance, but that doesn’t work well for boxes (e.g., a tall-skinny box and short-wide box can have the same area and Euclidean norm but behave very differently).\nInstead, they used a custom distance function:\n\nd=1−IoU(box, cluster center)\nSo two boxes are close if their IoU is high.\n\nAfter clustering:\n5.1 They used the average width and height of each cluster to define an anchor box.\n5.2 They selected K = 5, meaning each grid cell has 5 anchor boxes it can predict from.\nModel Output:\n\nFor each anchor box, YOLOv2 predicts:\n[tx, ty, tw, th, to] Where:\ntx,ty : offsets from the cell\ntw ,th: log-scale offset from anchor box dimensions\nto: objectness score\nNOTES:\n\nNormalize bounding box widths and heights to [0, 1] before clustering.\n\n\n\nWhy is this better than YOLOv1?\nAns: YOLOv1 predicted 2 arbitrary boxes per grid cell, regardless of dataset statistics.\nYOLOv2 learns K well-distributed, representative shapes — making the model better at fitting real-world object shapes.\n\n\n\nArchitecture\nVariant of vgg\nDarkNet-19: Similar to Yolov1, but had BatchNorm for regularization, replaced last linear layer with conv layer"
  },
  {
    "objectID": "posts/2025-06-04-yolo/yolo.html#yolov3",
    "href": "posts/2025-06-04-yolo/yolo.html#yolov3",
    "title": "YOLO Made Easy",
    "section": "YOLOv3",
    "text": "YOLOv3\nOriginal Paper\nWhat’s different from YOLOv2?\nAns: 1. Introduced Residual connections 2. Multi-scale predictions: 13x13, 26x26, 52x52 3. At each scale, 3 bboxes are predicted\n\nThis multi-scale module is going to be referred to as “neck“\n\nAnchor Boxes\nAt each scale 3 anchor boxes so, total 9 anchor boxes were used in Yolov3.\n\n\nArchitecture (Backbone + Neck)\nVariant of ResNet\n\nAt each scale, we predict 3 boxes (9 total). For each box, we predict [tx, ty, tw, th, to] as well as the probabilities of the different classes. If C is the number of classes, for each scale we predict:\n\\(N\\times N \\times 3 \\times (5 + C) \\text{ parameters}\\)\nThis multi-scale module is going to be referred to as “neck“\n\n\nTraining:\nClasses and the objectness score are activated by logistic regression and optimized using cross-entropy\n\\(L_{\\text{objectness}}=\\sum_{i \\in \\text{grid}}\\sum_{j \\in \\text{boxes}}C_i\\log\\hat{C}_i\\)\n\\(L_{\\text{classes}} =\\sum_{i \\in \\text{grid}}\\mathbb{I}_{\\{\\text{if object in } i\\}}\\sum_{c\\in \\text{classes}} p_i(c)\\log\\hat{p}_i(c)\\)"
  },
  {
    "objectID": "posts/2025-06-04-yolo/yolo.html#yolov4",
    "href": "posts/2025-06-04-yolo/yolo.html#yolov4",
    "title": "YOLO Made Easy",
    "section": "YOLOv4",
    "text": "YOLOv4\nOriginal Paper\nQ: What’s new from YOLOv3?\nAns:\n\nFormally introduced network in 3 parts: Backbone, Neck, Head\nLeakyReLU replaced by Mish activation\nResidual blocks are replaced by Cross Stage Partial (CSP) blocks\nAbility to detect even smaller sized objects: 19 x 19, 38 x 38, and 76 x 76 grids\nDid a thorough evaluation of activation functions, bbox regression loss, data augmentation strategies, regularization methods, skip-connections, etc. in order to optimize the model\nIntroduced IoU loss (CIoU loss) for regressing bbox coordinates.\n\n\nArchitecture\n\n\nBackbone: learns the feature map representations of the image\nNeck: that is the network that improves the receptive field of the backbone and allows for multi-scale predictions.\nHead: that is the end of the model that is responsible for making the predictions.\n\n\nBackbone\nResidual Blocks from YOLOv3 replaced by CSP Blocks\n\n\n\nYOLOv3 Residual Block\n\n\n\n\n\nYOLOv4 CSP Block\n\n\n\n\nAnd replaced leakyrelu by mish\n\n\n\nNeck\nNeck is composed of a Spatial pyramid pooling (SPP) and a Path aggregation network (PANet).\n\nSPP - helps with image inputs of different sizes and resolution.\nPANet - is the network used to enable multi-scale predictions. The grid sizes are now 19 x 19, 38 x 38, and 76 x 76 allowing to detect very small objects.\n\n\n\nYOLOv4 SPP\n\n\n\n\n\nYOLOv4 PANet\n\n\n\n\n\nTraining\nIoU loss\n\\(L_{\\text{position-shape}} = 1 - IoU(\\text{pred}, \\text{truth})\\)\nHowever, this loss only works when the bounding boxes overlap, and would not provide any moving gradient for non-overlapping cases. This is resolved by adding a penalty term, capturing the distance between the bounding box centers:\nCIoU Complete IoU loss\n\\(L_{\\text{position-shape}} = 1 - IoU(\\text{pred}, \\text{truth}) + R(\\text{pred}, \\text{truth})\\)\nwhere\n\\(R(\\text{pred}, \\text{truth}) =\\frac{\\rho^2(b, b^{th})}{c^2} + \\alpha v\\)\nwhere ⍴(b, bth) is the Euclidean distance between the bounding box centers, c is the diagonal length of the smallest enclosing box covering the two boxes, and v imposes the consistency of the aspect ratio.\nResources:\n\nhttps://arxiv.org/pdf/1506.02640 (YOLOv1)\nhttps://arxiv.org/pdf/1612.08242 (YOLOv2)\nhttps://arxiv.org/pdf/1804.02767 (YOLOv3)\nhttps://arxiv.org/pdf/2004.10934 (YOLOv4)\nhttps://arxiv.org/pdf/2304.00501 (Comparisons from YOLOv1 to YOLOv8)\nhttps://newsletter.theaiedge.io/p/deep-dive-how-yolo-works-part-1-from (Summary from YOLOv1 to YOLOv4)\nhttps://www.v7labs.com/blog/yolo-object-detection (Summary from YOLOv1 to YOLOv7)\nhttps://medium.com/analytics-vidhya/yolo-explained-5b6f4564f31 (Summary of Yolo v1)\n\nQuestions\n\n1D convolution\n1x1 kernel size convolution\nwhy 1x1 and not 3x3 ?\n\nAns: Speeds up computation. For each prediction in 3x3, we need to perform 9 computations. whereas for 1x1 only 1 computation is required.\n\nYolov1 architecture\nWhy once? How once? Why grids? What adv? What disadv?\nIoU loss? Modern yolo loss vs old yolo losses?\nNMS code\nWhy LeakyRelu?\nWhy replace LeakyRelu by Mish activations?\n\n\n\nClass label smoothing?\nProblem with IoU loss? and solutions around it?\nExact difference in training pipeline of 2 stage vs 1 stage object detection\nCode for Anchor Boxes with K-Means?"
  },
  {
    "objectID": "posts/2025-05-20-hash-map/Hash Table.html",
    "href": "posts/2025-05-20-hash-map/Hash Table.html",
    "title": "Hash Table Made Easy",
    "section": "",
    "text": "In this notebook, we will understand and build Hash Table aka Dictionary in Python from scratch. We will build it step-by-step after understanding its core concepts.\n\n# type hints\nfrom typing import List, Union\n\nQ. Why do we need Dictionary or a Hash map? Why not simply use array?\nAnswer: 2 reasons\n\nSearching in array has O(n) time-complexity whereas searching in dictionary has a constant O(1) time-complexity.\n\n\n\nAnother reason for using Hash table/ Hash map is that it make its use more intuitive. For example: in dictionary to find price on march 6, simply write stock_prices['march 6'] whereas, if we are using array, we need to write a for loop as shown above. It makes the code complex, inefficient and not super-intuitive to write.\n\n\nHash function\nIn Hash Table, we use a Hash Function - which converts a string key into a index value (integer).\nFor example: march 6 is converted to 9 by using sum of its ascii values and its mod with 10 as the hash function (assuming we are making a dictionary contaning only 10 elements). In Python, we can get ascii value of a character by using ord\n\nLet’s write a code to do this i.e. convert a string key into an index\n\ndef get_hash(key):\n    total = 0\n    for char in key:\n        total+=ord(char)\n    return total%10 # assuming 10 is the size of array\n\nprint('march 6: hashes into the index value = ', get_hash('march 6'))\n\nmarch 6: hashes into the index value =  9\n\n\nGreat! it works. Now, we can try writing the code for HashTable class.\n\n\nHashTable\nBefore diving into the code, it’s a good idea to pause for a moment and think what different methods/functionalities we want our HashTableto support. Once we have a clear understanding of these requirements, writing the code becomes much simpler.\nThroughout this blog, I will include visual diagrams to help illustrate what we’re aiming to build with the HashTable. These images will serve as mental maps, making it easier to write and understand the code. (Being able to visualize how data structures work is key to mastering them. Once you can visualize them, you’ll be able to code them effortlessly—without memorizing a single line.)\nQ. What functions we want in HashTable?\nAnswer: \n\nWe want to add a key-value pair\nWe want to get value by supplying key\nWe want to remove by supplying key\n\nTo implement any of the above functionalities, we’ll first need a way to hash a key into an integer index—since computers operate on numbers, not strings. (Technically, computers can handle strings, but only by converting them to numerical codes, which is abstracted away from us.)\nOnce we have this index, we’ll use it to internally manipulate data in the array to achieve the desired functionality.\nBased on the information presented above, the simplest signature of HashTable would look like this:\nclass HashTable:\n    def __init__(self):\n        pass\n\n    def get_hash(self, key: str):\n        \"\"\" converts a key into index (int) \"\"\"\n        pass\n\n    def add(self, key: str, value: Union[str, int]):\n        \"\"\" adds a key-value pair \"\"\"\n        pass\n\n    def get(self, key: str):\n        \"\"\" outputs value given a key \"\"\"\n        pass\n\n    def remove(self, key:str):\n        \"\"\" removes key-value given a key \"\"\"\n        pass\nLet’s start coding it.\nWe will try to test it for a simple case illustrated below:\n\n\nclass HashTable:\n    def __init__(self):\n        self.MAX = 10 # assume 10 is the size of dictionary\n        self.arr = [None for i in range(self.MAX)] # allocate memory\n\n    def get_hash(self, key: str):\n        \"\"\" converts a key into index (int) \"\"\"\n        total = 0\n        for char in key:\n            total+=ord(char)\n        return total%self.MAX\n\n    def add(self, key: str, value: Union[str, int]):\n        \"\"\" adds a key-value pair \"\"\"\n        idx = self.get_hash(key) # get the index for a key\n        self.arr[idx] = value    # insert value at the calculated index\n\n    def get(self, key: str):\n        \"\"\" outputs value given a key \"\"\"\n        idx = self.get_hash(key) # get the index for a key\n        return self.arr[idx]     # return the value at the calculated index\n\n    def remove(self, key:str):\n        \"\"\" removes value given a key \"\"\"\n        idx = self.get_hash(key) # get the index for a key\n        self.arr[idx] = None     # set the value to None at the calculated index\n\n\n# Let's test the code we created so far \nht = HashTable()\nprint('array on initialization', ht.arr)\n# add values\nht.add('march 6', 310)\nht.add('march 7', 340)\nht.add('march 8', 380)\nprint('after adding data', ht.arr)\n# get value\nprint('value on march 6 is: ', ht.get('march 6'))\n# remove value\nht.remove('march 8')\nprint('after removal: value on march 8 is: ', ht.get('march 8'))\n\narray on initialization [None, None, None, None, None, None, None, None, None, None]\nafter adding data [340, 380, None, None, None, None, None, None, None, 310]\nvalue on march 6 is:  310\nafter removal: value on march 8 is:  None\n\n\nGreat! this works as expected! Now let’s do testing on edge cases.\n\n# Edge case-1: get value for a non-existent key\nprint('value on march 17 is: ', ht.get('march 17'))\n\nvalue on march 17 is:  310\n\n\nOops!\nWhy are we getting the value for a non-existent key like march 17 to be the same as the value for march 6?\nAnswer: This happened because both march 17 and march 6 produced the same index - 9 - when passed through our get_hash function. And, since our get method relies solely on this index and ddoesn’t verify whether the actual string key exists at that index, it ends up returning an incorrect value.\nNote: In fact, there can be multiple string keys that can generate the same hash value (in this case, 9) when processed by our hash function, get_hash.This phenomenon is known as a hash collision, and it’s the reason why we got a misleading result for march 17.\n\n# Edge case-2: add a new key-value pair\nht.add('march 17', 459)\n# Now get the value of march 6\nprint('value on march 6 is: ', ht.get('march 6'))\n\nvalue on march 6 is:  459\n\n\nOops!\nWhy are we getting a wrong value for march 6? Initially it was 310 and now it comes out as 459. Why?\nAnswer: This happened because of the same index problem that we discussed above. Since the index of both march 17 and march 6 is 9, when we added march 17’s data, it overwrote at index 9 (which was for march 6) with the new valu the new value of 459 and removed 310. That’s why we don’t see the original value. As mentioned previously, this is called as Collision\n\nTo solve both these challenges (Edge cases 1 & 2), we need to modify our code to start storing the key along with value as a tuple., We can do this by initializing the array as an array of arrays (or lists).\nBut before we implement this change, let’s first refactor our code to use Python’s standard operators like __getitem__, __setitem__, and __delitem__ to make use of HashTable more intuitive.\n\nclass HashTable:\n    def __init__(self):\n        self.MAX = 10 # assume 10 is the size of dictionary\n        self.arr = [None for i in range(self.MAX)] # allocate memory\n\n    def get_hash(self, key: str):\n        \"\"\" converts a key into index (int) \"\"\"\n        total = 0\n        for char in key:\n            total+=ord(char)\n        return total%self.MAX\n\n    def __setitem__(self, key: str, value: Union[str, int]):\n        \"\"\" adds a key-value pair \"\"\"\n        idx = self.get_hash(key) # get the index for a key\n        self.arr[idx] = value    # insert value at the calculated index\n\n    def __getitem__(self, key: str):\n        \"\"\" outputs value given a key \"\"\"\n        idx = self.get_hash(key) # get the index for a key\n        return self.arr[idx]     # return the value at the calculated index\n\n    def __delitem__(self, key:str):\n        \"\"\" removes value given a key \"\"\"\n        idx = self.get_hash(key) # get the index for a key\n        del self.arr[idx]     # delete value at the calculated index\n\n\n# Let's test the code we created so far \nht = HashTable()\n# add values\nht['march 6'] = 310\nht['march 7'] = 340\nprint('after adding data', ht.arr)\n# get value\nprint('value on march 6 is: ', ht['march 6'])\n# remove value\ndel ht['march 7']\nprint('after removal of march 7: ', ht.arr)\n\nafter adding data [340, None, None, None, None, None, None, None, None, 310]\nvalue on march 6 is:  310\nafter removal of march 7:  [None, None, None, None, None, None, None, None, 310]\n\n\n\n\nHandling Collisions\nThere are 2 ways to handle collisions.\n\n1. Chaining:\n\nHere, we do 2 things.\n\nwe store a list at each index instead of single element (as briefly discussed earlier).\nEach element in this list is a tuple of (key, value) pair.\n\nTime complexity to retrieve an element in this case is O(n) in the worst-case, because we may need to scan through all elements in the list at a given index to find the correct key.\n\n\n2. Linear Probing:\n\nInstead of using lists at each index, linear probing handles collisions by searching for the next available empty slot in the array.\nFor eg. in the above image (LEFT), a collision occurs at index 9. To insert the key march 17, we look at the following indices one by one. Since index 9 is already occupied and it’s the end of the array, we wrap around to the beginning. We find that index 1 is empty, so we store the (march 17, 459) pair there, as shown on the RIGHT.\nNow, let’s build hash table first with Chaining technique to handle collision and then Linear Probing.\n\n\nChaining\n\nclass HashTable:\n    def __init__(self):\n        self.MAX = 10 \n        self.arr = [[] for i in range(self.MAX)] # initialize list in each memory location instead of None\n\n    def get_hash(self, key: str):\n        \"\"\" converts a key into index (int) \"\"\"\n        total = 0\n        for char in key:\n            total+=ord(char)\n        return total%self.MAX\n\n    ############################################\n    # # Partially correct but Incorrect 💔 😢\n    #############################################\n    # def __setitem__(self, key: str, value: Union[str, int]):\n    #     \"\"\" adds a key-value pair \"\"\"\n    #     idx = self.get_hash(key)              # get the index for a key\n    #     self.arr[idx].append((key, value))    # append (key, value) tuple to the list\n    ### The problem with above function is - if we add same key twice with different values, it will store both (key, value) tuples.\n    ### Ideally, it should update the old value by the new balue for the repeated key.\n\n    ############################################\n    # # Partially correct but Incorrect 💔 😢\n    #############################################\n    # def __setitem__(self, key: str, value: Union[str, int]):\n    #     \"\"\" adds a key-value pair \"\"\"\n    #     idx = self.get_hash(key) # get the index for a key\n\n    #     # if there is no k-v tuple yet, simply add the k-v tuple\n    #     if len(self.arr[idx])==0:\n    #         self.arr[idx].append((key, value))\n    #         return # STOP\n        \n    #     # else check if the key exist, and if it does then remove the existing k-v tuple and then add new k-v tuple\n    #     existing_keys = []\n    #     for kv in self.arr[idx]:\n    #         k, v = kv\n    #         if k==key:\n    #             v = value # update the value -&gt; Sadly, this won't work. Because this does not change the self.arr\n\n    def __setitem__(self, key: str, value: Union[str, int]):\n        \"\"\" adds a key-value pair \"\"\"\n        idx = self.get_hash(key) # get the index for a key\n\n        # if there is no k-v tuple yet, simply add the k-v tuple\n        if len(self.arr[idx])==0:\n            self.arr[idx].append((key, value))\n            return # STOP\n        \n        # else check if the key exist, and if it does then remove the existing k-v tuple and then add new k-v tuple\n        found = False\n        for i, kv in enumerate(self.arr[idx]):\n            k, v = kv\n            # if key exists (i.e. we need to update the old value by new value), break the loop, delete the tuple at index i \n            if k==key:\n                found = True\n                break\n\n        if found:\n            del self.arr[idx][i] # OR self.arr[idx][i] = (key,val); return\n        \n        # Now add the new k-v tuple\n        self.arr[idx].append((key, value))\n\n\n    def __getitem__(self, key: str):\n        \"\"\" outputs value given a key \"\"\"\n        idx = self.get_hash(key) # get the index for a key\n\n        # if the key exist, return the appropriate value else return None\n        for kv in self.arr[idx]:\n            k, v = kv\n            # if key exists, return the value\n            if k==key:\n                return v\n        # else return None\n        return None     \n\n    def __delitem__(self, key:str):\n        \"\"\" removes value given a key \"\"\"\n        idx = self.get_hash(key) # get the index for a key\n\n        # if the key exist-&gt; delete, else raise error that key not found\n        for i, kv in enumerate(self.arr[idx]):\n            k, v = kv\n            # if key exists, return the value\n            if k==key:\n                del self.arr[idx][i]\n                return # STOP\n            \n        # else raise error\n        print((f'⚠ key = {key} not found')) # OR raise Exception(f'key = {key} not found')\n\n\n# Let's test the code we created so far \nht = HashTable()\nprint('array on initialization', ht.arr)\n# add values\nht['march 6'] = 310\nht['march 7'] = 340\nht['march 17'] = 459\nprint('after adding data', ht.arr)\n\n# Edge case-1: get value for a non-existent key\nprint(\"-- Edge case-1 -- \")\nprint('value on a non-existing key (march 18) is: ', ht['march 18'])\n\n# Edge case-2: Finding value of a key whose hash_mapping is shared across multiple data points\nprint(\"-- Edge case-2 -- \")\nprint('value on march 6 is: ', ht['march 6'])\n\n# try deleting\ndel ht['march 6']\nprint('after deleting an existing key (march 6): ', ht.arr)\n\ndel ht['march 19']\nprint('after deleting a non-existing key (march 19): ', ht.arr)\n\narray on initialization [[], [], [], [], [], [], [], [], [], []]\nafter adding data [[('march 7', 340)], [], [], [], [], [], [], [], [], [('march 6', 310), ('march 17', 459)]]\n-- Edge case-1 -- \nvalue on a non-existing key (march 18) is:  None\n-- Edge case-2 -- \nvalue on march 6 is:  310\nafter deleting an existing key (march 6):  [[('march 7', 340)], [], [], [], [], [], [], [], [], [('march 17', 459)]]\n⚠ key = march 19 not found\nafter deleting a non-existing key (march 19):  [[('march 7', 340)], [], [], [], [], [], [], [], [], [('march 17', 459)]]\n\n\nGreat! this worked as expected.\n\n\n\nExercise-1: Implement hash table where collisions are handled using linear probing.\nYou can use the same example as shown in the image\n\n\n\nShow solution code\nclass HashTable:\n    def __init__(self):\n        self.MAX = 10 \n        self.arr = [None for i in range(self.MAX)] # Initialize the array with None at each index (not a list of lists like in Chaining)\n\n    def get_hash(self, key: str):\n        \"\"\" converts a key into index (int) \"\"\"\n        total = 0\n        for char in key:\n            total+=ord(char)\n        return total%self.MAX\n\n\n    def __setitem__(self, key: str, value: Union[str, int]):\n        \"\"\" adds a key-value pair \"\"\"\n        idx = self.get_hash(key) # get the index for a key\n\n        # if slot is empty i.e. k-v does not exist for idx, insert the (key, value) pair directly\n        if self.arr[idx] is None:\n            self.arr[idx] = (key, value)\n            return # STOP\n        \n        # else if k-v tuple already exists and k is same as key, then update it to new (key, value) tuple\n        elif self.arr[idx][0] == key:\n            self.arr[idx] = (key, value)\n\n        # else if k-v tuple already exists and k is different than key, search for an empty location (i.e. perform linear probing) and insert the (key, value) tuple\n        else: \n            while self.arr[idx] is not None:\n                idx = idx+1\n                idx = idx%self.MAX # to keep index in range from 0 to MAX-1. Also, it makes sure if we are at end, we start again from top\n            # above loop breaks as soon as idx reaches at memory space which is empty. Now insert the (key, value) tuple\n            self.arr[idx] = (key, value)\n\n\n    def __getitem__(self, key: str):\n        \"\"\" outputs value given a key \"\"\"\n        # As such, there is no point of using hash function, get_hash to get idx for a key. Because for collision cases, we are storing (key, value) tuple at next available locations.\n        # So, those cases will not be directly retrievable by indexing at idx. We would require to search through whole array arr to ensure that we do not miss it.\n        # We can do this linear search for any given key without coding the part where we get idx and just simply code the linear search for all key values.\n        # But, we will want to use get_hash to make the program run faster for cases that did not have collision.\n\n        idx = self.get_hash(key)\n        # if key at idx is same as key in the argument\n        if self.arr[idx][0]==key:\n            return self.arr[idx][1]\n        \n        else:\n            # iterate throughout the entire arr linearly, and see if key exists\n            for kv in self.arr:\n                if kv is not None: # if kv is None, we won't be able to index k,v as mentioned in line below\n                    k, v = kv\n                    if k == key:\n                        return v\n                    \n        # Key was not found      \n        print(f\"⚠ key = {key} not found\") # OR raise Exception(f\"key = {key} not found\") \n        \n\n\n    def __delitem__(self, key:str):\n        \"\"\" removes value given a key \"\"\"\n        idx = self.get_hash(key) # get the index for a key\n\n        # if key at idx is same as key in the argument\n        if self.arr[idx][0]==key:\n            del self.arr[idx]\n            return # STOP\n        \n        else:\n            # iterate throughout the entire arr linearly, and see if key exists\n            for i, kv in enumerate(self.arr):\n                if kv is not None:\n                    k, v = kv\n                    if k == key:\n                        del self.arr[i]\n                        return\n            \n        # Key was not found\n        print(f\"⚠ key = {key} not found\") # OR raise Exception(f\"key = {key} not found\") \n\n\n##########################\n# Test the code developed\n##########################\nht = HashTable()\nprint('array on initialization', ht.arr)\n# add values\nht['march 6'] = 310\nht['march 7'] = 340\nht['march 17'] = 459\nprint('after adding data', ht.arr)\n\n# Edge case-1: get value for a non-existent key\nprint(\"-- Edge case-1 -- \")\nprint('value on a non-existing key (march 18) is: ', ht['march 18'])\n\n# Edge case-2: Finding value of a key whose hash_mapping is shared across multiple data points\nprint(\"-- Edge case-2 -- \")\nprint('value on march 6 is: ', ht['march 6'])\nprint('value on march 17 is: ', ht['march 17'])\n\n# try deleting\ndel ht['march 6']\nprint('after deleting an existing key (march 6): ', ht.arr)\n\ndel ht['march 19']\nprint('after deleting a non-existing key (march 19): ', ht.arr)\n\n\narray on initialization [None, None, None, None, None, None, None, None, None, None]\nafter adding data [('march 7', 340), ('march 17', 459), None, None, None, None, None, None, None, ('march 6', 310)]\n-- Edge case-1 -- \n⚠ key = march 18 not found\nvalue on a non-existing key (march 18) is:  None\n-- Edge case-2 -- \nvalue on march 6 is:  310\nvalue on march 17 is:  459\nafter deleting an existing key (march 6):  [('march 7', 340), ('march 17', 459), None, None, None, None, None, None, None]\n⚠ key = march 19 not found\nafter deleting a non-existing key (march 19):  [('march 7', 340), ('march 17', 459), None, None, None, None, None, None, None]\n\n\nWe can notice that march 17 is stored at index 1 same as what is shown in the figure attached with the question. 😎\n\n\nResources:\nCodebasics Lecture 5, Lecture 6 on Hash Table"
  },
  {
    "objectID": "posts/2025-05-18-linear-regression-basics/2_linear_regression.html",
    "href": "posts/2025-05-18-linear-regression-basics/2_linear_regression.html",
    "title": "Linear Regression Made Easy",
    "section": "",
    "text": "In this notebook, we will be building a Linear Regression model from scratch to learn and familiarize ourselves with various governing foundational concepts about it. For this, we will use a sklearn.datasets.make_regression function to create a simple synthetic dataset in one variable. We can extend the concepts learned here to build multi-variate linear regression models.\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_regression\nfrom pprint import pprint\n\n\n# Generate the data\nX,y = make_regression(n_samples=100, n_features=1, noise=7, random_state=42) \n# Here, \n# X.shape: (n_samples, n_features) = (100, 1)\n# y.shape: (n_samples, ) = (100, )\n\nfig, ax = plt.subplots(figsize = (5,3))\nax.scatter(X,y)\nplt.show()\n\n\n\n\n\n\n\n\nWe can see that X axis varies from -2 to 2. It implies that our feature space is already normalized.😄\nHowever, if we are dealing with multiple features, then it is common to have different features in different range. For ex. in a dataset of house price prediction, the house area can range from 100-5000 while number of bedrooms typically range from 1-5. To make the linear regression model to give equal importance to all the features, it is a good practise to bring all the features in same range. So, we normalize the features. There are multiple ways to normalize:\n\nX/max: new feature range is 0 to 1\nX-mean/std: new feature range is -3 to 3 mostly (except outliers)\n(X-min)/(max-min): new feature range is 0 to 1\n\nIn Machine Learning, whenever, we want to build any model, we usually split it into 2 sets - train and val. We build algo on train and finetune its hyperparameters to optimise the loss/error function on val set. This step is mandatory. So, let’s build a helper function\n\ndef train_test_split(X, y, test_size=0.2, random_state=42):\n    \"\"\" splits the data into train and test sets\"\"\"\n\n    np.random.seed(random_state)\n    n = X.shape[0]\n\n    if isinstance(test_size, float) and test_size&lt;1:\n        test_size = int(n*test_size)\n    elif isinstance(test_size, int):\n        pass\n    else:\n        raise ValueError(\"test size must be a float/ int\")\n    \n    shuffled_indices = np.random.permutation(n)\n    test_indices = shuffled_indices[:test_size]\n    train_indices = shuffled_indices[test_size:]\n    X_train = X[train_indices]\n    X_test = X[test_indices]\n    y_train = y[train_indices]\n    y_test = y[test_indices]\n    return X_train, X_test, y_train, y_test\n\n\nX_train, X_val, y_train, y_val = train_test_split(X,y, test_size=20, random_state=0)\nprint(f'Train size: {X_train.shape[0]}, Test size: {X_val.shape[0]}')\n\n# plot the train and val data to see the split\nfig, ax = plt.subplots(figsize = (5,3))\nax.scatter(X_train,y_train, c='blue', label='train')\nax.scatter(X_val,y_val, c='red', label='val')\nax.legend()\nplt.show()\n\nTrain size: 80, Test size: 20\n\n\n\n\n\n\n\n\n\nNow, the question is how do we decide the size of val set and how we should create it?\nAnswer: There is no strict metric on how to decide the size of val set. The most important aspect to consider while creating val set is - it should be a representative of train set. This means, the points in the val set should have a good spread throughout the training data. What does this mean ? - Suppose, all val points (red) occur together and not separated from one another, then it is not a good split. Because this val data does not capture the distribution of train data and since the ultimate use of val data is to optimize the model, then it means we will end up optimizing the model only for a short spread of the data and not the entirety of it. Hence, we mostly perform random sampling to make sure that we get different and spread-out points in the hope that they would be a representative set of the entire training data. To achieve this, we can choose 20% of the data or 30% or 5% depending upon the distrubution of the data we are dealing with. Typical value is 15-25% sampled randomly. (But both the val data percentage and sampling method will vary depending upon the nature of the problem. Read more here )\nAfter creating the splits of train and val data, we can now write code to build a regression model.\n(FYI, if normalization of features is required as mentioned above, then normalization is performed after the data splitting. Various normalization constants are calculated from the train split of the data and stored for processing val and real test data that comes during the production stage.)\nWe are now writing a code for a simplistic model: yhat = wx + b\nwhere w and b are randomly initialized and\noptimized by Gradient Descent\nIn Linear regression, we use mean square error as loss function for optimiztion via Gradient Descent\n\n# hyperparameter\nlr = 0.001\nepochs = 101\n\n# initialize weights and biases randomly\nw = np.random.rand()\nb = np.random.rand()\n\n# Gradient Descent \ntrain_losses = []\nval_losses = []\nfor epoch in range(epochs):\n    # Go through TRAIN data\n    yhat = w*X_train + b\n    yhat = np.squeeze(yhat)\n    # mean square error loss\n    mse_loss = np.mean((y_train-yhat)**2)\n    if (epoch)%20==0:\n        print(f\"EPOCH: {epoch}, Train LOSS:{round(mse_loss, 3)}\")\n    train_losses.append(mse_loss)\n    # step of Gradient Descent\n    w = w - lr*(yhat-y_train)@X_train\n    x0 = np.ones((X_train.shape[0],1))\n    b = b - lr*(yhat-y_train)@x0 # or : b = b - lr * np.sum(yhat - y_train)\n    ## Monitor performance on VAL data\n    yhat = w*X_val + b\n    yhat = np.squeeze(yhat)\n    mse_loss = np.mean((y_val-yhat)**2)\n    if (epoch)%20==0:\n        print(f\"EPOCH: {epoch}, Val LOSS:{round(mse_loss, 3)}\")\n    val_losses.append(mse_loss)\n\nEPOCH: 0, Train LOSS:1567.608\nEPOCH: 0, Val LOSS:1552.406\nEPOCH: 20, Train LOSS:145.819\nEPOCH: 20, Val LOSS:168.799\nEPOCH: 40, Train LOSS:40.83\nEPOCH: 40, Val LOSS:73.231\nEPOCH: 60, Train LOSS:31.699\nEPOCH: 60, Val LOSS:68.759\nEPOCH: 80, Train LOSS:30.857\nEPOCH: 80, Val LOSS:69.459\nEPOCH: 100, Train LOSS:30.777\nEPOCH: 100, Val LOSS:69.843\n\n\n\nfig, ax = plt.subplots(figsize = (5,3))\nax.plot(range(epochs),train_losses, c=\"b\", label = 'train loss')\nax.plot(range(epochs),val_losses, c=\"red\", label = 'val loss')\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\nUsually, you should see decreasing error/loss values. If this does not happen, few things need to be checked: 1. Reduce learning rate, lr and retry 2. Always, check if the shapes of variables are correct. For ex: bias, b shape must be (1,); w: (1, ), mse_loss: (1,), yhat: (num_samples in train/val, ) and so on. I have seen many times, while working with numpy, if you are not careful of matrix multiplication and dot product rules, the shapes of your variables become incorrect causing weird model training. You could also see an up and down behaviour in train loss. For example: If you miss to account for x0 while calculating bias b -&gt; it will cause massive shape issues throughout.\nAnyhow, for our case, we did not encounter any such strange behavior. Let’s see the model we trained:\n\nprint(f\"Linear Regression Model: wx+b = {w}*x+{b}\")\n\nLinear Regression Model: wx+b = [44.0082033]*x+[0.38012626]\n\n\n\nxmin = min(X); yhat_min  = w*xmin+b\nxmax = max(X); yhat_max  = w*xmax+b\n\nfig, ax = plt.subplots(figsize = (5,3))\nax.scatter(X,y, label = 'data')\nax.plot([xmin, xmax], [yhat_min, yhat_max], c=\"red\", label='fitted model')\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe trained model fits really well visually. But how do we quantify the quality of fit?\nAnswer: r2-score\nI often used to forget the formula for r2-score, until I understood the reasoning behind it and then, I no longer needed to memorize it. I could reproduce the formula within seconds by following pure logic. In fact, I feel this is the best way to also sharpen your data understanding skills. Being able to reason about the data stuff and write it in terms of maths - this is the skill that will make you a data scientist with sharp eyes and mind.\n\nr2-score forumla\nLet’s understand the r2-score and derive its formula by asking just basic questions i.e. first principle thinking.\nQ. What is the simplest model we could use without any fancy math?\nA. use mean i.e. the average as the answer, yhat for any given x\nQ. Now, if we use the simplest model, what is the sum of squares of error?\nA. np.sum((y-mean)**2)\nQ. What is the sum of squares of error from our model?\nA. np.sum((y-yhat)**2)\nQ. If we have trained a good model, it should be better than baseline (simplest model, where we predict average no matter what X is). That is, sum of squares of error from trained model &lt; sum of squares of error from simplest model. But how much better?\nA. 1 - np.sum((y-yhat)^2) /np.sum((y-mean)^2)\n\n# simplest model \nymean = np.mean(y_train) # use train to calculate the model, where model = mean\nSST = np.sum((y_val - ymean)**2) # calculte sum of square of error on val data\nprint('SST: ', SST)\n\n# trained model\nyhat = w*X_val + b\nyhat = np.squeeze(yhat)\nRSS = np.sum((y_val-yhat)**2)\nprint('RSS: ', RSS)\n\n# R2-score\nr2_score = 1-RSS/SST\nprint('r2_score: ', r2_score)\n\nSST:  37726.247861497875\nRSS:  1396.867522387115\nr2_score:  0.9629735899653908\n\n\nFor simplest model,\nmodel = ymean, sum of square of error is called SST = Total sum of squares\nFor trained model,\nmodel = wx+b, sum of square of error is called RSS = Sum of square of Residuals\nr2_score close to 1 means the model explains the data well.\nr2_score ranges from 0 to 1. Can you think what what does r² = 0 mean?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI Blogs",
    "section": "",
    "text": "Comprehensive deep dive and implementation of Continuous Bag-of-Words\n\n\n\nNLP\n\n\n\nImplementing Word2Vec Paper\n\n\n\n\n\nAug 3, 2025\n\n\nMohit Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nPytorch Losses Review\n\n\n\nML\n\nDL\n\n\n\nMost commonly used losses in Pytorch\n\n\n\n\n\nJun 18, 2025\n\n\nMohit Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nGradCAM Review\n\n\n\nVision\n\n\n\nA basic overview on the working principle of GradCAM\n\n\n\n\n\nJun 9, 2025\n\n\nMohit Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nTransformers Made Easy (Draft)\n\n\n\nTransformers\n\n\n\nA deep dive into ‘Transformers’\n\n\n\n\n\nJun 8, 2025\n\n\nMohit Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nAttention is all you need (Draft)\n\n\n\nTransformers\n\n\n\nA deep dive into ‘Attention is all you need’\n\n\n\n\n\nJun 6, 2025\n\n\nMohit Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nYOLO Made Easy\n\n\n\nVision\n\n\n\nUnderstand the inner workings of an object detection model, YOLO. The architecture, The losses, The innovations…\n\n\n\n\n\nJun 4, 2025\n\n\nMohit Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nViT Made Easy (Draft)\n\n\n\nVision\n\n\n\nA deep dive into ‘An image is worth 16x16 words’\n\n\n\n\n\nJun 4, 2025\n\n\nMohit Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nStack Made Easy\n\n\n\nDSA\n\n\n\nBuild stack with deque. And, learn why list or array is not ideal for a Stack.\n\n\n\n\n\nMay 21, 2025\n\n\nMohit Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nHash Table Made Easy\n\n\n\nDSA\n\n\n\nCode & Collision Handling in Hash Table.\n\n\n\n\n\nMay 20, 2025\n\n\nMohit Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nLinked List Made Easy\n\n\n\nDSA\n\n\n\nCode & Debug Linked Lists.\n\n\n\n\n\nMay 19, 2025\n\n\nMohit Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression Made Easy\n\n\n\nCore ML\n\n\n\nUnderstanding and implementing core concepts related to Linear Regression.\n\n\n\n\n\nMay 18, 2025\n\n\nMohit Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nDecision Trees Made Easy\n\n\n\nCore ML\n\n\n\nUnderstanding and implementing core concepts related to decision trees.\n\n\n\n\n\nMay 1, 2025\n\n\nMohit Gupta\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Mohit Gupta is a reseacher with a Ph.D. in Computer Vision from Arizona State University. Check out his website to know more about him."
  },
  {
    "objectID": "posts/2025-05-04-decision-trees-basics/1_basic_decision_tree_for_classification.html",
    "href": "posts/2025-05-04-decision-trees-basics/1_basic_decision_tree_for_classification.html",
    "title": "Decision Trees Made Easy",
    "section": "",
    "text": "In this notebook, we will be building a basic decision tree to learn and familiarize ourselves with various governing foundational concepts about decision trees. For this, we will use a relatively simple dataframe containing purely numerical features for the task of classification. As our understanding about decision trees expands, we will extend the concepts learned here to handle complex and challenging dataframes (such as those with missing values, with categorical features, etc.)\nLet’s load the popular iris data. (We are using sklearn only and only to load a popular benchmark dataset. We will not use it to build decision trees. Instead, we will build our decision trees entirely from scratch in using numpy)\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom pprint import pprint\n\n\n# load the data\niris_data = load_iris()\nX = iris_data.data\ny = iris_data.target\n\n# Here X and y are numpy arrays. We do a bit of processing to convert X and y into pandas dataframe for readability\nfeature_names = iris_data.feature_names\ntarget_names = iris_data.target_names\n\ndf = pd.DataFrame(X, columns=feature_names) # start df with features i.e. X\ndf['target'] = y                            # add target column to df i.e. y\n\ndf['target'] = df['target'].apply(lambda x: target_names[x]) # Optional: convert integer labels to actual class names\ndf.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\nNow, we will build a decision tree which when given 4 numerical feature values will classify the plant species as one of 'setosa', 'versicolor', 'virginica'.\nFor this, we first need to split the data into train and val sets. Then we will use the train set to build the model and test it on the val set.\n\ndef train_test_split_df(df, test_size=0.2, random_state=42):\n    \"\"\" splits the data into train and test sets\"\"\"\n\n    if isinstance(test_size, float) and test_size&lt;1:\n        test_size = int(len(df)*test_size)\n    elif isinstance(test_size, int):\n        pass\n    else:\n        raise ValueError(\"test size must be a float/ int\")\n    \n    shuffled_indices = np.random.permutation(len(df))\n    test_indices = shuffled_indices[:test_size]\n    test_df = df.iloc[test_indices]\n    train_df = df.drop(test_indices)\n    return train_df, test_df\n\n\ntrain_df, test_df = train_test_split_df(df, test_size=20, random_state=0)\nprint(f'Train size: {len(train_df)}, Test size: {len(test_df)}')\ntrain_df.head()\n\nTrain size: 130, Test size: 20\n\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n\n\n\n\n\nTo build a decision tree, the dataset is split into 2 subsets using a condition - where one subset satisfies the condition and another does not.\nThe condition is nothing but thresholding of one the features. (For example, condition: sepal lenghth &lt;= 4.4 cms will split the dataset into 2 subsets - one where sepal lenghth is less than 4.4 cms and another where it is not)\nThe main question here is how we decide which feature to use and what threshold to pick?\nAnswer: Pick the feature which results in most information gain.\nNow to understand the above term information gain, we need to first familiarize ourselves with few more concepts.\n\nPurity - Measure of homogenity of a subset. For ex: if a data subset contains only red balls - it is pure. But if it contains even a single green ball - it is now impure. The extent of impurity is measured using entropy.\nEntropy - calculated as:\n  H(X) = — Σ (pi * log2 pi)\n\nwhere; X = Total number of samples, and, pi is the probability of class i\nPure dataset: - One which contains elements belonging to single class. For e.g.: All red balls - For a pure dataset: entropy = 0\nMost Impure dataset: - One which contains elements distributed equally among other classes. For e.g.: 10 blue balls and 10 red balls - For the most impure subset: entropy = 1\n\nInformation Gain - If we split the dataset, then the entropy (i.e. degree of impurity) in the children subsets should ideally be lower. This reduction in entropy is the information gain.\n Information Gain = entropy (parent) — [average entropy of ( children)]\n\nNow, based on the above concepts, we will split the dataset, using the feature that results in highest information gain.\nAnd how we do this?\nAnswer: Brute Force. Yes, Brute Force… We go through all the feature columns one at a time and for each feature column, we go through all of their possible values one at a time, splitting the data into 2 children nodes, calculating information gain, storing it and then pick one which gave us the highest information gain.\nWe repeat this process, either a fixed number of times or until the entropy of children becomes zero (i.e. children subsets become pure) and then we stop.\nLet’s see this step by step…\n\n# let's check the purity of our current data\n\ndef check_purity(df: pd.DataFrame) -&gt; bool:\n    \"\"\"\n    Check if data is pure.\n    \"\"\"\n    y = df.iloc[:, -1]\n    return len(y.unique())==1\n\nprint('Is the train data pure? -&gt;', check_purity(train_df))\n\nIs the train data pure? -&gt; False\n\n\nAs it is not pure. Let’s see how much impure it is by calculating entropy.\n\ndef calculate_entropy(df: pd.DataFrame) -&gt; float:\n    \"\"\"\n    Calculates the entropy of the data. Entropy =  — Σ (pi * log2 pi)\n    \"\"\"\n    y = df.iloc[:, -1]\n    values, counts = np.unique(y, return_counts=True)\n    probs = counts/len(y)\n    entropy = -np.sum(probs*np.log2(probs))\n    return entropy\n\nparent_entropy = calculate_entropy(train_df)\nprint(f\"Entropy of parent node: {parent_entropy}\")\n\nEntropy of parent node: 1.5848773505329046\n\n\nNow, let’s see the details of how we will split the data. The code below should be very straightforward and intuitive to understand.\nAs mentioned, now (1) we will go through all features and their all possible values to split the dataset and then (2)check the information gain. SO, let’s first create the function called get_potential_splits that does (1) for us and then we will implement determine_best_split that will do (2) for us.\n\n# Function (1)\ndef get_potential_splits(df: pd.DataFrame) -&gt; dict:\n    \"\"\"\n    Get all the possible potential splits of the data.\n    \"\"\"\n    potential_splits = {}\n    _, n_columns = df.shape\n    for column_index in range(n_columns - 1): # -1 to skip the target column\n        values = df.iloc[:, column_index]\n        unique_values = np.unique(values)\n        potential_splits[column_index] = [] # initialize a list for storing possible split values per column aka feature\n\n        # using mid-points between 2 consecutive unique values to split the data\n        for i in range(len(unique_values)-1):\n            split_value = (unique_values[i]+unique_values[i+1])/2\n            potential_splits[column_index].append(split_value)\n        \n    return potential_splits\n\n# let's see the potential splits for our data\npotential_splits = get_potential_splits(train_df)\nprint(\"Potential splits for each feature column:\")\nprint(potential_splits)\n\nPotential splits for each feature column:\n{0: [4.35, 4.45, 4.55, 4.65, 4.75, 4.85, 4.95, 5.05, 5.15, 5.25, 5.35, 5.45, 5.55, 5.65, 5.75, 5.85, 5.95, 6.05, 6.15, 6.25, 6.35, 6.45, 6.55, 6.65, 6.75, 6.85, 7.0, 7.15, 7.25, 7.35, 7.5, 7.65], 1: [2.1, 2.25, 2.3499999999999996, 2.45, 2.55, 2.6500000000000004, 2.75, 2.8499999999999996, 2.95, 3.05, 3.1500000000000004, 3.25, 3.3499999999999996, 3.45, 3.55, 3.6500000000000004, 3.75, 3.8499999999999996, 4.0, 4.15], 2: [1.05, 1.2000000000000002, 1.35, 1.45, 1.55, 1.65, 1.7999999999999998, 2.45, 3.15, 3.4, 3.55, 3.6500000000000004, 3.75, 3.8499999999999996, 3.95, 4.05, 4.15, 4.25, 4.35, 4.45, 4.55, 4.65, 4.75, 4.85, 4.95, 5.05, 5.15, 5.25, 5.35, 5.45, 5.55, 5.65, 5.75, 5.85, 5.95, 6.05, 6.199999999999999, 6.449999999999999, 6.65, 6.800000000000001], 3: [0.15000000000000002, 0.25, 0.35, 0.45, 0.55, 0.8, 1.05, 1.15, 1.25, 1.35, 1.45, 1.55, 1.65, 1.75, 1.85, 1.95, 2.05, 2.1500000000000004, 2.25, 2.3499999999999996, 2.45]}\n\n\n0: above is first column i.e. sepal length (cms), 1: is second column and so on.\nNow, we have find all the possible ways to split the data (mentioned as (1))\nNow, let’s see how to get the best split by developing determine_best_split (mentioned as (2)). For this, first create a function (2-1) that will split the data into 2 parts given a feature and its value. Then, we will use this function to split the data for all possible combinations, calculate information gain and pick the one that gives highest information gain (2-2).\n\n# Function (2-1)\ndef split_data(df: pd.DataFrame, split_column: int, split_value: float) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\" \n    SPlit the data into 2 subsets based on split column and split value.\n        split_column (int) : column index\n    \"\"\"\n    split_column_values = df.iloc[:, split_column]\n    left_split = df[split_column_values &lt;= split_value]\n    right_split = df[split_column_values &gt; split_value]\n\n    return left_split, right_split\n\n# Functions (2-2)\ndef determine_best_split(df: pd.DataFrame, potential_splits: dict) -&gt; tuple[int, float]:\n    \"\"\"\n    Determine the best split column and its value.\n    \"\"\"\n    best_split_column = None\n    best_split_value = None\n    best_info_gain = 0\n    parent_entropy = calculate_entropy(df) # Parent Entropy\n    \n    # (Recursive)Iterate over all possible combinations of columns and their split values\n    for column_index in potential_splits.keys():\n        for split_value in potential_splits[column_index]:\n            left_split, right_split = split_data(df, column_index, split_value)\n            average_children_entropy = calculate_average_children_entropy(left_split, right_split)\n            information_gain = parent_entropy - average_children_entropy\n            # print(\"Column:\", iris_data.feature_names[column_index], \"Split value:\", split_value, \"Information gain:\", information_gain)\n            \n            # pick the one with highest information gain\n            if information_gain &gt; best_info_gain:\n                best_info_gain = information_gain\n                best_split_column = column_index\n                best_split_value = split_value\n        \n    #print(\"Best Information Gain:\", best_info_gain)\n    return best_split_column, best_split_value\n\ndef calculate_average_children_entropy(left_split: pd.DataFrame, right_split: pd.DataFrame) -&gt; float:\n    \"\"\"\n    Calculates the overall entropy of the data after splitting i.e. average entropy of the children nodes\n    overall entropy = weighted average of children entropies = Σ (p_c * E(c))\n    \"\"\"\n    n = len(left_split) + len(right_split) # total size of data\n    w_left = len(left_split)/ n            # relative weight of left data\n    w_right = len(right_split)/ n          # relative weight of right data\n    overall_entropy = w_left * calculate_entropy(left_split) + w_right * calculate_entropy(right_split)\n    return overall_entropy\n\nAbove 2 functions should be straightforward to understand. Only new concept is: to get entropy of data after split - we calculate it as average weighted entropy of children nodes.\n(uncomment the print the statement in determine_best_split, if interested to see information gain for all possible splits)\n\nbest_split_column, best_split_value = determine_best_split(train_df, potential_splits)\nprint(\"Best split column:\", iris_data.feature_names[best_split_column], \"with value:\", best_split_value)\n\nBest split column: petal length (cm) with value: 2.45\n\n\nNote: The result one gets could be different if the seed/ random_state in train_val_split_df is changed (because it will change the underlying training data i.e. train_df)\nThe result I got: Best split column: petal length (cm) with value: 2.45\nNow, let’s create the splits\n\nleft_branch = train_df[train_df.iloc[:, best_split_column] &lt;= best_split_value] # branch that satisfies the condition: petal length (cm) &lt;= 2.45\nright_branch = train_df[train_df.iloc[:, best_split_column] &gt; best_split_value]\n\n# # or, we could also do\n# left_branch, right_branch = split_data(train_df, best_split_column, best_split_value)\n\nLet’s verify that after splitting the data has less impurity that is it has less entropy.\n(How do we do that? - By calculating weighted average entropy of children nodes)\n\nprint(f\"Before splitting,  Entropy: {parent_entropy}\")\nprint(f\"After splitting,  Entropy: {calculate_average_children_entropy(left_branch, right_branch)}\")\n\nBefore splitting,  Entropy: 1.5848773505329046\nAfter splitting,  Entropy: 0.6691669882046775\n\n\nSo, this verifies that our splitting was good.\nLet’s see if either of the child node (i.e. data split) is pure\n\ncheck_purity(left_branch), check_purity(right_branch)\n\n(True, False)\n\n\nWow! our left_branch is pure i.e. it contains all the datapoints that has single class. Thus, it would not need any further splitting.\nWhereas right_brach is not pure i.e. it contains datapoints from multiple classes. Thus, it would need further splitting.\n\nnp.unique(left_branch.target) # see classes in left_branch\n\narray(['setosa'], dtype=object)\n\n\n\nnp.unique(right_branch.target, return_counts=True)  # see classes in right_branch\n\n(array(['versicolor', 'virginica'], dtype=object),\n array([43, 44], dtype=int64))\n\n\nBased on just one condition, we can create a small subtree as follows:\nsub_tree = {\"condition\" : [\"left_split_answer\", \"right_split_answer\"]}\nIf we suppose if this small sub-tree is our final decision tree which we want to use for testing. So, if we were to classify one test_example as input, we will check it’s petal length, see if it is less than &lt;= 2.45 cm, we will return setosa as the class, else, we will pick the class with higher relative probability in the right_branch as the predicted class.\n\n# In plain english \nsub_tree = { \"petal length (cm) &lt;= 2.45\" : [\"setosa\", \"versicolor\"]} # right split is versicolor because it is dominant in right_branch\n\nAlmost Never, we create decision tree with only a single condition i.e. 2 child nodes. Usually a decision tree is composed of multiple sub-trees composing multiple conditons.\nProgramatically speaking, we repeat the process of splitting for both the left_branch and right_branch until we reach the stopping condition:\n\nchild nodes become pure\na fixed number of steps by setting hyperparameters like max_depth, min_samples, etc.\n\nNote: There should not be any doubt/confusion when I say that the condition for splitting a left_branch and right_branch belonging to same parent node could be completely different (because it depends upon the data distribution within the child node).\nLet’s build a full-fledged decision tree programatically using concepts of dynamic programming.\n\nDecision Tree code (for case 1. i.e keep splitting until child nodes become pure)N\n(Note: FYI: Case 2. i.e. splitting fo a fixed number of steps is also covered later in this notebook.)\n\ndef decision_tree_algorithm(df: pd.DataFrame) -&gt; dict:\n    data = df\n\n    # base case: If data is pure-&gt; stop and return the class of the child node\n    if check_purity(data):\n        predicted_class = np.unique(data.iloc[:, -1])[0] # only 1 unique value\n        return predicted_class\n    \n    # else: keep on splitting \n    # Recursive\n    else:\n        # for splitting: get_potential_splits -&gt; determine_best_split -&gt; split_data based on best_split_column and best_split_value\n        potential_splits = get_potential_splits(data)\n        best_split_column, best_split_value = determine_best_split(data, potential_splits)\n        left_branch, right_branch = split_data(data, best_split_column, best_split_value)\n\n        \n        condition = \"{} &lt;= {}\".format(list(df.columns)[best_split_column], best_split_value)\n        # create the sub-tree as a dictionary storing the condition as key and a list as the value. This list for a \n        # condition has either the `predicted_class` if the child node is pure or another condition that will further split the \n        # impure child node.\n        sub_tree = {condition: []}\n\n        # get the answer for the 2 child nodes we just created (Step-1) and append them to the sub-tree\n        # (Step-1): get answers\n        left_branch_answer = decision_tree_algorithm(left_branch)\n        right_branch_answer = decision_tree_algorithm(right_branch)\n\n        sub_tree[condition].append(left_branch_answer)\n        sub_tree[condition].append(right_branch_answer)\n\n    return sub_tree\n\n\nmy_tree = decision_tree_algorithm(train_df)\npprint(my_tree)\n\n{'petal length (cm) &lt;= 2.45': ['setosa',\n                               {'petal width (cm) &lt;= 1.75': [{'petal length (cm) &lt;= 4.95': [{'petal width (cm) &lt;= 1.65': ['versicolor',\n                                                                                                                          'virginica']},\n                                                                                            {'petal width (cm) &lt;= 1.55': ['virginica',\n                                                                                                                          {'sepal length (cm) &lt;= 6.95': ['versicolor',\n                                                                                                                                                         'virginica']}]}]},\n                                                             {'petal length (cm) &lt;= 4.85': [{'sepal length (cm) &lt;= 5.95': ['versicolor',\n                                                                                                                           'virginica']},\n                                                                                            'virginica']}]}]}\n\n\nAbove is the decision tree which we created, which can be read as follows. (Do not pay attention to the code but to the print block)\n\nLet’s writ some code to evaluate the decision tree we built.\n\ndef classify_example(example, tree):\n    question = list(tree.keys())[0]\n    feature_name, split_value = question.split(\" &lt;= \")\n    \n\n    # ask question\n    if example[feature_name] &lt;= float(split_value):\n        answer = tree[question][0]\n    else:\n        answer = tree[question][1]\n\n    # base case\n    if not isinstance(answer, dict): # if the answer is not a dictionary, then it is a leaf node\n        return answer\n\n    # recursive case\n    else:\n        residual_tree = answer\n        return classify_example(example, residual_tree)\n    \n\ndef calculate_accuracy(df, tree):\n    df  = df.copy()\n\n    # df[\"classification\"] = df.apply(classify_example, axis=1, args=(tree,))\n    # df[\"classification_correct\"] = df[\"classification\"] == df[\"target\"]\n\n    df.loc[:, \"classification\"] = df.apply(classify_example, axis=1, args=(tree,))\n    df.loc[:, \"classification_correct\"] = df[\"classification\"] == df[\"target\"]\n    \n    accuracy = df[\"classification_correct\"].mean()\n    \n    return accuracy\n\nSince, we created sub_trees in an uninhibited manner, it will result in perfect train accuracy.\n\ncalculate_accuracy(train_df, my_tree)\n\n1.0\n\n\nLet’s check the test accuracy\n\ncalculate_accuracy(test_df, my_tree)\n\n1.0\n\n\nSurprisingly! It also resulted in the perfect test accuracy\n\nControl the depth and min_samples in a decision tree\nIn the above case, we got test accuracy of 100%. But usually this is not the case because datasets are more complex.\nIf we allow the decision tree to grow unhibited manner then it overfits where it is possible that every leaf node would only have one data point. This is not a good decision tree because then prediction from such a tree becomes highly sensitive to small fluctuations in the data.\nHence, now we modify our code to pre-prune the tree i.e. limit its growth using max_depth and min_samples as the hyperparamters\n\n\n\nDecision Tree code (for case 2. i.e. splitting fo a fixed number of steps)\nBefore we write the actual code, we need a function to get the majority class label of the set if the subset is not pure but further splitting is not possible because stopping condition is reached.\n\ndef get_majority_class(df: pd.DataFrame) -&gt; int:\n    \"\"\"\n    Classify the data.\n    \"\"\"\n    y = df.iloc[:, -1]\n    return y.mode()[0]\n\n\ndef decision_tree_improved(df: pd.DataFrame, counter = 0,  min_samples=2, max_depth=5) -&gt; dict:\n\n    data = df\n    # base case: If data is pure or we hit max_depth or min_sample condition violates-&gt; stop and return the class of the child node\n    if check_purity(data) or (len(data) &lt; min_samples) or (counter == max_depth):\n        predicted_class = get_majority_class(data) \n        return predicted_class\n    \n    # else: keep on splitting \n    # Recursive\n    else:\n        counter+=1\n        # for splitting: get_potential_splits -&gt; determine_best_split -&gt; split_data based on best_split_column and best_split_value\n        potential_splits = get_potential_splits(data)\n        best_split_column, best_split_value = determine_best_split(data, potential_splits)\n        left_branch, right_branch = split_data(data, best_split_column, best_split_value)\n\n        \n        condition = \"{} &lt;= {}\".format(list(data.columns)[best_split_column], best_split_value)\n        # create the sub-tree as a dictionary storing the condition as key and a list as the value. This list for a \n        # condition has either the `predicted_class` if the child node is pure or another condition that will further split the \n        # impure child node.\n        sub_tree = {condition: []}\n\n        # get the answer for the 2 child nodes we just created (Step-1) and append them to the sub-tree\n        # (Step-1): get answers\n        left_branch_answer = decision_tree_improved(left_branch,  counter, min_samples, max_depth)\n        right_branch_answer = decision_tree_improved(right_branch, counter, min_samples, max_depth)\n\n        if left_branch_answer == right_branch_answer: # Example: Instead of {'petal length &lt;= 2.5': ['setosa', 'setosa']} just return 'setosa'\n            sub_tree = left_branch_answer\n\n        else:\n            sub_tree[condition].append(left_branch_answer)\n            sub_tree[condition].append(right_branch_answer)\n\n    return sub_tree\n\n\nmy_tree = decision_tree_improved(train_df, max_depth=3)\npprint(my_tree)\n\n{'petal length (cm) &lt;= 2.45': ['setosa',\n                               {'petal width (cm) &lt;= 1.75': [{'petal length (cm) &lt;= 4.95': ['versicolor',\n                                                                                            'virginica']},\n                                                             {'petal length (cm) &lt;= 4.85': ['versicolor',\n                                                                                            'virginica']}]}]}\n\n\n\n\ncalculate_accuracy(test_df, my_tree) # test accuracy with pruned tree\n\n0.95"
  },
  {
    "objectID": "posts/2025-05-19-linked-list/Linked_List.html",
    "href": "posts/2025-05-19-linked-list/Linked_List.html",
    "title": "Linked List Made Easy",
    "section": "",
    "text": "In this notebook, we will understand and build Linked List from scratch. We will build it step-by-step after understanding its core concepts. I will also perform testing and debugging of the code to show how we read errors, understand and debug them. This is going to be a long blog but quick to read since most of the code is copy pasted multiple times within the notebook.\n\n# type hints\nfrom typing import List, Union\n\nA Linked List is made up of nodes, where nodes contain data and a pointer to the next node.\n\nLet’s create a Node class which will have 2 attributes: data & a pointer to the next node\n\nclass Node:\n    def __init__(self, data=None, next=None):\n        self.data = data\n        self.next = next\n\nLet’s build a LinkedList in baby steps.\nFirst, we will build 2 functions in linked list. One to create a linked list by adding data and another to print it so that we can visualize and debug.\nclass LinkedList:\n    def __init__(self):\n        pass\n\n    def insert_at_beginning(self, data):\n        pass\n\n    def print(self):\n        pass\nOnce we build the above class with 2 simple functions - insert_at_beginning and print, it will be much simpler to complete the remaining functions to add different functionalities to LinkedList\nclass LinkedList:\n    def __init__(self):\n        pass\n\n    def insert_at_beginning(self, data):\n        pass\n\n    def print(self):\n        pass\n\n    def insert_at_end(self, data):\n        pass\n\n    def insert_values(self, data_list):\n        pass\n\n    def get_length(self):\n        pass\n\n    def insert_at(self, index):\n        pass\n\n    def remove_at(self, index):\n        pass\nIt is important to realize whether inputs are required for creating any function. If not, why not. If yes, what type of inputs.\nLet’s get started.\nConcepts: \n\nWe will add data by creating Nodes.\nWe use head to control any operation on Linked List we want to perform.\n\nhead is an object always pointing towards the start of the linked list. (head means top/start)\n\n\nclass LinkedList:\n    def __init__(self):\n        # head is an object always pointing towards the head i.e. start of the linked list\n        self.head = None # Step-1: initialize head to none. this will be updated once we add data to the linked list\n\n    def insert_at_beginning(self, data):\n        # In insert_at_beginning, we add element at the start of the linked list\n        \n        node = Node(data, self.head) # Steps-2&3: create a node with data and next pointer to the current head\n        self.head = node             # Step-4: update the current head to the new node just added       \n\n    def print(self):\n        # to print we iterate on the node starting from the first node\n        itr = self.head\n        if itr.data is None:\n            print(\"Linked List is empty\")\n        llstr = \"\"\n        while itr:\n            llstr+=str(itr.data)\n            llstr+='--&gt;'\n            itr = itr.next\n        print(llstr)\n\n\n# Let's test the code we created so far by creating a linked list and adding elements at the beginning\nll = LinkedList()\nll.insert_at_beginning(292)\nll.insert_at_beginning(301)\nll.print()\n\n301--&gt;292--&gt;\n\n\n\n# let's test for an edge case of empty list\nll = LinkedList()\nll.print()\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[11], line 3\n      1 # let's test for an edge case of empty list\n      2 ll = LinkedList()\n----&gt; 3 ll.print()\n\nCell In[9], line 15, in LinkedList.print(self)\n     12 def print(self):\n     13     # to print we iterate on the node starting from the first node\n     14     itr = self.head\n---&gt; 15     if itr.data is None:\n     16         print(\"Linked List is empty\")\n     17     llstr = \"\"\n\nAttributeError: 'NoneType' object has no attribute 'data'\n\n\n\nWe are getting an AttributeError because we are trying to refer an attribute data that does not even exist for an empty list. So, issue is for an empty list. It is so because empty list contains None element as head (self.head = None). You can see the topmost figue in insert at beginning. This None head does not have data attribute.\nSo, we need to change the code slightly and condition over the self.head to check for an empty list\n\nclass LinkedList:\n    def __init__(self):\n        # head is an object always pointing towards the head i.e. start of the linked list\n        self.head = None # Step-1: initialize head to none. this will be updated once we add data to the linked list\n\n    def insert_at_beginning(self, data):\n        # In insert_at_beginning, we add element at the start of the linked list\n        \n        node = Node(data, self.head) # Steps-2&3: create a node with data and next pointer to the current head\n        self.head = node             # Step-4: update the current head to the new node just added       \n\n    def print(self):        \n        # edge case: if ll is empty\n        if self.head is None:\n            print(\"Linked List is empty!\")\n        # to print we iterate on the node starting from the first node\n        itr = self.head\n        llstr = \"\"\n        while itr:\n            llstr+=str(itr.data)\n            llstr+='--&gt;'\n            itr = itr.next\n        print(llstr)\n\n\n# let's test for an edge case of empty list\nll = LinkedList()\nll.print()\n\nLinked List is empty!\n\n\n\nGreat! this works as expected. Now, let’s add insert_at_end\n\nclass LinkedList:\n    def __init__(self):\n        # head is an object always pointing towards the head i.e. start of the linked list\n        self.head = None # Step-1: initialize head to none. this will be updated once we add data to the linked list\n\n    def insert_at_beginning(self, data):\n        # In insert_at_beginning, we add element at the start of the linked list\n        \n        node = Node(data, self.head) # Steps-2&3: create a node with data and next pointer to the current head\n        self.head = node             # Step-4: update the current head to the new node just added      \n\n    def print(self):        \n        if self.head is None:\n            print(\"Linked List is empty\")\n        # to print we iterate on the node starting from the first node\n        itr = self.head\n        llstr = \"\"\n        while itr:\n            llstr+=str(itr.data)\n            llstr+='--&gt;'\n            itr = itr.next\n        print(llstr)\n\n    def insert_at_end(self, data):\n        # Remember - we use head as the iterator\n        itr = self.head # start from the beginning of the list\n        while itr.next:\n            itr = itr.next # reach the end\n        # once we reach end, do 2 things:\n        # 1. make the node of new data and point it to None because it is the end\n        node = Node(data, None)\n        # 2. update the old end's next to the new node\n        itr.next = node\n        \n        \n\nIn print I used while itr:, whereas in insert_at_end, I used while itr.next. Draw a diagram and think why this makes sense.\n\nIn fact, print could be rewritten as:\ndef print(self):\n    # edge case\n    itr = self.head\n    if itr.data is None:\n        print(\"Linked List is empty\")\n    llstr = \"\"\n    while itr.next: \n        llstr+=str(itr.data)\n        llstr+='--&gt;'\n        itr = itr.next\n    # above code end by reaching the last node\n    llstr+=str(itr.data) # add data about the last node\n    print(llstr)\n\n\n# Let's test the code we created so far:\nll = LinkedList()\nll.insert_at_beginning(292)\nll.insert_at_beginning(301)\nll.insert_at_end(512)\nll.print()\n\n301--&gt;292--&gt;512--&gt;\n\n\nNow let’s code insert_values - which creates a new linked list from the values given as a list.\n\nclass LinkedList:\n    def __init__(self):\n        # head is an object always pointing towards the head i.e. start of the linked list\n        self.head = None # Step-1: initialize head to none. this will be updated once we add data to the linked list\n\n    def insert_at_beginning(self, data):\n        # In insert_at_beginning, we add element at the start of the linked list\n        \n        node = Node(data, self.head) # Steps-2&3: create a node with data and next pointer to the current head\n        self.head = node             # Step-4: update the current head to the new node just added      \n\n    def print(self):        \n        if self.head is None:\n            print(\"Linked List is empty\")\n        # to print we iterate on the node starting from the first node\n        itr = self.head\n        llstr = \"\"\n        while itr:\n            llstr+=str(itr.data)\n            llstr+='--&gt;'\n            itr = itr.next\n        print(llstr)\n\n    def insert_at_end(self, data:  Union[str, int]):\n        # Remember - we use head as the iterator\n        itr = self.head # start from the beginning of the list\n        while itr.next:\n            itr = itr.next # reach the end\n        # once we reach end, do 2 things:\n        # 1. make the node of new data and point it to None because it is the end\n        node = Node(data, None)\n        # 2. update the old end's next to the new node\n        itr.next = node\n\n    def insert_values(self, data_list: List):\n        # initialize a new linked list\n        self.head = None\n\n        # Use the insert_at_beginning function\n        for data in data_list:\n            self.insert_at_beginning(data)\n\n\n# Let's test the code we created so far:\nll = LinkedList()\nll.insert_at_beginning(292)\nll.insert_at_beginning(301)\nll.insert_at_end(512)\nll.print()\nnew_data_list = [1,3,5,7,9]\nll = LinkedList()\nll.insert_values(new_data_list)\nll.print()\n\n301--&gt;292--&gt;512--&gt;\n9--&gt;7--&gt;5--&gt;3--&gt;1--&gt;\n\n\nOops! it added elements in reverse order because we used insert_at_beginning. To correct this, there are 2 options: 1. reverse the data_list and use insert_at_beginning: not recommended because it creates computational burden of reversing the list 2. modify the code to use insert_at_end\nMethod-1:reverse the data_list and use insert_at_beginning\n\nclass LinkedList:\n    def __init__(self):\n        # head is an object always pointing towards the head i.e. start of the linked list\n        self.head = None # Step-1: initialize head to none. this will be updated once we add data to the linked list\n\n    def insert_at_beginning(self, data):\n        # In insert_at_beginning, we add element at the start of the linked list\n        \n        node = Node(data, self.head) # Steps-2&3: create a node with data and next pointer to the current head\n        self.head = node             # Step-4: update the current head to the new node just added\n\n    def print(self):        \n        if self.head is None:\n            print(\"Linked List is empty\")\n        # to print we iterate on the node starting from the first node\n        itr = self.head\n        llstr = \"\"\n        while itr:\n            llstr+=str(itr.data)\n            llstr+='--&gt;'\n            itr = itr.next\n        print(llstr)\n\n    def insert_at_end(self, data:  Union[str, int]):\n        # Remember - we use head as the iterator\n        itr = self.head # start from the beginning of the list\n        while itr.next:\n            itr = itr.next # reach the end\n        # once we reach end, do 2 things:\n        # 1. make the node of new data and point it to None because it is the end\n        node = Node(data, None)\n        # 2. update the old end's next to the new node\n        itr.next = node\n\n    def insert_values(self, data_list: List):\n        # initialize a new linked list\n        self.head = None\n\n        # Use the insert_at_beginning function\n        data_list = data_list[::-1] # reverse it\n        for data in data_list:\n            self.insert_at_beginning(data)\n\n\n# Let's test the code we created so far:\nnew_data_list = [1,3,5,7,9]\nll = LinkedList()\nll.insert_values(new_data_list)\nll.print()\n\n1--&gt;3--&gt;5--&gt;7--&gt;9--&gt;\n\n\nMethod-2: Modify the code and use insert_at_end\n\nclass LinkedList:\n    def __init__(self):\n        # head is an object always pointing towards the head i.e. start of the linked list\n        self.head = None # Step-1: initialize head to none. this will be updated once we add data to the linked list\n\n    def insert_at_beginning(self, data):\n        # In insert_at_beginning, we add element at the start of the linked list\n        \n        node = Node(data, self.head) # Steps-2&3: create a node with data and next pointer to the current head\n        self.head = node             # Step-4: update the current head to the new node just added    \n\n    def print(self):        \n        if self.head is None:\n            print(\"Linked List is empty\")\n        # to print we iterate on the node starting from the first node\n        itr = self.head\n        llstr = \"\"\n        while itr:\n            llstr+=str(itr.data)\n            llstr+='--&gt;'\n            itr = itr.next\n        print(llstr)\n\n    def insert_at_end(self, data:  Union[str, int]):\n        # Remember - we use head as the iterator\n        itr = self.head # start from the beginning of the list\n        while itr.next:\n            itr = itr.next # reach the end\n        # once we reach end, do 2 things:\n        # 1. make the node of new data and point it to None because it is the end\n        node = Node(data, None)\n        # 2. update the old end's next to the new node\n        itr.next = node\n\n    def insert_values(self, data_list: List):\n        # initialize a new linked list\n        self.head = None\n\n        # Use the insert_at_end function\n        for data in data_list:\n            self.insert_at_end(data)\n\n\n# Let's test the code we created so far:\nnew_data_list = [1,3,5,7,9]\nll = LinkedList()\nll.insert_values(new_data_list)\nll.print()\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[21], line 4\n      2 new_data_list = [1,3,5,7,9]\n      3 ll = LinkedList()\n----&gt; 4 ll.insert_values(new_data_list)\n      5 ll.print()\n\nCell In[20], line 41, in LinkedList.insert_values(self, data_list)\n     39 # Use the insert_at_end function\n     40 for data in data_list:\n---&gt; 41     self.insert_at_end(data)\n\nCell In[20], line 27, in LinkedList.insert_at_end(self, data)\n     24 def insert_at_end(self, data:  Union[str, int]):\n     25     # Remember - we use head as the iterator\n     26     itr = self.head # start from the beginning of the list\n---&gt; 27     while itr.next:\n     28         itr = itr.next # reach the end\n     29     # once we reach end, do 2 things:\n     30     # 1. make the node of new data and point it to None because it is the end\n\nAttributeError: 'NoneType' object has no attribute 'next'\n\n\n\nOops! we got an error! This error is similar to what we saw earlier in print function.\nWe can see that it arises insert_at_end function. So, to debug, first inspect the insert_at_end for edge case of an empty list\n\nll = LinkedList()\nll.insert_at_end(23)\nll.print()\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[22], line 2\n      1 ll = LinkedList()\n----&gt; 2 ll.insert_at_end(23)\n      3 ll.print()\n\nCell In[20], line 27, in LinkedList.insert_at_end(self, data)\n     24 def insert_at_end(self, data:  Union[str, int]):\n     25     # Remember - we use head as the iterator\n     26     itr = self.head # start from the beginning of the list\n---&gt; 27     while itr.next:\n     28         itr = itr.next # reach the end\n     29     # once we reach end, do 2 things:\n     30     # 1. make the node of new data and point it to None because it is the end\n\nAttributeError: 'NoneType' object has no attribute 'next'\n\n\n\nNow we see the error! Earlier we did not see it because we used insert_at_end to a non-empty Linked List.\nll = LinkedList()\nll.insert_at_beginning(292)\nll.insert_at_beginning(301)\nll.insert_at_end(512) # -&gt; adding at the end to a non-empty list and that's why we didn't see this error\nll.print()\nIt is always a good idea to try visualize what would be happening, if linked list is empty or has 1 element. Then the mental visualization can be extended into a code smoothly for n numbers of elements.\nPerhaps this could help you visualize:\n\n\nclass LinkedList:\n    def __init__(self):\n        # head is an object always pointing towards the head i.e. start of the linked list\n        self.head = None # Step-1: initialize head to none. this will be updated once we add data to the linked list\n\n    def insert_at_beginning(self, data):\n        # In insert_at_beginning, we add element at the start of the linked list\n        \n        node = Node(data, self.head) # Steps-2&3: create a node with data and next pointer to the current head\n        self.head = node             # Step-4: update the current head to the new node just added  \n\n    def print(self):        \n        if self.head is None:\n            print(\"Linked List is empty\")\n        # to print we iterate on the node starting from the first node\n        itr = self.head\n        llstr = \"\"\n        while itr:\n            llstr+=str(itr.data)\n            llstr+='--&gt;'\n            itr = itr.next\n        print(llstr)\n\n    def insert_at_end(self, data:  Union[str, int]):\n        # if linked list is empty, we simply add the node and make it as head\n        if self.head is None:\n            node = Node(data, None) # create a node and point it to None because it is end\n            self.head = node        # make this node as new head so that we could iterate later\n\n        # Remember - we use head as the iterator\n        itr = self.head # start from the beginning of the list\n        while itr.next:\n            itr = itr.next # reach the end\n        # once we reach end, do 2 things:\n        # 1. make the node of new data and point it to None because it is the end\n        node = Node(data, None)\n        # 2. update the old end's next to the new node\n        itr.next = node\n\n    def insert_values(self, data_list: List):\n        # initialize a new linked list\n        self.head = None\n\n        # Use the insert_at_end function\n        for data in data_list:\n            self.insert_at_end(data)\n\n\n\nll = LinkedList()\nll.insert_at_end(23)\nll.print()\n\n23--&gt;23--&gt;\n\n\n\n\nll = LinkedList()\nll.insert_at_end(23)\nll.insert_at_end(43)\nll.print()\n\n23--&gt;23--&gt;43--&gt;\n\n\nHere we see the first element getting repeated twice! Why?\nBecause initially, when the Linked List is empty! we added the element but then we did not stop the code after adding the new node to run.\nSimply - To do so, add a return statement\n\nclass LinkedList:\n    def __init__(self):\n        # head is an object always pointing towards the head i.e. start of the linked list\n        self.head = None # Step-1: initialize head to none. this will be updated once we add data to the linked list\n\n    def insert_at_beginning(self, data):\n        # In insert_at_beginning, we add element at the start of the linked list\n        \n        node = Node(data, self.head) # Steps-2&3: create a node with data and next pointer to the current head\n        self.head = node             # Step-4: update the current head to the new node just added    \n\n    def print(self):        \n        if self.head is None:\n            print(\"Linked List is empty\")\n            return # if linked list is empty, no need to run the below code. So, return\n        # to print we iterate on the node starting from the first node\n        itr = self.head\n        llstr = \"\"\n        while itr:\n            llstr+=str(itr.data)\n            llstr+='--&gt;'\n            itr = itr.next\n        print(llstr)\n\n    def insert_at_end(self, data:  Union[str, int]):\n        # if linked list is empty\n        if self.head is None:\n            node = Node(data, None) # create a node and point it to None because it is end\n            self.head = node        # make this node as new head (because until now there is no head i.e. linked list is empty)\n            return # to stop the below code from running after adding the node to an empty Linked List\n        \n        # Remember - we use head as the iterator\n        itr = self.head # start from the beginning of the list\n        while itr.next:\n            itr = itr.next # reach the end\n        # once we reach end, do 2 things:\n        # 1. make the node of new data and point it to None because it is the end\n        node = Node(data, None)\n        # 2. update the old end's next to the new node\n        itr.next = node\n\n    def insert_values(self, data_list: List):\n        # initialize a new linked list\n        self.head = None\n\n        # Use the insert_at_end function\n        for data in data_list:\n            self.insert_at_end(data)\n\n\nll = LinkedList()\nll.insert_at_end(23)\nll.print()\n\nll = LinkedList()\nll.insert_at_end(23)\nll.insert_at_end(43)\nll.print()\n\n23--&gt;\n23--&gt;43--&gt;\n\n\nNow, let’s test it for insert_values\n\n# Let's test the code we created so far:\nnew_data_list = [1,3,5,7,9]\nll = LinkedList()\nll.insert_values(new_data_list)\nll.print()\n\n1--&gt;3--&gt;5--&gt;7--&gt;9--&gt;\n\n\nLet’s now complete the code for LinkedList class.\nSome helpful images for visualization for:\n\ninsert_at\n\n\nclass LinkedList:\n    def __init__(self):\n        # head is an object always pointing towards the head i.e. start of the linked list\n        self.head = None # Step-1: initialize head to none. this will be updated once we add data to the linked list\n\n    def insert_at_beginning(self, data):\n        # In insert_at_beginning, we add element at the start of the linked list\n        \n        node = Node(data, self.head) # Steps-2&3: create a node with data and next pointer to the current head\n        self.head = node             # Step-4: update the current head to the new node just added    \n\n    def print(self):        \n        if self.head is None:\n            print(\"Linked List is empty\")\n            return # if linked list is empty, no need to run the below code. So, return\n        # to print we iterate on the node starting from the first node\n        itr = self.head\n        llstr = \"\"\n        while itr:\n            llstr+=str(itr.data)\n            llstr+='--&gt;'\n            itr = itr.next\n        print(llstr)\n\n    def insert_at_end(self, data:  Union[str, int]):\n        # if linked list is empty\n        if self.head is None:\n            node = Node(data, None) # create a node and point it to None because it is end\n            self.head = node        # make this node as new head (because until now there is no head i.e. linked list is empty)\n            return # to stop the below code from running after adding the node to an empty Linked List\n        \n        # Remember - we use head as the iterator\n        itr = self.head # start from the beginning of the list\n        while itr.next:\n            itr = itr.next # reach the end\n        # once we reach end, do 2 things:\n        # 1. make the node of new data and point it to None because it is the end\n        node = Node(data, None)\n        # 2. update the old end's next to the new node\n        itr.next = node\n\n    def insert_values(self, data_list: List):\n        # initialize a new linked list\n        self.head = None\n\n        # Use the insert_at_end function\n        for data in data_list:\n            self.insert_at_end(data)\n    \n    def get_length(self):\n        counter = 0\n        # Edge case: if linked list is empty. Then we need to check the `head`\n        if self.head is None:\n            return 0\n        # else, iterate over head\n        itr = self.head\n        while itr:\n            itr = itr.next\n            counter+=1\n        return counter\n    \n    def insert_at(self, index: int, data: Union[str, int]):\n        # Edgecases: adding element at the start, or at the end \n        if index==0:\n            self.insert_at_beginning(data)\n        elif index == self.get_length():\n            self.insert_at_end(data)\n        if index&lt;0 or index&gt;self.get_length():\n            raise Exception(\"Out of range\")\n        # By now, we are adding element in a non-empty list and somewhere in the middle\n        # begin from the head (i.e. start of a lInked list)\n        itr = self.head\n        counter = 0\n        while counter&lt;index-1:\n            itr = itr.next\n            counter+=1\n        # above loop will break as soon as we are at node just before where we want to add the new data\n        # now add new data as a node and pointing to the next node in the original linked list\n        node = Node(data, itr.next)\n        # now point the previous node's next pointer to the new node just added\n        itr.next = node\n\n        \n\n\n# Let's test the code for get_length:\nnew_data_list = [1,3,5,7,9]\nll = LinkedList()\nll.insert_values(new_data_list)\nll.print()\nprint(ll.get_length())\n\nnew_data_list = [1]\nll = LinkedList()\nll.insert_values(new_data_list)\nll.print()\nprint(ll.get_length())\n\nll = LinkedList()\nll.print()\nprint(ll.get_length())\n\nll = LinkedList()\nll.insert_at_beginning(23)\nll.print()\nprint(ll.get_length())\n\nll = LinkedList()\nll.insert_at_end(23)\nll.print()\nprint(ll.get_length())\n\n1--&gt;3--&gt;5--&gt;7--&gt;9--&gt;\n5\n1--&gt;\n1\nLinked List is empty\n0\n23--&gt;\n1\n23--&gt;\n1\n\n\n\n# Let's test the code for insert_at:\nnew_data_list = ['figs','banana', 'mango', 'grapes', 'orange']\nll = LinkedList()\nll.insert_values(new_data_list)\nll.print()\nll.insert_at(2, 'jackfruit')\nll.print()\n\nfigs--&gt;banana--&gt;mango--&gt;grapes--&gt;orange--&gt;\nfigs--&gt;banana--&gt;jackfruit--&gt;mango--&gt;grapes--&gt;orange--&gt;\n\n\n\n# Let's test the code for insert_at:\nnew_data_list = ['figs','banana', 'mango', 'grapes', 'orange']\nll = LinkedList()\nll.insert_values(new_data_list)\nll.print()\nll.insert_at(0, 'jackfruit')\nll.print()\n\nnew_data_list = ['figs','banana', 'mango', 'grapes', 'orange']\nll = LinkedList()\nll.insert_values(new_data_list)\nll.print()\nll.insert_at(5, 'jackfruit')\nll.print()\n\nfigs--&gt;banana--&gt;mango--&gt;grapes--&gt;orange--&gt;\njackfruit--&gt;jackfruit--&gt;figs--&gt;banana--&gt;mango--&gt;grapes--&gt;orange--&gt;\nfigs--&gt;banana--&gt;mango--&gt;grapes--&gt;orange--&gt;\nfigs--&gt;banana--&gt;mango--&gt;grapes--&gt;orange--&gt;jackfruit--&gt;jackfruit--&gt;\n\n\nThis is same error! We forgot to use return statement\n\nclass LinkedList:\n    def __init__(self):\n        # head is an object always pointing towards the head i.e. start of the linked list\n        self.head = None # Step-1: initialize head to none. this will be updated once we add data to the linked list\n\n    def insert_at_beginning(self, data):\n        # In insert_at_beginning, we add element at the start of the linked list\n        \n        node = Node(data, self.head) # Steps-2&3: create a node with data and next pointer to the current head\n        self.head = node             # Step-4: update the current head to the new node just added    \n\n    def print(self):        \n        if self.head is None:\n            print(\"Linked List is empty\")\n            return # if linked list is empty, no need to run the below code. So, return\n        # to print we iterate on the node starting from the first node\n        itr = self.head\n        llstr = \"\"\n        while itr:\n            llstr+=str(itr.data)\n            llstr+='--&gt;'\n            itr = itr.next\n        print(llstr)\n\n    def insert_at_end(self, data:  Union[str, int]):\n        # if linked list is empty\n        if self.head is None:\n            node = Node(data, None) # create a node and point it to None because it is end\n            self.head = node        # make this node as new head (because until now there is no head i.e. linked list is empty)\n            return # to stop the below code from running after adding the node to an empty Linked List\n        \n        # Remember - we use head as the iterator\n        itr = self.head # start from the beginning of the list\n        while itr.next:\n            itr = itr.next # reach the end\n        # once we reach end, do 2 things:\n        # 1. make the node of new data and point it to None because it is the end\n        node = Node(data, None)\n        # 2. update the old end's next to the new node\n        itr.next = node\n\n    def insert_values(self, data_list: List):\n        # initialize a new linked list\n        self.head = None\n\n        # Use the insert_at_end function\n        for data in data_list:\n            self.insert_at_end(data)\n    \n    def get_length(self):\n        counter = 0\n        # Edge case: if linked list is empty. Then we need to check the `head`\n        if self.head is None:\n            return 0\n        # else, iterate over head\n        itr = self.head\n        while itr:\n            itr = itr.next\n            counter+=1\n        return counter\n    \n    def insert_at(self, index: int, data: Union[str, int]):\n        # Edgecases: adding element at the start, or at the end \n        if index==0:\n            self.insert_at_beginning(data)\n            return\n        elif index == self.get_length():\n            self.insert_at_end(data)\n            return\n        if index&lt;0 or index&gt;self.get_length():\n            raise Exception(\"Out of range\")\n        # By now, we are adding element in a non-empty list and somewhere in the middle\n        # begin from the head (i.e. start of a lInked list)\n        itr = self.head\n        counter = 0\n        while counter&lt;index-1:\n            itr = itr.next\n            counter+=1\n        # above loop will break as soon as we are at node just before where we want to add the new data\n        # now add new data as a node and pointing to the next node in the original linked list\n        node = Node(data, itr.next)\n        # now point the previous node's next pointer to the new node just added\n        itr.next = node\n\n    def remove_at(self, index: int):\n        if index&lt;0 or index&gt;self.get_length()-1:\n            raise Exception(\"Out of range\")\n        \n        # Edgecases: removing element at the start, or at the end \n        if self.head is None:\n            print('Cannot remove from an empty list')\n            return\n        # remove the first element: simply update the head\n        if index==0:\n            self.head = self.head.next # update the head(i.e. start) to the next node\n            return # DO NOT MISS THIS\n        \n        # By now, we are removing element in a non-empty list and somewhere in the middle or at the end\n        # begin from the head (i.e. start of a lInked list)\n        itr = self.head\n        counter = 0\n        while counter&lt;index-1:\n            itr = itr.next\n            counter+=1\n        # above loop will break as soon as we are at node just before where we want to remove the data\n        # now we remove the node by pointing the current node to the next's next node skipping the node at index = index\n        itr.next = itr.next.next\n\n\n# Let's test the code for insert_at:\nnew_data_list = ['figs','banana', 'mango', 'grapes', 'orange']\nll = LinkedList()\nll.insert_values(new_data_list)\nll.print()\nll.insert_at(0, 'jackfruit')\nll.print()\n\nnew_data_list = ['figs','banana', 'mango', 'grapes', 'orange']\nll = LinkedList()\nll.insert_values(new_data_list)\nll.print()\nll.insert_at(5, 'jackfruit')\nll.print()\n\nfigs--&gt;banana--&gt;mango--&gt;grapes--&gt;orange--&gt;\njackfruit--&gt;figs--&gt;banana--&gt;mango--&gt;grapes--&gt;orange--&gt;\nfigs--&gt;banana--&gt;mango--&gt;grapes--&gt;orange--&gt;\nfigs--&gt;banana--&gt;mango--&gt;grapes--&gt;orange--&gt;jackfruit--&gt;\n\n\n\n# Let's test the code for remove_at:\nnew_data_list = ['figs','banana', 'mango', 'grapes', 'orange']\nll = LinkedList()\nll.insert_values(new_data_list)\nll.print()\nll.remove_at(0)\nll.print()\nprint('')\n\nnew_data_list = ['figs','banana', 'mango', 'grapes', 'orange']\nll = LinkedList()\nll.insert_values(new_data_list)\nll.print()\nll.remove_at(4)\nll.print()\nprint('')\nnew_data_list = ['figs','banana', 'mango', 'grapes', 'orange']\nll = LinkedList()\nll.insert_values(new_data_list)\nll.print()\nll.remove_at(1)\nll.print()\nprint('')\n\nfigs--&gt;banana--&gt;mango--&gt;grapes--&gt;orange--&gt;\nbanana--&gt;mango--&gt;grapes--&gt;orange--&gt;\n\nfigs--&gt;banana--&gt;mango--&gt;grapes--&gt;orange--&gt;\nfigs--&gt;banana--&gt;mango--&gt;grapes--&gt;\n\nfigs--&gt;banana--&gt;mango--&gt;grapes--&gt;orange--&gt;\nfigs--&gt;mango--&gt;grapes--&gt;orange--&gt;\n\n\n\n\n\nExercise-1: add following 2 methods:\ndef insert_after_value(self, data_after, data_to_insert):\n    # Search for first occurance of data_after value in linked list\n    # Now insert data_to_insert after data_after node\n\ndef remove_by_value(self, data):\n    # Remove first node that contains data\n\n\nShow solution code\nclass LinkedList:\n    def __init__(self):\n        # head is an object always pointing towards the head i.e. start of the linked list\n        self.head = None # Step-1: initialize head to none. this will be updated once we add data to the linked list\n\n    def insert_at_beginning(self, data):\n        # In insert_at_beginning, we add element at the start of the linked list\n        \n        node = Node(data, self.head) # Steps-2&3: create a node with data and next pointer to the current head\n        self.head = node             # Step-4: update the current head to the new node just added\n\n    def print(self):        \n        if self.head is None:\n            print(\"Linked List is empty\")\n            return # if linked list is empty, no need to run the below code. So, return\n        # to print we iterate on the node starting from the first node\n        itr = self.head\n        llstr = \"\"\n        while itr:\n            llstr+=str(itr.data)\n            llstr+='--&gt;'\n            itr = itr.next\n        print(llstr)\n\n    def insert_at_end(self, data:  Union[str, int]):\n        # if linked list is empty\n        if self.head is None:\n            node = Node(data, None) # create a node and point it to None because it is end\n            self.head = node        # make this node as new head (because until now there is no head i.e. linked list is empty)\n            return # to stop the below code from running after adding the node to an empty Linked List\n        \n        # Remember - we use head as the iterator\n        itr = self.head # start from the beginning of the list\n        while itr.next:\n            itr = itr.next # reach the end\n        # once we reach end, do 2 things:\n        # 1. make the node of new data and point it to None because it is the end\n        node = Node(data, None)\n        # 2. update the old end's next to the new node\n        itr.next = node\n\n    def insert_values(self, data_list: List):\n        # initialize a new linked list\n        self.head = None\n\n        # Use the insert_at_end function\n        for data in data_list:\n            self.insert_at_end(data)\n    \n    def get_length(self):\n        counter = 0\n        # Edge case: if linked list is empty. Then we need to check the `head`\n        if self.head is None:\n            return 0\n        # else, iterate over head\n        itr = self.head\n        while itr:\n            itr = itr.next\n            counter+=1\n        return counter\n    \n    def insert_at(self, index: int, data: Union[str, int]):\n        # Edgecases: adding element at the start, or at the end \n        if index==0:\n            self.insert_at_beginning(data)\n            return\n        elif index == self.get_length():\n            self.insert_at_end(data)\n            return\n        if index&lt;0 or index&gt;self.get_length():\n            raise Exception(\"Out of range\")\n        # By now, we are adding element in a non-empty list and somewhere in the middle\n        # begin from the head (i.e. start of a lInked list)\n        itr = self.head\n        counter = 0\n        while counter&lt;index-1:\n            itr = itr.next\n            counter+=1\n        # above loop will break as soon as we are at node just before where we want to add the new data\n        # now add new data as a node and pointing to the next node in the original linked list\n        node = Node(data, itr.next)\n        # now point the previous node's next pointer to the new node just added\n        itr.next = node\n\n    def remove_at(self, index: int):\n        if index&lt;0 or index&gt;self.get_length()-1:\n            raise Exception(\"Out of range\")\n        \n        # Edgecases: removing element at the start, or at the end \n        if self.head is None:\n            print('Cannot remove from an empty list')\n            return\n        # remove the first element: simply update the head\n        if index==0:\n            self.head = self.head.next # update the head(i.e. start) to the next node\n            return # DO NOT MISS THIS\n        \n        # By now, we are removing element in a non-empty list and somewhere in the middle or at the end\n        # begin from the head (i.e. start of a lInked list)\n        itr = self.head\n        counter = 0\n        while counter&lt;index-1:\n            itr = itr.next\n            counter+=1\n        # above loop will break as soon as we are at node just before where we want to remove the data\n        # now we remove the node by pointing the current node to the next's next node skipping the node at index = index\n        itr.next = itr.next.next\n        \n        \n    def insert_after_value(self, data_after: Union[str, int], data_to_insert: Union[str, int]):\n        # if linked list is empty, we cannot index the data_after, hence, raise error \n        if self.head is None:\n            raise Exception(\"list is empty!\")\n        \n        # else iterate over linked list, and as we find the value, we insert using insert_at\n        itr = self.head # initialize the iterator at the start of the head\n        counter = 0\n        while itr:\n            value = itr.data\n            if value == data_after:\n                break\n            counter+=1\n            itr = itr.next\n\n        if counter==self.get_length(): # means we could not find data_after in the linked list\n            raise Exception(f'data_after = {data_after} not found in the Linked List.')\n        # else, insert at 1 index away from where we found data_after\n        self.insert_at(counter+1, data=data_to_insert)\n        \n    def remove_by_value(self, data: Union[str, int]):\n        # if linked list is empty, we cannot remove anything from it\n        if self.head is None:\n            raise Exception(\"list is empty!\")\n        \n         # else iterate over linked list, and as we find the value, we remove using remove_at\n        itr = self.head # initialize the iterator at the start of the head\n        counter = 0\n        while itr:\n            value = itr.data\n            if value == data:\n                break\n            counter+=1\n            itr = itr.next\n\n        if counter==self.get_length(): # means we could not find data to remove in the linked list\n            raise Exception(f'data = {data} not found in the Linked List.')\n        # else, remove the same index where we found data\n        self.remove_at(counter)\n\n\n##########################\n# Test the code developed\n##########################\nnew_data_list = ['figs','banana', 'mango', 'grapes', 'orange']\nll = LinkedList()\nll.insert_values(new_data_list)\nprint('Original')\nll.print()\nll.insert_after_value('banana', 'Mousami')\nprint('Modified')\nll.print()\nprint('')\nll.insert_after_value('orange', 'Pineapple')\nprint('Modified-2')\nll.print()\nprint('')\nll.insert_after_value('Apple', 'Custard Apple')\nprint('Modified-3')\nll.print()\nprint('')\n# test remove_by_value\nnew_data_list = ['figs','banana', 'mango', 'grapes', 'orange']\nll = LinkedList()\nll.insert_values(new_data_list)\nprint('Original')\nll.print()\nll.remove_by_value('mango')\nprint('Modified')\nll.print()\nll.remove_by_value('figs')\nprint('Modified-2')\nll.print()\nll.remove_by_value('mango')\nprint('Modified-3')\nll.print()\n\n\n\n\nExercise-2: Can you test for yourself by looking at the code above why this table is correct?\n\n\n\nExcercise-3: Try creating a doubly Linked List by yourself.\nHint:The only difference with regular linked list is that double linked has prev node reference as well. That way you can iterate in forward and backward direction. Your node class will look this this:\nclass Node:\n    def __init__(self, data=None, next=None, prev=None):\n        self.data = data\n        self.next = next\n        self.prev = prev\n\ninsert_at\n\n\n\nShow solution code\nclass Node:\n    def __init__(self, data=None, next=None, prev=None):\n        self.data = data\n        self.next = next\n        self.prev = prev\n    \n    \nclass DoublyLinkedList:\n    def __init__(self):\n        # initialize an empty doubly linked list. head is the start node (always)\n        self.head = None # make a None start node as head\n    \n    def insert_at_beginning(self, data: Union[str, int]):\n\n        # Case-1: We start with an empty Linked List\n        if self.head == None:\n            # make a node of the data, point next of this node to old head, point prev of this node to null/None as it is the new head (i.e. start)\n            node = Node(data, next=self.head, prev=None)\n            self.head = node # make the new node as the new head (i.e. start)\n            return\n        \n        else:\n            # Case-2: We have some nodes in place\n            node = Node(data, next=self.head, prev=None) # make a node, point next to old head, point prev to none\n            self.head.prev = node # point old head's prev to new node\n            self.head = node # make the new node as the new head (i.e. start)\n\n    \n    def print_forward(self):\n        if self.head is None:\n            print(\"Linked List is empty!\")\n            return\n        \n        # else we will print it. \n        # Iterate over the head, starting from the first node and moving 1 step further.\n        itr = self.head\n        llstr = \"\"\n        while itr:\n            llstr+= str(itr.data)\n            llstr+='--&gt;'\n            itr = itr.next\n        print(llstr)\n\n    def get_last_node(self):\n        if self.head is None:\n            print(\"Linked List is empty!\")\n            return\n        # else, iterate and get the last node\n        itr=self.head\n        while itr.next: # NOT itr. Can you reason-Why?🤔\n            itr = itr.next\n        return itr\n\n\n    def print_backward(self):\n        if self.head is None:\n            print(\"Linked List is empty!\")\n            return\n        # else we will print it. \n        # Iterate over the head, starting from the first node and moving 1 step further.\n        itr = self.get_last_node()\n        llstr = \"\"\n        while itr:\n            llstr+= str(itr.data)\n            llstr+='--&gt;'\n            itr = itr.prev\n        print(llstr)\n\n    def insert_at_end(self, data: Union[str, int]):\n\n        # Case-1: We start with an empty Linked List\n        if self.head == None:\n            # make a node of the data, point next of this node to old head, point prev of this node to null/None as it is the new head (i.e. start)\n            node = Node(data, next=self.head, prev=None)\n            self.head = node # make the new node as the new head (i.e. start)\n            return\n        \n        else:\n            # Case-2: We have some nodes in place\n            last_node = self.get_last_node() # get the last node\n            node = Node(data, next=None, prev=last_node) # make a node, point next to None, point prev to last node in linked_list\n            last_node.next = node # point last node's next to new node\n\n    def get_length(self):\n        counter = 0\n        # Edge case: if linked list is empty. Then we need to check the `head`\n        if self.head is None:\n            return 0\n        # else, iterate over head\n        itr = self.head\n        while itr:\n            itr = itr.next\n            counter+=1\n        return counter\n            \n\n    def insert_at(self, index: int, data: Union[str, int]):\n        # Edgecases: adding element at the start, or at the end \n        if index==0:\n            self.insert_at_beginning(data)\n            return\n        elif index == self.get_length():\n            self.insert_at_end(data)\n            return\n        if index&lt;0 or index&gt;self.get_length():\n            raise Exception(f\"index= {index} is Out of Range!\")\n        # By now, we are adding element in a non-empty list and somewhere in the middle\n        # begin from the head (i.e. start of a doubly linked list)\n        itr = self.head\n        counter = 0\n        while counter&lt;index-1:\n            itr = itr.next\n            counter+=1\n        # above loop will break as soon as we are at node just before where we want to add the new data\n        \n        node = Node(data, next=itr.next, prev=itr)# Create node of new data; point it's next to the next node in the original linked list; point it's prev to current itr node\n        if node.next:\n            node.next.prev = node # if next node exists, point its prev to new node\n        itr.next = node # point previous node's next pointer to the new node just added\n\n    def insert_values(self, data_list: List):\n        # initialize a new linked list\n        self.head = None\n\n        # Use the insert_at_end function\n        for data in data_list:\n            self.insert_at_end(data)\n\n\n##########################\n# Test the code developed\n##########################\ndll = DoublyLinkedList()\ndll.print_forward()\ndll.print_backward()\n# insert_at_beginning\ndll.insert_at_beginning('apple')\ndll.insert_at_beginning('banana')\ndll.insert_at_beginning('mango')\ndll.print_forward()\ndll.print_backward()\n# insert_at_end\ndll.insert_at_end('grapes')\ndll.print_forward()\n# insert_at\ndll.insert_at(1, 'mausami')\ndll.print_forward()\ndll.insert_at(0, '0')\ndll.print_forward()\ndll.insert_at(6, '6')\ndll.print_forward()\n# insert_values\ndll = DoublyLinkedList()\nfruits_list = ['apple', 'banana']\ndll.insert_values(fruits_list)\ndll.print_forward()\n\n\n\n\n\nResources:\nCodebasics Lecture on Linked List"
  },
  {
    "objectID": "posts/2025-05-21-stack/stack.html",
    "href": "posts/2025-05-21-stack/stack.html",
    "title": "Stack Made Easy",
    "section": "",
    "text": "In this notebook, we will understand what stack is, where we interact with stack it our daily lives and how can we build a stack in Python.\nBefore understanding what stack is, let’s see 2 most common examples of our daily life interaction with a stack which is now an integral part of modern day lives with computers, tablets and smartphones.\nInternet browsing - Suppose we are browsing the pages of a news channel CNN. Initially we are at their homepage - cnn.com, then we went to read news in the Entertainment section - cnn.com/entertainment, on this page we want to read about celebrities - cnn.comentertainment/celebrities and now we want to go back to the homepage (cnn.com) and read the news in the business world - cnn.com/business. Stack is the underlying data structure which stores our browsing history and provides a convenient way for us to go back or forward from our existing page to a page that has been visited in the past.\n\nUndo/Redo operations - If we are writing text in word document, we can conveiently press Ctrl + Z to undo or Ctrl+Y to redo becuase we are storing the user operations in a stack so that user experience is enhanced where it becomes extremly simple to either undo a change or redo it.\nWe can build stack in python by 2 ways: 1. List - okay but not ideal 2. Deque - better\n\n1. Stack using List\n\nbrowsing_history_stack = [] # initialize an empty list to store browsing history\n\nbrowsing_history_stack.append('cnn.com')\nbrowsing_history_stack.append('cnn.com/entertainment')\nbrowsing_history_stack.append('cnn.com/entertainment/celebrities')\n\nprint('browsing history: ', browsing_history_stack)\n\n# If we want to revisit the last browsing page: use pop\nprint('Last page visited was: ', browsing_history_stack.pop())\n\n# pop removes the last page and updates the browsing_history_stack\nprint('new browsing history: ', browsing_history_stack)\n\nbrowsing history:  ['cnn.com', 'cnn.com/entertainment', 'cnn.com/entertainment/celebrities']\nLast page visited was:  cnn.com/entertainment/celebrities\nnew browsing history:  ['cnn.com', 'cnn.com/entertainment']\n\n\nQ. Why a List is not ideal for storing the browser history i.e. a Stack?\nAnswer History is dynamic - There is no fixed size of history. In a session, a person can browse 5 websites or 50000. There is no limit. Hence, static arrays or list cannot be used thus, dynamic array is the only option. But with dynamic array there is memory overload problem.\n“The issue with using a list as a stack is that list uses dymanic array internally and when it reaches its capacity it will reallocate a big chunk of memory somewhere else in memory area and copy all the elements. For example in below diagram if a list has a capacity of 10 and we try to insert 11th element, it will not allocate new memory in a different memory region, copy all 10 elements and then insert the 11th element. So overhead here is (1) allocate new memory plus (2) copy all existing elements in new memory area.”\n\n\n\n2. Stack using deque\nRead documentation about deque here.\n\nfrom collections import deque # import deque\n\nstack = deque() # initialize a stack\n\ndir(stack)      # read what different methods are present with a deque class object\n\n['__add__',\n '__class__',\n '__class_getitem__',\n '__contains__',\n '__copy__',\n '__delattr__',\n '__delitem__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getitem__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__iadd__',\n '__imul__',\n '__init__',\n '__init_subclass__',\n '__iter__',\n '__le__',\n '__len__',\n '__lt__',\n '__module__',\n '__mul__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__reversed__',\n '__rmul__',\n '__setattr__',\n '__setitem__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n 'append',\n 'appendleft',\n 'clear',\n 'copy',\n 'count',\n 'extend',\n 'extendleft',\n 'index',\n 'insert',\n 'maxlen',\n 'pop',\n 'popleft',\n 'remove',\n 'reverse',\n 'rotate']\n\n\nNotice, deque has many similar attributes to that of list like append, insert, pop, remove, index. It means using deque stack is going to be exactly same as if we are handling a list object.\n\nstack = deque() \n# add browsing history\nstack.append('cnn.com')\nstack.append('cnn.com/entertainment')\nstack.append('cnn.com/entertainment/celebrities')\n\n# Use of deque is same as a list\nprint('browsing history: ', stack)\nprint('Last page visited was: ', stack.pop())\nprint('new browsing history: ', stack)\n\nbrowsing history:  deque(['cnn.com', 'cnn.com/entertainment', 'cnn.com/entertainment/celebrities'])\nLast page visited was:  cnn.com/entertainment/celebrities\nnew browsing history:  deque(['cnn.com', 'cnn.com/entertainment'])\n\n\nCan we index in or slice a deque stack?\n\nprint(stack[1]) # indexing\nprint(stack[:2])# slicing\n\ncnn.com/entertainment\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[35], line 2\n      1 print(stack[1]) # indexing\n----&gt; 2 print(stack[:2])\n\nTypeError: sequence index must be integer, not 'slice'\n\n\n\nWow!😮\nSo, with deque stacks, we can :\nperform indexing ✅\nbut slicing ❌\nQ. How to perform slicing?\nAns. Possible solutions:\n\nconvert stack to list. For eg: list(stack)[:2]\nget one element at a time in the index range, aggregate and return the aggregated results\n\n\n\nWriting a Stack class\nWhile writing a Stack class, we first need to think what functionalities/methods we want our Stack class to support.\n\nWe want to add elements i.e. push\nWe want to get last element i.e. pop\nWe want to get the size of the stack (e.g. browsing history)\nWe want to see if the stack is_empty\nWe want to see the last element in the stack i.e. peek\nget last_k_items\nprint the current stack i.e. printStack\n\n\nclass Stack:\n    def __init__(self):\n        self.container = deque()\n\n    def push(self, item):\n        self.container.append(item)\n    \n    def pop(self):\n        return self.container.pop()\n    \n    def size(self):\n        return len(self.container)\n    \n    def is_empty(self):\n        return self.size()==0\n\n    def peek(self):\n        return self.container[-1]\n    \n    def last_k_items(self, k=5):\n        return list(self.container)[-k:]\n    \n    def printStack(self):\n        \"\"\"print the stack from bottom to top \"\"\"\n        # make a copy because pop operation would remove elements in place\n        temp = self.container.copy()\n        stack_str = \"\\n\" # initialize a string to print stack\n        stack_str+=\"**--\"\n        # edge case: if stack is empty\n        if len(temp)==0:\n            stack_str+='\\n⚠ Stack is empty'\n        \n        while len(temp)!=0: \n            stack_str+=f\"\\n{str(temp.pop())}\"\n\n        stack_str+=\"\\n--**\"\n        return stack_str\n\n\nbrowsing_stack = Stack() \n# add browsing history\nbrowsing_stack.push('cnn.com')\nbrowsing_stack.push('cnn.com/entertainment')\nbrowsing_stack.push('cnn.com/entertainment/celebrities')\n\n# Test methods\nprint('1. browsing history: ', browsing_stack.printStack())\nprint('2. Last page visited was: ', browsing_stack.pop())\nprint('3. new browsing history: ', browsing_stack.printStack())\nprint('4. Length of history: ', browsing_stack.size())\nprint('5. Is browsing history empty? : ', browsing_stack.is_empty())\n\n1. browsing history:  \n**--\ncnn.com/entertainment/celebrities\ncnn.com/entertainment\ncnn.com\n--**\n2. Last page visited was:  cnn.com/entertainment/celebrities\n3. new browsing history:  \n**--\ncnn.com/entertainment\ncnn.com\n--**\n4. Length of history:  2\n5. Is browsing history empty? :  False\n\n\n\n\nFrom above we can convince easily that the time complexity of Stack is as follows:\n\nIn different languages, Stack can be implemented as follows:\n\n\n\nExercise-1: Write a function in python that can reverse a string using stack data structure (use the Stack class implemented above.)\nreverse_string(\"We will conquere COVID-19\") should return \"91-DIVOC ereuqnoc lliw eW\"\nHint: Treat each character as a browing history item. So our goal is to print browsing history in reverse order\n\n\nShow solution code\ndef reverse_string(string):\n    # initialize a stack\n    mystack = Stack()       \n\n    # push one character at a time\n    for char in string:\n        mystack.push(char)\n\n    reversed_str = \"\"\n    for i in range(mystack.size()):\n        reversed_str+=mystack.pop()\n    return reversed_str\n\n\nreverse_string('We will conquere COVID-19')      \n\n\n'91-DIVOC ereuqnoc lliw eW'\n\n\n\n\nExercise-2: Write a function in python that checks if paranthesis in the string are balanced or not. Possible parantheses are “{}’,”()” or “[]”.\nis_balanced(\"({a+b})\")     --&gt; True\nis_balanced(\"))((a+b}{\")   --&gt; False\nis_balanced(\"((a+b))\")     --&gt; True\nis_balanced(\"))\")          --&gt; False\nis_balanced(\"[a+b]*(x+2y)*{gg+kk}\") --&gt; True\n\n\nShow solution code (Without using Stack)\n# This solution is probably the first solution one would think without using a Stack\ndef is_balanced(string):\n\n    normal_bracket_start = '('\n    curly_bracket_start = '{'\n    square_bracket_start = '['\n\n    normal_bracket_start_count = 0\n    curly_bracket_start_count = 0\n    square_bracket_start_count = 0\n\n    normal_bracket_end = ')'\n    curly_bracket_end = '}'\n    square_bracket_end = ']'\n\n    normal_bracket_end_count = 0\n    curly_bracket_end_count = 0\n    square_bracket_end_count = 0\n\n    # if any of the 3 paranthesis is presnt - it has to start with '(', '{' or '['. If at any instant the count of \n    # any _end bracket &gt; _start of that particular bracket, it means unbalanced.\n    # Also at end, if count of _start != _end for any bracket type =&gt; unbalanced\n\n    # let's iterate through the characters\n    for char in string:\n        # update the counts\n        if char == normal_bracket_start:\n            normal_bracket_start_count+=1\n        elif char == normal_bracket_end:\n            normal_bracket_end_count+=1\n        elif char == curly_bracket_start:\n            curly_bracket_start_count+=1\n        elif char == curly_bracket_end:\n            curly_bracket_end_count+=1\n        elif char == square_bracket_start:\n            square_bracket_start_count+=1\n        elif char == square_bracket_end:\n            square_bracket_end_count+=1\n        \n        # if at any instant count of _end bracket &gt; _start of that particular bracket\n        if normal_bracket_end_count&gt;normal_bracket_start_count or curly_bracket_end_count&gt;curly_bracket_start_count or square_bracket_end_count&gt;square_bracket_start_count:\n            return False\n\n    # at end\n    if normal_bracket_end_count!=normal_bracket_start_count or curly_bracket_end_count!=curly_bracket_start_count or square_bracket_end_count!=square_bracket_start_count:\n        return False\n    return True\n\n\n# Test\nprint(is_balanced(\"({a+b})\"))\nprint(is_balanced(\"))((a+b}{\"))\nprint(is_balanced(\"((a+b))\"))  \nprint(is_balanced(\"))\"))   \nprint(is_balanced(\"[a+b]*(x+2y)*{gg+kk}\"))\n\n\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\nShow solution code (using Stack)\n# For balanced brackets, the closing brackets would appear in a reverse order. FOr ex:\n# if at start we have first normal and then curly like ( { then for closing we will first have a curly and then normal like }) ---(A)\n# We can use stack, where we would store just the opening brackets and the whenever a closing bracket occurs, we pop the last element in the stack. \n# Because the last element of the stack has to be the counterpart of the closing bracket just encountered (as explained in ---(A))\n\ndef is_match(ch1, ch2):\n    \"\"\" check whether the counterpart of ch1 and ch2 are same or not \"\"\"\n    # storing the counterparts for the closing brackets\n    match_dict = {\n        ')': '(',\n        '}': '{',\n        ']': '['\n    }\n    return match_dict[ch1]==ch2\n\ndef is_balanced(string):\n    stack = Stack()\n    for char in string:\n        # only add opening brackets to the stack\n        if char=='(' or char=='{' or char=='[':\n            stack.push(char)\n        # if char is closing bracket\n        if char==')' or char=='}' or char==']':\n            # if there is no opening bracket in the stack and closing bracket starts\n            if stack.size()==0:\n                return False\n            else:\n                # pop the last element in the stack by checking if it is indeed the counter part else return False\n                if not is_match(char, stack.pop()): # stack.pop() gives the last element\n                    return False\n    \n    if stack.size()==0:\n        return True\n    return False\n    \n\n# Test\nprint(is_balanced(\"({a+b})\"))\nprint(is_balanced(\"))((a+b}{\"))\nprint(is_balanced(\"((a+b))\"))  \nprint(is_balanced(\"))\"))   \nprint(is_balanced(\"[a+b]*(x+2y)*{gg+kk}\"))\n\n\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n######################\n# code refactoring tip\n######################\n\n# Instead of writing 3 lines:\nif stack.size()==0:\n        return True\n    return False\n\n# simply write this 1 line:\nreturn stack.size()==0\n\n\n\nResources:\nCodebasics Lecture 7 on Stack"
  },
  {
    "objectID": "posts/2025-06-06-attention-is-all-you-need/nlp_attention.html",
    "href": "posts/2025-06-06-attention-is-all-you-need/nlp_attention.html",
    "title": "Attention is all you need (Draft)",
    "section": "",
    "text": "Transformers are based solely & completely on attention mechanism.\nTransformers completely removed recurrence and convolutions to make computation parallelizable i.e. faster training times but more importantly, capturing long-range dependencies.\nMathematical Proof:\nQ. Follow-up: How Transformer solves this?\nAns: One key factor affecting the ability to learn long-term dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies.\nA self-attention layer connects all positions with a constant number of sequentially executed operations, whereas\na recurrent layer requires O(n) sequential operations, and\na single convolutional layer with kernel width k &lt; n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels.\nQ. Follow-up: Why Recurrent Models are slow?\nAns: Recurrent Models generate a sequence of hidden states \\(h_t\\), as a function of the previous hidden state \\(h_{t−1}\\) and the input for position t. This inherently sequential nature (1) precludes parallelization within training examples, and (2) for longer sequence lengths you start to encounter memory constraints. So, in such cases you need to truncate the size of the sequences.\nQ. Follow-up: Why convolution is slow?\nAns: In Convolution number of operations required to relate signals from two arbitrary input or output positions grows with distance between the position- this makes it more difficult to learn dependencies between distant positions. (Hint: O(n/k)). Transformers reduce this to a constant number of oprations."
  },
  {
    "objectID": "posts/2025-06-06-attention-is-all-you-need/nlp_attention.html#encoder-stack",
    "href": "posts/2025-06-06-attention-is-all-you-need/nlp_attention.html#encoder-stack",
    "title": "Attention is all you need (Draft)",
    "section": "Encoder Stack",
    "text": "Encoder Stack\n\nStack of 6 identical layers\nEach layer has 2 sub-layers\nFirst sub-layer: MSA (Multi-headed self-attention)\nSecond sub-layer: Point-wise Fully-Connected Feed Forward Network\nAlso, there is a Residual connection and LayerNorm across each sublayer (i.e. output of each sub-layer is LayerNorm(x + Sublayer(x)))"
  },
  {
    "objectID": "posts/2025-06-06-attention-is-all-you-need/nlp_attention.html#decoder-stack",
    "href": "posts/2025-06-06-attention-is-all-you-need/nlp_attention.html#decoder-stack",
    "title": "Attention is all you need (Draft)",
    "section": "Decoder Stack",
    "text": "Decoder Stack\n\nStack of 6 identical layers\nEach layer has 3 sub-layers\nFirst sub-layer: MSA (Masked Multi-headed self-attention)\nSecond sub-layer: MA (Multi-headed attention over the output of the encoder stack) : CROSS-ATTENTION\nThird sub-layer: Point-wise Fully-Connected Feed Forward Network\nAlso, there is a Residual connection and LayerNorm across each sublayer (i.e. output of each sub-layer is LayerNorm(x + Sublayer(x)))"
  },
  {
    "objectID": "posts/2025-06-06-attention-is-all-you-need/nlp_attention.html#attention",
    "href": "posts/2025-06-06-attention-is-all-you-need/nlp_attention.html#attention",
    "title": "Attention is all you need (Draft)",
    "section": "Attention",
    "text": "Attention\n“An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.”\n“The output is computed as a weighted sum of the values…” - means softmax(…)*V\n“…where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.” - means \nQ. What is Attention Mechanism?\nAns: ??\n\nScaled Dot-Product Attention\nThey called the attention “Scaled Dot-Product Attention” because it involves scaling with square root of dimension \\(\\sqrt{d_k}\\) + they scale the output with Softmax and the calculation involves dot product between Q & K.\n\nQ: What is Dot-Product Attention vs Scaled Dot-Product Attention?\nAns:Dot-Product Attention is commonly called multiplicative attention. It is identical to Scaled Dot-product attention explained above except that it does not peform scaling with \\(\\sqrt{d_k}\\). In such cases, what happens is that for large dimension \\(d_k\\) values, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, they scaled the dot products by \\(1/\\sqrt{d_k}\\)\nQ: What is additive attention? Prior research suggests it works better than multiplicative attention. The why authors did not use it?\nAns: Additive attention computes the compatibility function using a feed-forward network witha single hidden layer. While the Additive and Multiplicative are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\nAlso, note that it works better than multiplicative attention (vanilla w/o scaling) for large dimension \\(d_k\\) values only because of the probable small gradients in vanilla multiplicative attention (as explained in para above). Whereas for small \\(d_k\\) values, both additive and dot-product are equally well.\nNow, having said that researchers found that using scaling, the multiplicative attention could be significantly improved for large \\(d_k\\).\n\n\nSelf-attention (intra-attention)\nSelf-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.\n\n\n\nMulti-Head Attention\nInstead of performing a single attention function with \\(d_model\\) -dimensional keys, values and queries, they found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to \\(d_k\\), \\(d_k\\) and \\(d_v\\) dimensions, respectively. Attention from each head is computed, concatenated and once again projected, resulting in the final value as shown below:\n\nQ: Why multi-head why not single-head?\nAns:\nComputational wise it is same + you learn more\n\n\nExcerpts\n“relying entirely on an attention mechanism to draw global dependencies between input and output”\n(Mine) - RNNs: global dependencies is constraint in RNNs + processing is sequential (not parallelizable).\nConvolutions: Allows some parallelization. But number of operations required to capture global relations b/w 2 input positions grows exponentially - thus again difficult to capture long-term (global) dependencies"
  },
  {
    "objectID": "posts/2025-06-09-GradCAM/gradcam_project.html",
    "href": "posts/2025-06-09-GradCAM/gradcam_project.html",
    "title": "GradCAM Review",
    "section": "",
    "text": "Full-form of GradCAM\nGradient-Weighted Class Activation Mapping\n\n\nWorking principle of GradCAM?\n\n5 step process:\nStep 1: Forward Pass - Pass the input (image/signal) through the CNN to get the feature maps from the last convolutional layer: \\(A^k\\) - Get raw outputs (logits) before softmax\nStep 2: Select the Target Class - Choose the class c we want to explain (usually the predicted class with highest score) and calculate its score (logit i.e. output before softmax): \\(y^c\\)\nStep 3: Compute the Gradients - Compute the gradient of the target class score \\(y^c\\) with respect to the feature maps \\(A^k\\) of the selected convolutional layer, i.e., \\(\\frac{\\partial y^c}{\\partial A^k}\\)\n\nThese gradients show how important each feature map is for the target class.\n\nStep 4: Compute the Grad-CAM - For each filter k, global average pool the gradients spatially (over width i and height j) to get a single scalar weight \\({a_k}^c\\):\n\\(\\alpha_k^c = \\frac{1}{Z} \\sum_{i} \\sum_{j} \\frac{\\partial y^c}{\\partial A_{ij}^k}\\)\n\n\\(\\alpha_k^c\\) is the importance weight for kth feature map in \\(A^k\\)\nFinally, to calculate the final Grad-CAM, multiply each feature map \\(A^k\\) by its corresponding importance weight \\(\\alpha_k^c\\) and then sum.\nApply a ReLU activation to keep only the parts that positively influence the target class. This makes the Grad-CAM map focus only on features that support the class, not those that suppress it.\n\n\\(L^c_{Grad-CAM} = ReLU (\\sum_{k} \\alpha_k^c A^k)\\)\nStep 5: Post-processing - Resize the Grad-CAM map to the same spatial size as the input\n\n\nWhy we used feature maps \\(A^k\\) from last convolutional layer?\nGrad-CAM uses the feature maps from the last conv layer because it typically has the most high-level, semantically rich features but still retains some spatial information. Using earlier layers would provide too much low-level information (edges, textures) and not enough conceptual understanding.\n\n\nGlobal Average Pooling?\nIt computes the average of all pixel values within each feature map, effectively collapsing each map into a single scalar value. This process reduces the number of parameters in the model, making it less prone to overfitting and more robust to spatial translations.\nThe global average pooling means that you have a 3D 8,8,10 tensor and compute the average over the 8,8 slices, you end up with a 3D tensor of shape 1,1,10 that you reshape into a 1D vector of shape 10. And then you add a softmax operator without any operation in between. The tensor before the average pooling is supposed to have as many channels as your model has classification categories.\nglobal average pooling as implemented in SqueezeNet:\nfinal_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\nself.classifier = nn.Sequential(\n    nn.Dropout(p=0.5),\n    final_conv,\n    nn.ReLU(inplace=True),\n    nn.AvgPool2d(13)\n)\n(512 is the number of channels in the feature maps feeding in to this layer, and 13 is the number of rows and columns in the feature maps going in to this layer. You’ll need to change these depending on your network structure.)\n\n\nWhat is Adaptive Pooling?\n\nIn average-pooling or max-pooling, you essentially set the stride and kernel-size by your own, setting them as hyper-parameters. You will have to re-configure them if you happen to change your input size.\n\nIn Adaptive Pooling on the other hand, we specify the output size instead. And the stride and kernel-size are automatically selected to adapt to the needs. The following equations are used to calculate the value in the source code.\nStride = (input_size//output_size)\nKernel size = input_size - (output_size-1)*stride\nPadding = 0\n(Not related to global average pooling as such but we can also use adaptive_avg_pool2d to achieve global average pooling, just set the output size to (1, 1),\nimport torch.nn.functional as F\nx = F.adaptive_avg_pool2d(x, (1, 1))\n)\n\n\nGeneral conv formula\n\\(n_{out} = \\frac{n_{in} - k + 2p}{s}\\)\n(Derivation) very easy to conceptualize - step 1 (reduce feature map size by kernel size): \\(n_{in} - k\\) - step 2 (add padding on both sides): \\(n_{in} - k + 2p\\) - step 3 (divide by stride): \\(n_{out} = \\frac{n_{in} - k + 2p}{s}\\)\n\n\nOther Approaches to XAI in Computer Vision\n\nVisualizing features learned in different layers\nGradCAM\nGuided Backpropagation\nDeepDream (Need to verify)\nDimensionality Reduction & Feature Visualization (like PCA for last FC layers)\nSensitivity Analysis: (how much each part of the input image is involved in the decision of network’s classifier. The algorithm blocks out different regions of an input image with a sliding gray square and then it runs these occluded images through the network and displays their probabilities for correct class using a heatmap)\nSaliency Map (General)\nLocal Interpretable ModelAgnostic Explanations (LIME)\nSHapely Additive exPlanations (SHAP)\n\n\n\nList of xAI works in Computer Vision\nReference: Paper\n ..  .. \n\n\nt-SNE vs PCA?\nAns: t-SNE for local structure; PCA for global. “In high-dimensional data, it is usually impossible to keep similar data-points close to each other using linear projections (like in PCA). Hence, non-linear methods (like t-SNE) are more suitable in such cases, as they can preserve the local structure of data very well.”\n\n\nWhy explainability?\nHaving appropriate answers to: - (i) Verification of the model; - (ii) Improving a model by understanding its failure points; - (iii) Extracting new insights and hidden laws of the model, and; - (iv) Identifying modules responsible for incorrect decisions.\n\n\nDifference b/w VGG, ResNet, DenseNet, SqueezeNet, InceptionNet?\n🔷 1. VGG (e.g., VGG16, VGG19)\nUniqueness: Very simple and deep architecture using only 3x3 convolutions and stacked layers.\nPros: Easy to implement and understand.\nCons: Large number of parameters (≈138M), leading to slow training and high memory usage.\nKey Idea: Depth over architectural complexity.\n🔷 2. ResNet (Residual Network)\nMotivation (Optional): Intuitively, deeper networks should not perform worse than the shallower networks, but in practice, the deeper networks performed worse than the shallower networks, caused not by overfitting, but by an optimization problem (vanishing gradient problem). This is the problem Resnet tried to solve.\nUniqueness: Introduces skip connections (residual connections) to allow gradient flow through very deep networks.\nPros: Enables training of networks with 100+ layers without vanishing gradient issues.\nCons: Slightly more complex architecture.\nKey Idea: Learn residuals: F(x) + x instead of just F(x).\n Ques: How does Resnet or residual connection prevent vanishing gradient problem? \nAns: Suppose\n\nWithout Residual Connection:\n\n\\(y = \\mathcal{F}(x) \\Rightarrow \\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial \\mathcal{F}}{\\partial x}\\)\nIf \\(\\frac{\\partial \\mathcal{F}}{\\partial x} \\rightarrow 0\\), then the gradient vanishes.\n\nWith Residual Connection:\n\n\\(y = \\mathcal{F}(x) + x \\Rightarrow \\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot \\left( \\frac{\\partial \\mathcal{F}}{\\partial x} + I \\right)\\)\nHere, the identity ensures gradient flow even if \\(\\frac{\\partial \\mathcal{F}}{\\partial x} \\rightarrow 0\\).\n🔷 3. DenseNet (Densely Connected Network)\nUniqueness: Each layer receives input from all previous layers (dense connectivity).\nPros: Encourages feature reuse, fewer parameters than ResNet with similar performance.\nCons: High memory usage due to many connections.\nKey Idea: Dense connectivity between layers: x_l = H([x_0, x_1, …, x_{l-1}]).\n Ques: Resnet vs DenseNet? \nAns:\nResnet - element-wise feature summation - large number of params but, - relatively less memory consumption - memory requirements grow linearly as number of layers increase - relatively faster inference (above point)\nDenseNet - feature concatenation - less number of params but, - more memory consumption: dense connectivity - everything is connected - memory requirements grow quadratically as number of layers increase - slower inference (above point)\n🔷 4. SqueezeNet\nUniqueness: Achieves AlexNet-level accuracy with 50x fewer parameters. - used global average pooling (discussed above). This also reduced the number of parameters\nPros: Very lightweight, ideal for edge devices or deployment in resource-constrained environments.\nCons: Slightly lower accuracy on large datasets.\nKey Idea: Replace 3x3 filters with 1x1, use Fire modules (squeeze + expand).\n🔷 5. InceptionNet (GoogLeNet and its variants)\nUniqueness: Uses Inception modules to capture multi-scale features (1x1, 3x3, 5x5 convolutions in parallel).\nPros: Efficient and computationally optimized, better utilization of model capacity.\nCons: More complex to design and tune.\nKey Idea: Parallel filters with different sizes + dimensionality reduction via 1x1 convolutions.\n Ques: When to use Resnet vs InceptionNet? \nAns:\nResnet: - learns deep feature - useful for general image classification, segmentation tasks\nInceptionNet: - learns multi-scale feature - useful for fine-grained image classification tasks - satellite imagery analysis - texture recognition\n\n\n\nResources\n\nhttps://arxiv.org/pdf/2102.01792\nhttps://research.google/blog/inceptionism-going-deeper-into-neural-networks/\nhttps://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html\nhttps://cs.stanford.edu/people/karpathy/cnnembed/\nhttps://github.com/jacobgil/pytorch-grad-cam\nhttps://xai-tutorials.readthedocs.io/en/latest/_model_specific_xai/Grad-CAM.html"
  },
  {
    "objectID": "posts/2025-06-18-PyLosses/pytorch_losses.html",
    "href": "posts/2025-06-18-PyLosses/pytorch_losses.html",
    "title": "Pytorch Losses Review",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\n\n\nRegression Losses\n\ny_true = torch.tensor([[3.0], [0.0], [2.0], [8.0]]) \ny_pred = torch.tensor([[1.5], [3.5], [4.5], [8.5]]) # error: -1.5, 3.5, 2.5, 0.5\n\n# L1 Loss aka MAE\ncriterion = nn.L1Loss(reduction='sum')\nloss = criterion(y_pred, y_true) # error: |-1.5| + |3.5| + |2.5| + |0.5| = 8\nprint('L1 Loss: ', loss)\n\n# L2 Loss aka MSE\ncriterion = nn.MSELoss(reduction='sum')\nloss = criterion(y_pred, y_true) # error: (-1.5)^2 + (3.5)^2 + (2.5)^2 + (0.5)^2 = 21\nprint('L2 Loss: ', loss)\n\nL1 Loss:  tensor(8.)\nL2 Loss:  tensor(21.)\n\n\n\n When to choose L1 loss and when to choose MSE loss?\n\nUse L1 loss when feature selection is required\nUse L1 loss when outliers are present because L2 loss will make outliers even more (because it squares the difference), making loss more sensitive to outliers\nUse L2 loss generally for well-behaved datasets (i.e. less outliers)\nL2 loss is easier to optimize than L1 loss\n\n\n\n Smooth L1 loss?\n\nDetail: Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. It achieves this by transitioning from a quadratic (MSE-like) loss for small errors to a linear (MAE-like) loss for larger errors.\nPrimary Advantage: tries to solve the problem discussed above: making L2 loss less sensitive to outlier\nOther Advantages:\n\nit is less prone to exploding gradients.\nL2 region provides smoothness over L1Loss near 0\n\n\n\n\n\n Huber Loss?\n\nsimilar to Smooth L1 loss i.e. it tries to make MSE less sensitive to outliers by transitioning from a quadratic (MSE-like) loss for small errors to a linear (MAE-like) loss for larger errors.\nconceptually, it is similar to Smooth L1, it only differs in parametrization\n\n\n\n\nSmooth L1\n\n\nHuber\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMulti-class classification losses\n\ntarget = torch.tensor([1, 0, 4])\nlogits = torch.randn(3, 5, requires_grad=True) # logits are the raw, unnormalized scores output by the model for each class\nprint('Logits: ', logits, '\\n')\n\n# Cross Entropy Loss\ncriterion = nn.CrossEntropyLoss()\nloss = criterion(logits, target)\nprint('Cross Entropy Loss: ', loss)\n\n# Cross Entropy Loss with label smoothing\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1)\nloss = criterion(logits, target)\nprint('Cross Entropy Loss with label smoothing: ', loss)\n\n# NLL Loss\nlog_softmax = nn.LogSoftmax(dim=1)\nlog_softmax_logits = log_softmax(logits)\ncriterion = nn.NLLLoss()\nloss = criterion(log_softmax_logits, target)\nprint('NLL Loss: ', loss)\n\n\nLogits:  tensor([[-0.5214,  0.4200, -0.5788, -0.2010, -0.8349],\n        [ 1.4741, -1.7601, -1.3311, -1.6192, -1.0451],\n        [ 0.2042,  0.6382,  0.4343,  0.7187, -0.8201]], requires_grad=True) \n\nCross Entropy Loss:  tensor(1.3132, grad_fn=&lt;NllLossBackward0&gt;)\nCross Entropy Loss with label smoothing:  tensor(1.3812, grad_fn=&lt;AddBackward0&gt;)\nNLL Loss:  tensor(1.3132, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\n Cross Entropy Loss explained…\nSuppose data is:\n\nWe used normalized logits i.e. after softmac to calculate loss w.r.t. GT\n\nFormula:\n\nCalculation:\n\nNOTE: In Pytorch if one uses the nn.CrossEntropyLoss the input must be unnormalized raw value (aka logits), the target must be class index instead of one hot encoded vectors.\ncriterion = nn.CrossEntropyLoss()\ninput = torch.tensor([[3.2, 1.3,0.2, 0.8]],dtype=torch.float)\ntarget = torch.tensor([0], dtype=torch.long)\ncriterion(input, target)\nOutput: tensor(0.2547)\n\n\n What is input to nn.CrossEntropyLoss()?\nLogits not softmaxed logits\nLogits are unnormalized raw outputs without activation in the last i.e. output layer\n\n\n What is label smoothing?\nLabel smoothing replaces the hard label (e.g.,[1.0, 0.0, 0.0]) with a smoothed version (e.g., [0.9, 0.05, 0.05]).\nThe uniform distribution acts as a regularizer, preventing the model from becoming overly confident in its predictions for a single class.\n\n\n Under the hood: Softmax?\n\nFormula: \nUnder the hood we do not want to compute this directly because if \\(f_i\\)’s are large then computing their exponential causes numerical instability.\nTo solve this, we use a trick. Multiply numerator and denominator by a constant C\n\n\n\nUse mathematical operations to show\n\n\n\nWe keep C = \nBasically this means we make largest term = 0\n\nf = np.array([123, 456, 789]) # example with 3 classes and each having large scores\np = np.exp(f) / np.sum(np.exp(f)) # Bad: Numeric problem, potential blowup\n\n# instead: first shift the values of f so that the highest number is 0:\nf -= np.max(f) # f becomes [-666, -333, 0]\np = np.exp(f) / np.sum(np.exp(f)) # safe to do, gives the correct answer\n\n\n LogSoftmax?\n\\(\\text{LogSoftmax}(x_{i})=\\log \\left(\\frac{\\exp (x_{i})}{\\sum _{j}\\exp (x_{j})}\\right)\\)\nWhy we need Logsoftmax? Consider this:\n\nAs the numbers are too big the exponents will probably blow up (computer cannot handle such big numbers) giving Nan as output. Also, dividing large numbers for softmax calculation, can be numerically unstable.\nAnother advantage of taking log of probabilities: Since the probabilities of independent events multiply, and logarithms convert multiplication to addition, log probabilities of independent events add. Log probabilities are thus practical for computations.\n\n\n Under the hood: nn.CrossEntropyLoss combines LogSoftmax and NLLLoss.\n\n\n Why softmax is used for normalization? Why not other function?\nWhy Softmax 1. \\(e^x\\) is always positive no matter the sign of x. This is good because to read anything as probab or chances, it has to be + 2. training neural networks with \\(e^x\\) is easy because derivative calculation is easy 3. quashes low values. For example: - softmax([1,2])\n- [0.26894142, 0.73105858] # it is a cat perhaps !? - softmax([10,20])\n- [0.0000453978687, 0.999954602] # it is definitely a CAT !\n\nIt applies a lower and upper bound so that they’re understandable.\n\n​Why not others:\n\nMin-Max normalization OR Standard normalization\n\n\nSoftmax normalization reacts to small and large variation/change differently but standard normalization does not differentiate the stimulus by intensity so longest the proportion is the same, for example,\nstd_norm([1,2])\n\n[0.333, 0.667] # it is a cat perhaps !?\n\nstd_norm([10,20])\n\n[0.333, 0.667] # it is a cat perhaps !?\n\nBUT\nsoftmax([1,2])\n\n[0.26894142, 0.73105858] # it is a cat perhaps !?\n\nsoftmax([10,20])\n\n[0.0000453978687, 0.999954602] # it is definitely a CAT !\n\nNot differentiable at min/max boundaries.\nDoesn’t sum to 1 → not valid for probability.\nAnother problem arises when there are negative values in the logits. In that case, you will end up with negative probabilities in the output (if using standard normalization). The Softmax is not affected with negative values because exponent of any value (positive or negative) is always a positive value.\n\nOther Benefits of Softmax:\n\nProbabilistic Interpretation: Softmax ensures all outputs are in [0,1] and sum to 1 → valid probability vector.\nDifferentiability: It is smooth and differentiable → critical for backpropagation in neural nets.\nRelative Comparison: It models relative confidence — a higher \\(z_i\\) leads to much higher \\(𝑝_𝑖\\)\n\n\n\n Categorical cross-entropy vs sparse categorical cross-entropy ?\nBasically they are same thing. It is just unncessary jargon which can cause confusion.\n\nCategorical cross-entropy: is used when true labels are one-hot encoded, for example, we have the following true values for 3-class classification problem [1,0,0], [0,1,0] and [0,0,1].\nIn sparse categorical cross-entropy , truth labels are integer encoded, for example, [1], [2] and [3] for 3-class problem.\n\n\n\n\nBinary classification losses\n\n# binary classification\ntarget = torch.tensor([[1], [0], [1]])\nlogits = torch.tensor([[-2.5], [3.2], [4.8]])\n\n# BCEWithLogitsLoss\n## used for binary classification\ncriterion = nn.BCEWithLogitsLoss() # takes raw logits\nloss = criterion(logits, target.float())\nprint('BCEWithLogitsLoss: ', loss)\n\n# BCELoss\ncriterion = nn.BCELoss() # takes sigmoided logits\nloss = criterion(logits.sigmoid(), target.float())\nprint('BCELoss: ', loss)\n\nBCEWithLogitsLoss:  tensor(1.9423)\nBCELoss:  tensor(1.9423)\n\n\n\n Gradient clipping in Binary Cross Entropy?\n\nif predicted class probab = 0 -&gt; \\(log(p) = -inf\\) : undesirable\n\nMoreover, in gradient descent,and \\(\\frac{d log(p)}{d x} = \\frac{1}{p} = \\frac{1}{0} = inf\\) : also Undesirable\n\nSolution: Gradient clipping: Under the hood Pytorch’s BCELoss clamps its log function outputs to be greater than or equal to -100\n\n\n\n How to use BCEWithLogitsLoss for binary segmentation?\n\ngood practise is to use pos_weight argument to weigh the positive or negative class more.\nBut note that since it is BCEWithLogitsLoss i.e. binary meaning there output neuron is 1 in this case. So, pos_weight is the weight for positive class (e.g. foreground) which can be calculated by counting the number of positive and negative pixels for complete training dataset and use the average of these counts to calculate the pos_weight.\n\n\n\n 2 Strategies for class-imbalance?\n\nuse class weights while calculating loss/ training model to give weights to each class: inversely proportional to their frequency count\nuse an alternative loss like Focal loss\n\n\n\n How to convert multi-class classification into multi-label classification in Pytorch?\n1. Change Target Format\n\nMulti-class: target is a single integer per sample (e.g., tensor([2, 0, 1]))\nMulti-label: target is a binary vector per sample (e.g., tensor([[0,1,1], [1,1,0], [1,0,0]]))\n\n2. Loss Function\n\nMulti-class: nn.CrossEntropyLoss() (expects class index targets)\nMulti-label: nn.BCEWithLogitsLoss() (expects multi-hot binary vectors)\n\n3. Inference\n\nMulti-class:\n# Apply softmax to get probabilities (optional)\nprobs = torch.softmax(logits, dim=1)\n\n# Get predicted class (index of highest probability)\npreds = torch.argmax(logits, dim=1)\nMulti-label:\nprobs = torch.sigmoid(logits)\npredictions = (probs &gt; 0.5).int()  # thresholding\n\n(conceptually: replace softmax with sigmoid)\n\nMulti-class: nn.Softmax(dim=1) (during inference)\nMulti-label: nn.Sigmoid() (applies independently to each class)\n\n\n\n\nContrastive Loss\n\nnn.TripletMarginLoss:\n\ncomputes triplet loss = max(d(a,p)-d(a,n)+margin, 0)\n\nhere d(a,p) is distance b/w a and p using norm. Default is L2 norm\n\n\nnn.TripletMarginWithDistanceLoss:\n\ncomputes triplet loss with a custom distance function = max(d(a,p)-d(a,n)+margin, 0)\n\nhere d(a,p) is a custom function which user can define. Default is pairwise distance\nit can be easily modified to 1-cosine_similarity\n\n\n\n\n When to choose euclidean loss over cosine similarity loss or vice-versa?\nEuclidean Distance - reconstruction (AutoEncoders) - regression - small-dimension size\nCosine Distance - Large dimensional - textual data - NLP, Retrieval\n\n\n\nKL divergence loss\n\nis a measure of how one probability distribution differs from another\nKL Divergence helps you understand how much one set of things (like candies in a bag) is different from another set (like another bag of candies)\nused in\n\nTraining VAE\nKnowledge Distillation\n\nThe key idea behind KL Divergence Loss is to quantify how much information is lost when we try to approximate one distribution (the “true” distribution) with another (the “predicted” distribution).\n\nMathematically, the KL Divergence Loss is defined as:\nKL(P||Q) = Σ P(x) log(P(x) / Q(x))\nWhere P is the true distribution and Q is the predicted/approximated distribution. The KL Divergence is a non-symmetric measure — it tells us how much information is lost when using Q to approximate P, but not vice versa.\n\n Relation b/w CrossEntropyLoss and KL Divergence Loss?\n\nboth cross entropy and KL divergence measure the difference between two probability distributions.\nThe difference is that it calculates the the total entropy between the distributions, while KL divergence represents relative entropy.\nKL divergence measures the information loss when one distribution is used to approximate another\nCross entropy is typically used in supervised learning tasks where there is a fixed target distribution, while KL divergence is more suitable for unsupervised learning tasks and applications involving the comparison or approximation of two probability distributions.\nMathematically, H(P, Q) = H(P) + D_KL(P || Q)\n\nwhere H(P, Q) : Cross Entropy; H(P): Entropy of P\n\nH(P, Q) = - ∑ P(x) * log(Q(x))\nD_KL(P || Q) = ∑ P(x) * log(P(x) / Q(x))\n=&gt; H(P, Q) = H(P) + D_KL(P || Q)\n\n\n\n Entropy explained… Also why is there a negative sign in Entropy?\nFormula:\n\nWhy -ve sign?\nLook at the graph of log(x)\n\n\n\nGraph of log(x)\n\n\n\n\nWe can see that - log(0.00001) -&gt; it is close to -infinity - log(0.99999) -&gt; it is close to 0 (like -0.0000001)\nNow, Entropy reflects Randomness.\nSuppose \\(p = probability of us being correct\\)\nIf p is low =&gt; less confidence =&gt; more Entropy (aka randomness) - if -ve sign is not there then, log(0.00001) = -inf =&gt; meaning very less randomness: which is contradictory.\nSimilarly, if p is high =&gt; less confidence =&gt; more Entropy (aka randomness) - if -ve sign is not there then, log(0.999999) = -0.000001 =&gt; meaning high randomness on full Real scale ranging from -inf to +inf.\nThat is why we use, -ve sign.\nRefer this article for more insights about Entropy and understanding it better!"
  }
]