[
  {
    "objectID": "posts/2025-05-19-linked-list/Linked_List.html",
    "href": "posts/2025-05-19-linked-list/Linked_List.html",
    "title": "Linked List Made Easy",
    "section": "",
    "text": "In this notebook, we will understand and build Linked List from scratch. We will build it step-by-step after understanding its core concepts. I will also perform testing and debugging of the code to show how we read errors, understand and debug them. This is going to be a long blog but quick to read since most of the code is copy pasted multiple times within the notebook.\n\n# type hints\nfrom typing import List, Union\n\nA Linked List is made up of nodes, where nodes contain data and a pointer to the next node.\n\nLet’s create a Node class which will have 2 attributes: data & a pointer to the next node\n\nclass Node:\n    def __init__(self, data=None, next=None):\n        self.data = data\n        self.next = next\n\nLet’s build a LinkedList in baby steps.\nFirst, we will build 2 functions in linked list. One to create a linked list by adding data and another to print it so that we can visualize and debug.\nclass LinkedList:\n    def __init__(self):\n        pass\n\n    def insert_at_beginning(self, data):\n        pass\n\n    def print(self):\n        pass\nOnce we build the above class with 2 simple functions - insert_at_beginning and print, it will be much simpler to complete the remaining functions to add different functionalities to LinkedList\nclass LinkedList:\n    def __init__(self):\n        pass\n\n    def insert_at_beginning(self, data):\n        pass\n\n    def print(self):\n        pass\n\n    def insert_at_end(self, data):\n        pass\n\n    def insert_values(self, data_list):\n        pass\n\n    def get_length(self):\n        pass\n\n    def insert_at(self, index):\n        pass\n\n    def remove_at(self, index):\n        pass\nIt is important to realize whether inputs are required for creating any function. If not, why not. If yes, what type of inputs.\nLet’s get started.\nConcepts: 1. We will add data by creating Nodes. 2. We use head to control any operation on Linked List we want to perform. head is an object always pointing towards the head i.e. start of the linked list\n\n\nclass LinkedList:\n    def __init__(self):\n        # head is an object always pointing towards the head i.e. start of the linked list\n        self.head = None # Step-1: initialize head to none. this will be updated once we add data to the linked list\n\n    def insert_at_beginning(self, data):\n        # In insert_at_beginning, we add element at the start of the linked list\n        \n        node = Node(data, self.head) # Steps-2&3: create a node with data and next pointer to the current head\n        self.head = node             # Step-4: update the current head to the new node just added       \n\n    def print(self):\n        # to print we iterate on the node starting from the first node\n        itr = self.head\n        if itr.data is None:\n            print(\"Linked List is empty\")\n        llstr = \"\"\n        while itr:\n            llstr+=str(itr.data)\n            llstr+='--&gt;'\n            itr = itr.next\n        print(llstr)\n\n\n# Let's test the code we created so far by creating a linked list and adding elements at the beginning\nll = LinkedList()\nll.insert_at_beginning(292)\nll.insert_at_beginning(301)\nll.print()\n\n301--&gt;292--&gt;\n\n\n\n# let's test for an edge case of empty list\nll = LinkedList()\nll.print()\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[84], line 3\n      1 # let's test for an edge case of empty list\n      2 ll = LinkedList()\n----&gt; 3 ll.print()\n\nCell In[82], line 15, in LinkedList.print(self)\n     12 def print(self):\n     13     # to print we iterate on the node starting from the first node\n     14     itr = self.head\n---&gt; 15     if itr.data is None:\n     16         print(\"Linked List is empty\")\n     17     llstr = \"\"\n\nAttributeError: 'NoneType' object has no attribute 'data'\n\n\n\nWe are getting an AttributeError because for an empty Linked List, we are trying to refer an element data that does not even exist. So, issue is for an empty list, we never defined attributes of data instead we only defined the empty Linked List by head i.e. self.head = None. So, we need to change the code slightly and condition over the self.head to check for an empty list\n\nclass LinkedList:\n    def __init__(self):\n        # head is an object always pointing towards the head i.e. start of the linked list\n        self.head = None # initialize head to none. this will be updated once we add data to the linked list\n\n    def insert_at_beginning(self, data):\n        # In insert_at_beginning, we add element at the start of the linked list\n        \n        node = Node(data, self.head) # step-1: create a node with data and next pointer to the current head\n        self.head = node             # step-2: update the current head to the new node just added       \n\n    def print(self):        \n        if self.head is None:\n            print(\"Linked List is empty\")\n        # to print we iterate on the node starting from the first node\n        itr = self.head\n        llstr = \"\"\n        while itr:\n            llstr+=str(itr.data)\n            llstr+='--&gt;'\n            itr = itr.next\n        print(llstr)\n\n\n# let's test for an edge case of empty list\nll = LinkedList()\nll.print()\n\nLinked List is empty\n\n\n\nGreat! this works as expected. Now, let’s add other functions\n\nclass LinkedList:\n    def __init__(self):\n        # head is an object always pointing towards the head i.e. start of the linked list\n        self.head = None # initialize head to none. this will be updated once we add data to the linked list\n\n    def insert_at_beginning(self, data):\n        # In insert_at_beginning, we add element at the start of the linked list\n        \n        node = Node(data, self.head) # step-1: create a node with data and point it to the existing head (i.e. start)\n        self.head = node             # step-2: update the head (i.e. new start) to the new node just added       \n\n    def print(self):        \n        if self.head is None:\n            print(\"Linked List is empty\")\n        # to print we iterate on the node starting from the first node\n        itr = self.head\n        llstr = \"\"\n        while itr:\n            llstr+=str(itr.data)\n            llstr+='--&gt;'\n            itr = itr.next\n        print(llstr)\n\n    def insert_at_end(self, data):\n        # Remember - we use head as the iterator\n        itr = self.head # start from the beginning of the list\n        while itr.next:\n            itr = itr.next # reach the end\n        # once we reach end, do 2 things:\n        # 1. make the node of new data and point it to None because it is the end\n        node = Node(data, None)\n        # 2. update the old end's next to the new node\n        itr.next = node\n        \n        \n\n\n# Let's test the code we created so far:\nll = LinkedList()\nll.insert_at_beginning(292)\nll.insert_at_beginning(301)\nll.insert_at_end(512)\nll.print()\n\n301--&gt;292--&gt;512--&gt;\n\n\nNow let’s code insert_values - which creates a new linked list from the values given as a list.\n\nclass LinkedList:\n    def __init__(self):\n        # head is an object always pointing towards the head i.e. start of the linked list\n        self.head = None # initialize head to none. this will be updated once we add data to the linked list\n\n    def insert_at_beginning(self, data: Union[str, int]):\n        # In insert_at_beginning, we add element at the start of the linked list\n        \n        node = Node(data, self.head) # step-1: create a node with data and point it to the existing head (i.e. start)\n        self.head = node             # step-2: update the head (i.e. new start) to the new node just added       \n\n    def print(self):        \n        if self.head is None:\n            print(\"Linked List is empty\")\n        # to print we iterate on the node starting from the first node\n        itr = self.head\n        llstr = \"\"\n        while itr:\n            llstr+=str(itr.data)\n            llstr+='--&gt;'\n            itr = itr.next\n        print(llstr)\n\n    def insert_at_end(self, data:  Union[str, int]):\n        # Remember - we use head as the iterator\n        itr = self.head # start from the beginning of the list\n        while itr.next:\n            itr = itr.next # reach the end\n        # once we reach end, do 2 things:\n        # 1. make the node of new data and point it to None because it is the end\n        node = Node(data, None)\n        # 2. update the old end's next to the new node\n        itr.next = node\n\n    def insert_values(self, data_list: List):\n        # initialize a new linked list\n        self.head = None\n\n        # Use the insert_at_beginning function\n        for data in data_list:\n            self.insert_at_beginning(data)\n\n\n# Let's test the code we created so far:\nll = LinkedList()\nll.insert_at_beginning(292)\nll.insert_at_beginning(301)\nll.insert_at_end(512)\nll.print()\nnew_data_list = [1,3,5,7,9]\nll = LinkedList()\nll.insert_values(new_data_list)\nll.print()\n\n301--&gt;292--&gt;512--&gt;\n9--&gt;7--&gt;5--&gt;3--&gt;1--&gt;\n\n\nOops! it added elements in reverse order because we used insert_at_beginning. To correct this, there are 2 options: 1. reverse the data_list and use insert_at_beginning: not recommended because it creates computational burden of reversing the list 2. modify the code to use insert_at_end\nMethod-1:reverse the data_list and use insert_at_beginning\n\nclass LinkedList:\n    def __init__(self):\n        # head is an object always pointing towards the head i.e. start of the linked list\n        self.head = None # initialize head to none. this will be updated once we add data to the linked list\n\n    def insert_at_beginning(self, data: Union[str, int]):\n        # In insert_at_beginning, we add element at the start of the linked list\n        \n        node = Node(data, self.head) # step-1: create a node with data and point it to the existing head (i.e. start)\n        self.head = node             # step-2: update the head (i.e. new start) to the new node just added       \n\n    def print(self):        \n        if self.head is None:\n            print(\"Linked List is empty\")\n        # to print we iterate on the node starting from the first node\n        itr = self.head\n        llstr = \"\"\n        while itr:\n            llstr+=str(itr.data)\n            llstr+='--&gt;'\n            itr = itr.next\n        print(llstr)\n\n    def insert_at_end(self, data:  Union[str, int]):\n        # Remember - we use head as the iterator\n        itr = self.head # start from the beginning of the list\n        while itr.next:\n            itr = itr.next # reach the end\n        # once we reach end, do 2 things:\n        # 1. make the node of new data and point it to None because it is the end\n        node = Node(data, None)\n        # 2. update the old end's next to the new node\n        itr.next = node\n\n    def insert_values(self, data_list: List):\n        # initialize a new linked list\n        self.head = None\n\n        # Use the insert_at_beginning function\n        data_list = data_list[::-1] # reverse it\n        for data in data_list:\n            self.insert_at_beginning(data)\n\n\n# Let's test the code we created so far:\nnew_data_list = [1,3,5,7,9]\nll = LinkedList()\nll.insert_values(new_data_list)\nll.print()\n\n1--&gt;3--&gt;5--&gt;7--&gt;9--&gt;\n\n\nMethod-2: Modify the code and use insert_at_end\n\nclass LinkedList:\n    def __init__(self):\n        # head is an object always pointing towards the head i.e. start of the linked list\n        self.head = None # initialize head to none. this will be updated once we add data to the linked list\n\n    def insert_at_beginning(self, data: Union[str, int]):\n        # In insert_at_beginning, we add element at the start of the linked list\n        \n        node = Node(data, self.head) # step-1: create a node with data and point it to the existing head (i.e. start)\n        self.head = node             # step-2: update the head (i.e. new start) to the new node just added       \n\n    def print(self):        \n        if self.head is None:\n            print(\"Linked List is empty\")\n        # to print we iterate on the node starting from the first node\n        itr = self.head\n        llstr = \"\"\n        while itr:\n            llstr+=str(itr.data)\n            llstr+='--&gt;'\n            itr = itr.next\n        print(llstr)\n\n    def insert_at_end(self, data:  Union[str, int]):\n        # Remember - we use head as the iterator\n        itr = self.head # start from the beginning of the list\n        while itr.next:\n            itr = itr.next # reach the end\n        # once we reach end, do 2 things:\n        # 1. make the node of new data and point it to None because it is the end\n        node = Node(data, None)\n        # 2. update the old end's next to the new node\n        itr.next = node\n\n    def insert_values(self, data_list: List):\n        # initialize a new linked list\n        self.head = None\n\n        # Use the insert_at_end function\n        for data in data_list:\n            self.insert_at_end(data)\n\n\n# Let's test the code we created so far:\nnew_data_list = [1,3,5,7,9]\nll = LinkedList()\nll.insert_values(new_data_list)\nll.print()\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[94], line 4\n      2 new_data_list = [1,3,5,7,9]\n      3 ll = LinkedList()\n----&gt; 4 ll.insert_values(new_data_list)\n      5 ll.print()\n\nCell In[93], line 41, in LinkedList.insert_values(self, data_list)\n     39 # Use the insert_at_end function\n     40 for data in data_list:\n---&gt; 41     self.insert_at_end(data)\n\nCell In[93], line 27, in LinkedList.insert_at_end(self, data)\n     24 def insert_at_end(self, data:  Union[str, int]):\n     25     # Remember - we use head as the iterator\n     26     itr = self.head # start from the beginning of the list\n---&gt; 27     while itr.next:\n     28         itr = itr.next # reach the end\n     29     # once we reach end, do 2 things:\n     30     # 1. make the node of new data and point it to None because it is the end\n\nAttributeError: 'NoneType' object has no attribute 'next'\n\n\n\nOops! we got an error! This error is similar to what we got for print function.\nTo debug, try to first stress test the insert_at_end for edge cases\n\nll = LinkedList()\nll.insert_at_end(23)\nll.print()\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[95], line 2\n      1 ll = LinkedList()\n----&gt; 2 ll.insert_at_end(23)\n      3 ll.print()\n\nCell In[93], line 27, in LinkedList.insert_at_end(self, data)\n     24 def insert_at_end(self, data:  Union[str, int]):\n     25     # Remember - we use head as the iterator\n     26     itr = self.head # start from the beginning of the list\n---&gt; 27     while itr.next:\n     28         itr = itr.next # reach the end\n     29     # once we reach end, do 2 things:\n     30     # 1. make the node of new data and point it to None because it is the end\n\nAttributeError: 'NoneType' object has no attribute 'next'\n\n\n\nNow we see the error! Earlier we did not see it because we used insert_at_end to a non-empty Linked List.\nll = LinkedList()\nll.insert_at_beginning(292)\nll.insert_at_beginning(301)\nll.insert_at_end(512) # -&gt; adding at the end to a non-empty list and that's why we didn't see this error\nll.print()\n\nclass LinkedList:\n    def __init__(self):\n        # head is an object always pointing towards the head i.e. start of the linked list\n        self.head = None # initialize head to none. this will be updated once we add data to the linked list\n\n    def insert_at_beginning(self, data: Union[str, int]):\n        # In insert_at_beginning, we add element at the start of the linked list\n        \n        node = Node(data, self.head) # step-1: create a node with data and point it to the existing head (i.e. start)\n        self.head = node             # step-2: update the head (i.e. new start) to the new node just added       \n\n    def print(self):        \n        if self.head is None:\n            print(\"Linked List is empty\")\n        # to print we iterate on the node starting from the first node\n        itr = self.head\n        llstr = \"\"\n        while itr:\n            llstr+=str(itr.data)\n            llstr+='--&gt;'\n            itr = itr.next\n        print(llstr)\n\n    def insert_at_end(self, data:  Union[str, int]):\n        # if linked list is empty\n        if self.head is None:\n            node = Node(data, None) # create a node and point it to None because it is end\n            self.head = node        # make this node as new head (because until now there is no head i.e. linked list is empty)\n\n        # Remember - we use head as the iterator\n        itr = self.head # start from the beginning of the list\n        while itr.next:\n            itr = itr.next # reach the end\n        # once we reach end, do 2 things:\n        # 1. make the node of new data and point it to None because it is the end\n        node = Node(data, None)\n        # 2. update the old end's next to the new node\n        itr.next = node\n\n    def insert_values(self, data_list: List):\n        # initialize a new linked list\n        self.head = None\n\n        # Use the insert_at_end function\n        for data in data_list:\n            self.insert_at_end(data)\n\n\n\nll = LinkedList()\nll.insert_at_end(23)\nll.print()\n\n23--&gt;23--&gt;\n\n\n\n\nll = LinkedList()\nll.insert_at_end(23)\nll.insert_at_end(43)\nll.print()\n\n23--&gt;23--&gt;43--&gt;\n\n\nHere we see the first element getting repeated twice! Why?\nBecause initially, when the Linked List is empty! we added the element but then we did not stop the code after adding the new node to run.\nSimply - To do so, add a return statement\n\nclass LinkedList:\n    def __init__(self):\n        # head is an object always pointing towards the head i.e. start of the linked list\n        self.head = None # initialize head to none. this will be updated once we add data to the linked list\n\n    def insert_at_beginning(self, data: Union[str, int]):\n        # In insert_at_beginning, we add element at the start of the linked list\n        \n        node = Node(data, self.head) # step-1: create a node with data and point it to the existing head (i.e. start)\n        self.head = node             # step-2: update the head (i.e. new start) to the new node just added       \n\n    def print(self):        \n        if self.head is None:\n            print(\"Linked List is empty\")\n            return # if linked list is empty, no need to run the below code. So, return\n        # to print we iterate on the node starting from the first node\n        itr = self.head\n        llstr = \"\"\n        while itr:\n            llstr+=str(itr.data)\n            llstr+='--&gt;'\n            itr = itr.next\n        print(llstr)\n\n    def insert_at_end(self, data:  Union[str, int]):\n        # if linked list is empty\n        if self.head is None:\n            node = Node(data, None) # create a node and point it to None because it is end\n            self.head = node        # make this node as new head (because until now there is no head i.e. linked list is empty)\n            return # to stop the below code from running after adding the node to an empty Linked List\n        \n        # Remember - we use head as the iterator\n        itr = self.head # start from the beginning of the list\n        while itr.next:\n            itr = itr.next # reach the end\n        # once we reach end, do 2 things:\n        # 1. make the node of new data and point it to None because it is the end\n        node = Node(data, None)\n        # 2. update the old end's next to the new node\n        itr.next = node\n\n    def insert_values(self, data_list: List):\n        # initialize a new linked list\n        self.head = None\n\n        # Use the insert_at_end function\n        for data in data_list:\n            self.insert_at_end(data)\n\n\nll = LinkedList()\nll.insert_at_end(23)\nll.print()\n\nll = LinkedList()\nll.insert_at_end(23)\nll.insert_at_end(43)\nll.print()\n\n23--&gt;\n23--&gt;43--&gt;\n\n\nNow, let’s test it for insert_values\n\n# Let's test the code we created so far:\nnew_data_list = [1,3,5,7,9]\nll = LinkedList()\nll.insert_values(new_data_list)\nll.print()\n\n1--&gt;3--&gt;5--&gt;7--&gt;9--&gt;\n\n\nLet’s now complete the code for LinkedList class.\nSome helpful images for visualization for:\n\ninsert_at\n\n\nclass LinkedList:\n    def __init__(self):\n        # head is an object always pointing towards the head i.e. start of the linked list\n        self.head = None # initialize head to none. this will be updated once we add data to the linked list\n\n    def insert_at_beginning(self, data: Union[str, int]):\n        # In insert_at_beginning, we add element at the start of the linked list\n        \n        node = Node(data, self.head) # step-1: create a node with data and point it to the existing head (i.e. start)\n        self.head = node             # step-2: update the head (i.e. new start) to the new node just added       \n\n    def print(self):        \n        if self.head is None:\n            print(\"Linked List is empty\")\n            return # if linked list is empty, no need to run the below code. So, return\n        # to print we iterate on the node starting from the first node\n        itr = self.head\n        llstr = \"\"\n        while itr:\n            llstr+=str(itr.data)\n            llstr+='--&gt;'\n            itr = itr.next\n        print(llstr)\n\n    def insert_at_end(self, data:  Union[str, int]):\n        # if linked list is empty\n        if self.head is None:\n            node = Node(data, None) # create a node and point it to None because it is end\n            self.head = node        # make this node as new head (because until now there is no head i.e. linked list is empty)\n            return # to stop the below code from running after adding the node to an empty Linked List\n        \n        # Remember - we use head as the iterator\n        itr = self.head # start from the beginning of the list\n        while itr.next:\n            itr = itr.next # reach the end\n        # once we reach end, do 2 things:\n        # 1. make the node of new data and point it to None because it is the end\n        node = Node(data, None)\n        # 2. update the old end's next to the new node\n        itr.next = node\n\n    def insert_values(self, data_list: List):\n        # initialize a new linked list\n        self.head = None\n\n        # Use the insert_at_end function\n        for data in data_list:\n            self.insert_at_end(data)\n    \n    def get_length(self):\n        counter = 0\n        # Edge case: if linked list is empty. Then we need to check the `head`\n        if self.head is None:\n            return 0\n        # else, iterate over head\n        itr = self.head\n        while itr:\n            itr = itr.next\n            counter+=1\n        return counter\n    \n    def insert_at(self, index: int, data: Union[str, int]):\n        # Edgecases: adding element at the start, or at the end \n        if index==0:\n            self.insert_at_beginning(data)\n        elif index == self.get_length():\n            self.insert_at_end(data)\n        if index&lt;0 or index&gt;self.get_length():\n            raise Exception(\"Out of range\")\n        # By now, we are adding element in a non-empty list and somewhere in the middle\n        # begin from the head (i.e. start of a lInked list)\n        itr = self.head\n        counter = 0\n        while counter&lt;index-1:\n            itr = itr.next\n            counter+=1\n        # above loop will break as soon as we are at node just before where we want to add the new data\n        # now add new data as a node and pointing to the next node in the original linked list\n        node = Node(data, itr.next)\n        # now point the previous node's next pointer to the new node just added\n        itr.next = node\n\n        \n\n\n# Let's test the code for get_length:\nnew_data_list = [1,3,5,7,9]\nll = LinkedList()\nll.insert_values(new_data_list)\nll.print()\nprint(ll.get_length())\n\nnew_data_list = [1]\nll = LinkedList()\nll.insert_values(new_data_list)\nll.print()\nprint(ll.get_length())\n\nll = LinkedList()\nll.print()\nprint(ll.get_length())\n\nll = LinkedList()\nll.insert_at_beginning(23)\nll.print()\nprint(ll.get_length())\n\nll = LinkedList()\nll.insert_at_end(23)\nll.print()\nprint(ll.get_length())\n\n1--&gt;3--&gt;5--&gt;7--&gt;9--&gt;\n5\n1--&gt;\n1\nLinked List is empty\n0\n23--&gt;\n1\n23--&gt;\n1\n\n\n\n# Let's test the code for insert_at:\nnew_data_list = ['figs','banana', 'mango', 'grapes', 'orange']\nll = LinkedList()\nll.insert_values(new_data_list)\nll.print()\nll.insert_at(2, 'jackfruit')\nll.print()\n\nfigs--&gt;banana--&gt;mango--&gt;grapes--&gt;orange--&gt;\nfigs--&gt;banana--&gt;jackfruit--&gt;mango--&gt;grapes--&gt;orange--&gt;\n\n\n\n# Let's test the code for insert_at:\nnew_data_list = ['figs','banana', 'mango', 'grapes', 'orange']\nll = LinkedList()\nll.insert_values(new_data_list)\nll.print()\nll.insert_at(0, 'jackfruit')\nll.print()\n\nnew_data_list = ['figs','banana', 'mango', 'grapes', 'orange']\nll = LinkedList()\nll.insert_values(new_data_list)\nll.print()\nll.insert_at(5, 'jackfruit')\nll.print()\n\nfigs--&gt;banana--&gt;mango--&gt;grapes--&gt;orange--&gt;\njackfruit--&gt;jackfruit--&gt;figs--&gt;banana--&gt;mango--&gt;grapes--&gt;orange--&gt;\nfigs--&gt;banana--&gt;mango--&gt;grapes--&gt;orange--&gt;\nfigs--&gt;banana--&gt;mango--&gt;grapes--&gt;orange--&gt;jackfruit--&gt;jackfruit--&gt;\n\n\nThis is same error! We forgot to use return statement\n\nclass LinkedList:\n    def __init__(self):\n        # head is an object always pointing towards the head i.e. start of the linked list\n        self.head = None # initialize head to none. this will be updated once we add data to the linked list\n\n    def insert_at_beginning(self, data: Union[str, int]):\n        # In insert_at_beginning, we add element at the start of the linked list\n        \n        node = Node(data, self.head) # step-1: create a node with data and point it to the existing head (i.e. start)\n        self.head = node             # step-2: update the head (i.e. new start) to the new node just added       \n\n    def print(self):        \n        if self.head is None:\n            print(\"Linked List is empty\")\n            return # if linked list is empty, no need to run the below code. So, return\n        # to print we iterate on the node starting from the first node\n        itr = self.head\n        llstr = \"\"\n        while itr:\n            llstr+=str(itr.data)\n            llstr+='--&gt;'\n            itr = itr.next\n        print(llstr)\n\n    def insert_at_end(self, data:  Union[str, int]):\n        # if linked list is empty\n        if self.head is None:\n            node = Node(data, None) # create a node and point it to None because it is end\n            self.head = node        # make this node as new head (because until now there is no head i.e. linked list is empty)\n            return # to stop the below code from running after adding the node to an empty Linked List\n        \n        # Remember - we use head as the iterator\n        itr = self.head # start from the beginning of the list\n        while itr.next:\n            itr = itr.next # reach the end\n        # once we reach end, do 2 things:\n        # 1. make the node of new data and point it to None because it is the end\n        node = Node(data, None)\n        # 2. update the old end's next to the new node\n        itr.next = node\n\n    def insert_values(self, data_list: List):\n        # initialize a new linked list\n        self.head = None\n\n        # Use the insert_at_end function\n        for data in data_list:\n            self.insert_at_end(data)\n    \n    def get_length(self):\n        counter = 0\n        # Edge case: if linked list is empty. Then we need to check the `head`\n        if self.head is None:\n            return 0\n        # else, iterate over head\n        itr = self.head\n        while itr:\n            itr = itr.next\n            counter+=1\n        return counter\n    \n    def insert_at(self, index: int, data: Union[str, int]):\n        # Edgecases: adding element at the start, or at the end \n        if index==0:\n            self.insert_at_beginning(data)\n            return\n        elif index == self.get_length():\n            self.insert_at_end(data)\n            return\n        if index&lt;0 or index&gt;self.get_length():\n            raise Exception(\"Out of range\")\n        # By now, we are adding element in a non-empty list and somewhere in the middle\n        # begin from the head (i.e. start of a lInked list)\n        itr = self.head\n        counter = 0\n        while counter&lt;index-1:\n            itr = itr.next\n            counter+=1\n        # above loop will break as soon as we are at node just before where we want to add the new data\n        # now add new data as a node and pointing to the next node in the original linked list\n        node = Node(data, itr.next)\n        # now point the previous node's next pointer to the new node just added\n        itr.next = node\n\n    def remove_at(self, index: int):\n        if index&lt;0 or index&gt;self.get_length()-1:\n            raise Exception(\"Out of range\")\n        \n        # Edgecases: removing element at the start, or at the end \n        if self.head is None:\n            print('Cannot remove from an empty list')\n            return\n        # remove the first element: simply update the head\n        if index==0:\n            self.head = self.head.next # update the head(i.e. start) to the next node\n            return # DO NOT MISS THIS\n        \n        # By now, we are removing element in a non-empty list and somewhere in the middle or at the end\n        # begin from the head (i.e. start of a lInked list)\n        itr = self.head\n        counter = 0\n        while counter&lt;index-1:\n            itr = itr.next\n            counter+=1\n        # above loop will break as soon as we are at node just before where we want to remove the data\n        # now we remove the node by pointing the current node to the next's next node skipping the node at index = index\n        itr.next = itr.next.next\n        \n        \n        \n\n\n# Let's test the code for insert_at:\nnew_data_list = ['figs','banana', 'mango', 'grapes', 'orange']\nll = LinkedList()\nll.insert_values(new_data_list)\nll.print()\nll.insert_at(0, 'jackfruit')\nll.print()\n\nnew_data_list = ['figs','banana', 'mango', 'grapes', 'orange']\nll = LinkedList()\nll.insert_values(new_data_list)\nll.print()\nll.insert_at(5, 'jackfruit')\nll.print()\n\nfigs--&gt;banana--&gt;mango--&gt;grapes--&gt;orange--&gt;\njackfruit--&gt;figs--&gt;banana--&gt;mango--&gt;grapes--&gt;orange--&gt;\nfigs--&gt;banana--&gt;mango--&gt;grapes--&gt;orange--&gt;\nfigs--&gt;banana--&gt;mango--&gt;grapes--&gt;orange--&gt;jackfruit--&gt;\n\n\n\n# Let's test the code for remove_at:\nnew_data_list = ['figs','banana', 'mango', 'grapes', 'orange']\nll = LinkedList()\nll.insert_values(new_data_list)\nll.print()\nll.remove_at(0)\nll.print()\nprint('')\n\nnew_data_list = ['figs','banana', 'mango', 'grapes', 'orange']\nll = LinkedList()\nll.insert_values(new_data_list)\nll.print()\nll.remove_at(4)\nll.print()\nprint('')\nnew_data_list = ['figs','banana', 'mango', 'grapes', 'orange']\nll = LinkedList()\nll.insert_values(new_data_list)\nll.print()\nll.remove_at(1)\nll.print()\nprint('')\n\nfigs--&gt;banana--&gt;mango--&gt;grapes--&gt;orange--&gt;\nbanana--&gt;mango--&gt;grapes--&gt;orange--&gt;\n\nfigs--&gt;banana--&gt;mango--&gt;grapes--&gt;orange--&gt;\nfigs--&gt;banana--&gt;mango--&gt;grapes--&gt;\n\nfigs--&gt;banana--&gt;mango--&gt;grapes--&gt;orange--&gt;\nfigs--&gt;mango--&gt;grapes--&gt;orange--&gt;\n\n\n\n\n\nExercise-1: add following 2 methods:\ndef insert_after_value(self, data_after, data_to_insert):\n    # Search for first occurance of data_after value in linked list\n    # Now insert data_to_insert after data_after node\n\ndef remove_by_value(self, data):\n    # Remove first node that contains data\n\nclass LinkedList:\n    def __init__(self):\n        # head is an object always pointing towards the head i.e. start of the linked list\n        self.head = None # initialize head to none. this will be updated once we add data to the linked list\n\n    def insert_at_beginning(self, data: Union[str, int]):\n        # In insert_at_beginning, we add element at the start of the linked list\n        \n        node = Node(data, self.head) # step-1: create a node with data and point it to the existing head (i.e. start)\n        self.head = node             # step-2: update the head (i.e. new start) to the new node just added       \n\n    def print(self):        \n        if self.head is None:\n            print(\"Linked List is empty\")\n            return # if linked list is empty, no need to run the below code. So, return\n        # to print we iterate on the node starting from the first node\n        itr = self.head\n        llstr = \"\"\n        while itr:\n            llstr+=str(itr.data)\n            llstr+='--&gt;'\n            itr = itr.next\n        print(llstr)\n\n    def insert_at_end(self, data:  Union[str, int]):\n        # if linked list is empty\n        if self.head is None:\n            node = Node(data, None) # create a node and point it to None because it is end\n            self.head = node        # make this node as new head (because until now there is no head i.e. linked list is empty)\n            return # to stop the below code from running after adding the node to an empty Linked List\n        \n        # Remember - we use head as the iterator\n        itr = self.head # start from the beginning of the list\n        while itr.next:\n            itr = itr.next # reach the end\n        # once we reach end, do 2 things:\n        # 1. make the node of new data and point it to None because it is the end\n        node = Node(data, None)\n        # 2. update the old end's next to the new node\n        itr.next = node\n\n    def insert_values(self, data_list: List):\n        # initialize a new linked list\n        self.head = None\n\n        # Use the insert_at_end function\n        for data in data_list:\n            self.insert_at_end(data)\n    \n    def get_length(self):\n        counter = 0\n        # Edge case: if linked list is empty. Then we need to check the `head`\n        if self.head is None:\n            return 0\n        # else, iterate over head\n        itr = self.head\n        while itr:\n            itr = itr.next\n            counter+=1\n        return counter\n    \n    def insert_at(self, index: int, data: Union[str, int]):\n        # Edgecases: adding element at the start, or at the end \n        if index==0:\n            self.insert_at_beginning(data)\n            return\n        elif index == self.get_length():\n            self.insert_at_end(data)\n            return\n        if index&lt;0 or index&gt;self.get_length():\n            raise Exception(\"Out of range\")\n        # By now, we are adding element in a non-empty list and somewhere in the middle\n        # begin from the head (i.e. start of a lInked list)\n        itr = self.head\n        counter = 0\n        while counter&lt;index-1:\n            itr = itr.next\n            counter+=1\n        # above loop will break as soon as we are at node just before where we want to add the new data\n        # now add new data as a node and pointing to the next node in the original linked list\n        node = Node(data, itr.next)\n        # now point the previous node's next pointer to the new node just added\n        itr.next = node\n\n    def remove_at(self, index: int):\n        if index&lt;0 or index&gt;self.get_length()-1:\n            raise Exception(\"Out of range\")\n        \n        # Edgecases: removing element at the start, or at the end \n        if self.head is None:\n            print('Cannot remove from an empty list')\n            return\n        # remove the first element: simply update the head\n        if index==0:\n            self.head = self.head.next # update the head(i.e. start) to the next node\n            return # DO NOT MISS THIS\n        \n        # By now, we are removing element in a non-empty list and somewhere in the middle or at the end\n        # begin from the head (i.e. start of a lInked list)\n        itr = self.head\n        counter = 0\n        while counter&lt;index-1:\n            itr = itr.next\n            counter+=1\n        # above loop will break as soon as we are at node just before where we want to remove the data\n        # now we remove the node by pointing the current node to the next's next node skipping the node at index = index\n        itr.next = itr.next.next\n        \n        \n    def insert_after_value(self, data_after: Union[str, int], data_to_insert: Union[str, int]):\n        # if linked list is empty, we cannot index the data_after, hence, raise error \n        if self.head is None:\n            raise Exception(\"list is empty!\")\n        \n        # else iterate over linked list, and as we find the value, we insert using insert_at\n        itr = self.head # initialize the iterator at the start of the head\n        counter = 0\n        while itr:\n            value = itr.data\n            if value == data_after:\n                break\n            counter+=1\n            itr = itr.next\n\n        if counter==self.get_length(): # means we could not find data_after in the linked list\n            raise Exception(f'data_after = {data_after} not found in the Linked List.')\n        # else, insert at 1 index away from where we found data_after\n        self.insert_at(counter+1, data=data_to_insert)\n        \n    def remove_by_value(self, data: Union[str, int]):\n        # if linked list is empty, we cannot remove anything from it\n        if self.head is None:\n            raise Exception(\"list is empty!\")\n        \n         # else iterate over linked list, and as we find the value, we remove using remove_at\n        itr = self.head # initialize the iterator at the start of the head\n        counter = 0\n        while itr:\n            value = itr.data\n            if value == data:\n                break\n            counter+=1\n            itr = itr.next\n\n        if counter==self.get_length(): # means we could not find data to remove in the linked list\n            raise Exception(f'data = {data} not found in the Linked List.')\n        # else, remove the same index where we found data\n        self.remove_at(counter)\n\n\n# test insert_after_value\nnew_data_list = ['figs','banana', 'mango', 'grapes', 'orange']\nll = LinkedList()\nll.insert_values(new_data_list)\nprint('Original')\nll.print()\nll.insert_after_value('banana', 'Mousami')\nprint('Modified')\nll.print()\nprint('')\nll.insert_after_value('orange', 'Pineapple')\nprint('Modified-2')\nll.print()\nprint('')\nll.insert_after_value('Apple', 'Custard Apple')\nprint('Modified-3')\nll.print()\nprint('')\n\nOriginal\nfigs--&gt;banana--&gt;mango--&gt;grapes--&gt;orange--&gt;\nModified\nfigs--&gt;banana--&gt;Mousami--&gt;mango--&gt;grapes--&gt;orange--&gt;\n\nModified-2\nfigs--&gt;banana--&gt;Mousami--&gt;mango--&gt;grapes--&gt;orange--&gt;Pineapple--&gt;\n\n\n\n\n---------------------------------------------------------------------------\nException                                 Traceback (most recent call last)\nCell In[112], line 17\n     13 ll.print()\n     14 print('')\n---&gt; 17 ll.insert_after_value('Apple', 'Custard Apple')\n     18 print('Modified-3')\n     19 ll.print()\n\nCell In[110], line 126, in LinkedList.insert_after_value(self, data_after, data_to_insert)\n    123     itr = itr.next\n    125 if counter==self.get_length(): # means we could not find data_after in the linked list\n--&gt; 126     raise Exception(f'data_after = {data_after} not found in the Linked List.')\n    127 # else, insert at 1 index away from where we found data_after\n    128 self.insert_at(counter+1, data=data_to_insert)\n\nException: data_after = Apple not found in the Linked List.\n\n\n\n\n# test remove_by_value\nnew_data_list = ['figs','banana', 'mango', 'grapes', 'orange']\nll = LinkedList()\nll.insert_values(new_data_list)\nprint('Original')\nll.print()\nll.remove_by_value('mango')\nprint('Modified')\nll.print()\nll.remove_by_value('figs')\nprint('Modified-2')\nll.print()\nll.remove_by_value('mango')\nprint('Modified-3')\nll.print()\n\nOriginal\nfigs--&gt;banana--&gt;mango--&gt;grapes--&gt;orange--&gt;\nModified\nfigs--&gt;banana--&gt;grapes--&gt;orange--&gt;\nModified-2\nbanana--&gt;grapes--&gt;orange--&gt;\n\n\n\n---------------------------------------------------------------------------\nException                                 Traceback (most recent call last)\nCell In[117], line 13\n     11 print('Modified-2')\n     12 ll.print()\n---&gt; 13 ll.remove_by_value('mango')\n     14 print('Modified-3')\n     15 ll.print()\n\nCell In[113], line 146, in LinkedList.remove_by_value(self, data)\n    143     itr = itr.next\n    145 if counter==self.get_length(): # means we could not find data to remove in the linked list\n--&gt; 146     raise Exception(f'data = {data} not found in the Linked List.')\n    147 # else, remove the same index where we found data\n    148 self.remove_at(counter)\n\nException: data = mango not found in the Linked List.\n\n\n\n\n\nExcercise-2: Try creating a doubly Linked List by yourself.\n\n\nExercise-3: Can you test for yourself by looking at the code above why this table is correct?\n\nResources:\nCodebasics Lecture on Linked List"
  },
  {
    "objectID": "posts/2025-05-04-decision-trees-basics/1_basic_decision_tree_for_classification.html",
    "href": "posts/2025-05-04-decision-trees-basics/1_basic_decision_tree_for_classification.html",
    "title": "Decision Trees Made Easy",
    "section": "",
    "text": "In this notebook, we will be building a basic decision tree to learn and familiarize ourselves with various governing foundational concepts about decision trees. For this, we will use a relatively simple dataframe containing purely numerical features for the task of classification. As our understanding about decision trees expands, we will extend the concepts learned here to handle complex and challenging dataframes (such as those with missing values, with categorical features, etc.)\nLet’s load the popular iris data. (We are using sklearn only and only to load a popular benchmark dataset. We will not use it to build decision trees. Instead, we will build our decision trees entirely from scratch in using numpy)\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom pprint import pprint\n\n\n# load the data\niris_data = load_iris()\nX = iris_data.data\ny = iris_data.target\n\n# Here X and y are numpy arrays. We do a bit of processing to convert X and y into pandas dataframe for readability\nfeature_names = iris_data.feature_names\ntarget_names = iris_data.target_names\n\ndf = pd.DataFrame(X, columns=feature_names) # start df with features i.e. X\ndf['target'] = y                            # add target column to df i.e. y\n\ndf['target'] = df['target'].apply(lambda x: target_names[x]) # Optional: convert integer labels to actual class names\ndf.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\nNow, we will build a decision tree which when given 4 numerical feature values will classify the plant species as one of 'setosa', 'versicolor', 'virginica'.\nFor this, we first need to split the data into train and val sets. Then we will use the train set to build the model and test it on the val set.\n\ndef train_test_split_df(df, test_size=0.2, random_state=42):\n    \"\"\" splits the data into train and test sets\"\"\"\n\n    if isinstance(test_size, float) and test_size&lt;1:\n        test_size = int(len(df)*test_size)\n    elif isinstance(test_size, int):\n        pass\n    else:\n        raise ValueError(\"test size must be a float/ int\")\n    \n    shuffled_indices = np.random.permutation(len(df))\n    test_indices = shuffled_indices[:test_size]\n    test_df = df.iloc[test_indices]\n    train_df = df.drop(test_indices)\n    return train_df, test_df\n\n\ntrain_df, test_df = train_test_split_df(df, test_size=20, random_state=0)\nprint(f'Train size: {len(train_df)}, Test size: {len(test_df)}')\ntrain_df.head()\n\nTrain size: 130, Test size: 20\n\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n\n\n\n\n\nTo build a decision tree, the dataset is split into 2 subsets using a condition - where one subset satisfies the condition and another does not.\nThe condition is nothing but thresholding of one the features. (For example, condition: sepal lenghth &lt;= 4.4 cms will split the dataset into 2 subsets - one where sepal lenghth is less than 4.4 cms and another where it is not)\nThe main question here is how we decide which feature to use and what threshold to pick?\nAnswer: Pick the feature which results in most information gain.\nNow to understand the above term information gain, we need to first familiarize ourselves with few more concepts.\n\nPurity - Measure of homogenity of a subset. For ex: if a data subset contains only red balls - it is pure. But if it contains even a single green ball - it is now impure. The extent of impurity is measured using entropy.\nEntropy - calculated as:\n  H(X) = — Σ (pi * log2 pi)\n\nwhere; X = Total number of samples, and, pi is the probability of class i\nPure dataset: - One which contains elements belonging to single class. For e.g.: All red balls - For a pure dataset: entropy = 0\nMost Impure dataset: - One which contains elements distributed equally among other classes. For e.g.: 10 blue balls and 10 red balls - For the most impure subset: entropy = 1\n\nInformation Gain - If we split the dataset, then the entropy (i.e. degree of impurity) in the children subsets should ideally be lower. This reduction in entropy is the information gain.\n Information Gain = entropy (parent) — [average entropy of ( children)]\n\nNow, based on the above concepts, we will split the dataset, using the feature that results in highest information gain.\nAnd how we do this?\nAnswer: Brute Force. Yes, Brute Force… We go through all the feature columns one at a time and for each feature column, we go through all of their possible values one at a time, splitting the data into 2 children nodes, calculating information gain, storing it and then pick one which gave us the highest information gain.\nWe repeat this process, either a fixed number of times or until the entropy of children becomes zero (i.e. children subsets become pure) and then we stop.\nLet’s see this step by step…\n\n# let's check the purity of our current data\n\ndef check_purity(df: pd.DataFrame) -&gt; bool:\n    \"\"\"\n    Check if data is pure.\n    \"\"\"\n    y = df.iloc[:, -1]\n    return len(y.unique())==1\n\nprint('Is the train data pure? -&gt;', check_purity(train_df))\n\nIs the train data pure? -&gt; False\n\n\nAs it is not pure. Let’s see how much impure it is by calculating entropy.\n\ndef calculate_entropy(df: pd.DataFrame) -&gt; float:\n    \"\"\"\n    Calculates the entropy of the data. Entropy =  — Σ (pi * log2 pi)\n    \"\"\"\n    y = df.iloc[:, -1]\n    values, counts = np.unique(y, return_counts=True)\n    probs = counts/len(y)\n    entropy = -np.sum(probs*np.log2(probs))\n    return entropy\n\nparent_entropy = calculate_entropy(train_df)\nprint(f\"Entropy of parent node: {parent_entropy}\")\n\nEntropy of parent node: 1.5848773505329046\n\n\nNow, let’s see the details of how we will split the data. The code below should be very straightforward and intuitive to understand.\nAs mentioned, now (1) we will go through all features and their all possible values to split the dataset and then (2)check the information gain. SO, let’s first create the function called get_potential_splits that does (1) for us and then we will implement determine_best_split that will do (2) for us.\n\n# Function (1)\ndef get_potential_splits(df: pd.DataFrame) -&gt; dict:\n    \"\"\"\n    Get all the possible potential splits of the data.\n    \"\"\"\n    potential_splits = {}\n    _, n_columns = df.shape\n    for column_index in range(n_columns - 1): # -1 to skip the target column\n        values = df.iloc[:, column_index]\n        unique_values = np.unique(values)\n        potential_splits[column_index] = [] # initialize a list for storing possible split values per column aka feature\n\n        # using mid-points between 2 consecutive unique values to split the data\n        for i in range(len(unique_values)-1):\n            split_value = (unique_values[i]+unique_values[i+1])/2\n            potential_splits[column_index].append(split_value)\n        \n    return potential_splits\n\n# let's see the potential splits for our data\npotential_splits = get_potential_splits(train_df)\nprint(\"Potential splits for each feature column:\")\nprint(potential_splits)\n\nPotential splits for each feature column:\n{0: [4.35, 4.45, 4.55, 4.65, 4.75, 4.85, 4.95, 5.05, 5.15, 5.25, 5.35, 5.45, 5.55, 5.65, 5.75, 5.85, 5.95, 6.05, 6.15, 6.25, 6.35, 6.45, 6.55, 6.65, 6.75, 6.85, 7.0, 7.15, 7.25, 7.35, 7.5, 7.65], 1: [2.1, 2.25, 2.3499999999999996, 2.45, 2.55, 2.6500000000000004, 2.75, 2.8499999999999996, 2.95, 3.05, 3.1500000000000004, 3.25, 3.3499999999999996, 3.45, 3.55, 3.6500000000000004, 3.75, 3.8499999999999996, 4.0, 4.15], 2: [1.05, 1.2000000000000002, 1.35, 1.45, 1.55, 1.65, 1.7999999999999998, 2.45, 3.15, 3.4, 3.55, 3.6500000000000004, 3.75, 3.8499999999999996, 3.95, 4.05, 4.15, 4.25, 4.35, 4.45, 4.55, 4.65, 4.75, 4.85, 4.95, 5.05, 5.15, 5.25, 5.35, 5.45, 5.55, 5.65, 5.75, 5.85, 5.95, 6.05, 6.199999999999999, 6.449999999999999, 6.65, 6.800000000000001], 3: [0.15000000000000002, 0.25, 0.35, 0.45, 0.55, 0.8, 1.05, 1.15, 1.25, 1.35, 1.45, 1.55, 1.65, 1.75, 1.85, 1.95, 2.05, 2.1500000000000004, 2.25, 2.3499999999999996, 2.45]}\n\n\n0: above is first column i.e. sepal length (cms), 1: is second column and so on.\nNow, we have find all the possible ways to split the data (mentioned as (1))\nNow, let’s see how to get the best split by developing determine_best_split (mentioned as (2)). For this, first create a function (2-1) that will split the data into 2 parts given a feature and its value. Then, we will use this function to split the data for all possible combinations, calculate information gain and pick the one that gives highest information gain (2-2).\n\n# Function (2-1)\ndef split_data(df: pd.DataFrame, split_column: int, split_value: float) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\" \n    SPlit the data into 2 subsets based on split column and split value.\n        split_column (int) : column index\n    \"\"\"\n    split_column_values = df.iloc[:, split_column]\n    left_split = df[split_column_values &lt;= split_value]\n    right_split = df[split_column_values &gt; split_value]\n\n    return left_split, right_split\n\n# Functions (2-2)\ndef determine_best_split(df: pd.DataFrame, potential_splits: dict) -&gt; tuple[int, float]:\n    \"\"\"\n    Determine the best split column and its value.\n    \"\"\"\n    best_split_column = None\n    best_split_value = None\n    best_info_gain = 0\n    parent_entropy = calculate_entropy(df) # Parent Entropy\n    \n    # (Recursive)Iterate over all possible combinations of columns and their split values\n    for column_index in potential_splits.keys():\n        for split_value in potential_splits[column_index]:\n            left_split, right_split = split_data(df, column_index, split_value)\n            average_children_entropy = calculate_average_children_entropy(left_split, right_split)\n            information_gain = parent_entropy - average_children_entropy\n            # print(\"Column:\", iris_data.feature_names[column_index], \"Split value:\", split_value, \"Information gain:\", information_gain)\n            \n            # pick the one with highest information gain\n            if information_gain &gt; best_info_gain:\n                best_info_gain = information_gain\n                best_split_column = column_index\n                best_split_value = split_value\n        \n    #print(\"Best Information Gain:\", best_info_gain)\n    return best_split_column, best_split_value\n\ndef calculate_average_children_entropy(left_split: pd.DataFrame, right_split: pd.DataFrame) -&gt; float:\n    \"\"\"\n    Calculates the overall entropy of the data after splitting i.e. average entropy of the children nodes\n    overall entropy = weighted average of children entropies = Σ (p_c * E(c))\n    \"\"\"\n    n = len(left_split) + len(right_split) # total size of data\n    w_left = len(left_split)/ n            # relative weight of left data\n    w_right = len(right_split)/ n          # relative weight of right data\n    overall_entropy = w_left * calculate_entropy(left_split) + w_right * calculate_entropy(right_split)\n    return overall_entropy\n\nAbove 2 functions should be straightforward to understand. Only new concept is: to get entropy of data after split - we calculate it as average weighted entropy of children nodes.\n(uncomment the print the statement in determine_best_split, if interested to see information gain for all possible splits)\n\nbest_split_column, best_split_value = determine_best_split(train_df, potential_splits)\nprint(\"Best split column:\", iris_data.feature_names[best_split_column], \"with value:\", best_split_value)\n\nBest split column: petal length (cm) with value: 2.45\n\n\nNote: The result one gets could be different if the seed/ random_state in train_val_split_df is changed (because it will change the underlying training data i.e. train_df)\nThe result I got: Best split column: petal length (cm) with value: 2.45\nNow, let’s create the splits\n\nleft_branch = train_df[train_df.iloc[:, best_split_column] &lt;= best_split_value] # branch that satisfies the condition: petal length (cm) &lt;= 2.45\nright_branch = train_df[train_df.iloc[:, best_split_column] &gt; best_split_value]\n\n# # or, we could also do\n# left_branch, right_branch = split_data(train_df, best_split_column, best_split_value)\n\nLet’s verify that after splitting the data has less impurity that is it has less entropy.\n(How do we do that? - By calculating weighted average entropy of children nodes)\n\nprint(f\"Before splitting,  Entropy: {parent_entropy}\")\nprint(f\"After splitting,  Entropy: {calculate_average_children_entropy(left_branch, right_branch)}\")\n\nBefore splitting,  Entropy: 1.5848773505329046\nAfter splitting,  Entropy: 0.6691669882046775\n\n\nSo, this verifies that our splitting was good.\nLet’s see if either of the child node (i.e. data split) is pure\n\ncheck_purity(left_branch), check_purity(right_branch)\n\n(True, False)\n\n\nWow! our left_branch is pure i.e. it contains all the datapoints that has single class. Thus, it would not need any further splitting.\nWhereas right_brach is not pure i.e. it contains datapoints from multiple classes. Thus, it would need further splitting.\n\nnp.unique(left_branch.target) # see classes in left_branch\n\narray(['setosa'], dtype=object)\n\n\n\nnp.unique(right_branch.target, return_counts=True)  # see classes in right_branch\n\n(array(['versicolor', 'virginica'], dtype=object),\n array([43, 44], dtype=int64))\n\n\nBased on just one condition, we can create a small subtree as follows:\nsub_tree = {\"condition\" : [\"left_split_answer\", \"right_split_answer\"]}\nIf we suppose if this small sub-tree is our final decision tree which we want to use for testing. So, if we were to classify one test_example as input, we will check it’s petal length, see if it is less than &lt;= 2.45 cm, we will return setosa as the class, else, we will pick the class with higher relative probability in the right_branch as the predicted class.\n\n# In plain english \nsub_tree = { \"petal length (cm) &lt;= 2.45\" : [\"setosa\", \"versicolor\"]} # right split is versicolor because it is dominant in right_branch\n\nAlmost Never, we create decision tree with only a single condition i.e. 2 child nodes. Usually a decision tree is composed of multiple sub-trees composing multiple conditons.\nProgramatically speaking, we repeat the process of splitting for both the left_branch and right_branch until we reach the stopping condition:\n\nchild nodes become pure\na fixed number of steps by setting hyperparameters like max_depth, min_samples, etc.\n\nNote: There should not be any doubt/confusion when I say that the condition for splitting a left_branch and right_branch belonging to same parent node could be completely different (because it depends upon the data distribution within the child node).\nLet’s build a full-fledged decision tree programatically using concepts of dynamic programming.\n\nDecision Tree code (for case 1. i.e keep splitting until child nodes become pure)N\n(Note: FYI: Case 2. i.e. splitting fo a fixed number of steps is also covered later in this notebook.)\n\ndef decision_tree_algorithm(df: pd.DataFrame) -&gt; dict:\n    data = df\n\n    # base case: If data is pure-&gt; stop and return the class of the child node\n    if check_purity(data):\n        predicted_class = np.unique(data.iloc[:, -1])[0] # only 1 unique value\n        return predicted_class\n    \n    # else: keep on splitting \n    # Recursive\n    else:\n        # for splitting: get_potential_splits -&gt; determine_best_split -&gt; split_data based on best_split_column and best_split_value\n        potential_splits = get_potential_splits(data)\n        best_split_column, best_split_value = determine_best_split(data, potential_splits)\n        left_branch, right_branch = split_data(data, best_split_column, best_split_value)\n\n        \n        condition = \"{} &lt;= {}\".format(list(df.columns)[best_split_column], best_split_value)\n        # create the sub-tree as a dictionary storing the condition as key and a list as the value. This list for a \n        # condition has either the `predicted_class` if the child node is pure or another condition that will further split the \n        # impure child node.\n        sub_tree = {condition: []}\n\n        # get the answer for the 2 child nodes we just created (Step-1) and append them to the sub-tree\n        # (Step-1): get answers\n        left_branch_answer = decision_tree_algorithm(left_branch)\n        right_branch_answer = decision_tree_algorithm(right_branch)\n\n        sub_tree[condition].append(left_branch_answer)\n        sub_tree[condition].append(right_branch_answer)\n\n    return sub_tree\n\n\nmy_tree = decision_tree_algorithm(train_df)\npprint(my_tree)\n\n{'petal length (cm) &lt;= 2.45': ['setosa',\n                               {'petal width (cm) &lt;= 1.75': [{'petal length (cm) &lt;= 4.95': [{'petal width (cm) &lt;= 1.65': ['versicolor',\n                                                                                                                          'virginica']},\n                                                                                            {'petal width (cm) &lt;= 1.55': ['virginica',\n                                                                                                                          {'sepal length (cm) &lt;= 6.95': ['versicolor',\n                                                                                                                                                         'virginica']}]}]},\n                                                             {'petal length (cm) &lt;= 4.85': [{'sepal length (cm) &lt;= 5.95': ['versicolor',\n                                                                                                                           'virginica']},\n                                                                                            'virginica']}]}]}\n\n\nAbove is the decision tree which we created, which can be read as follows. (Do not pay attention to the code but to the print block)\n\nLet’s writ some code to evaluate the decision tree we built.\n\ndef classify_example(example, tree):\n    question = list(tree.keys())[0]\n    feature_name, split_value = question.split(\" &lt;= \")\n    \n\n    # ask question\n    if example[feature_name] &lt;= float(split_value):\n        answer = tree[question][0]\n    else:\n        answer = tree[question][1]\n\n    # base case\n    if not isinstance(answer, dict): # if the answer is not a dictionary, then it is a leaf node\n        return answer\n\n    # recursive case\n    else:\n        residual_tree = answer\n        return classify_example(example, residual_tree)\n    \n\ndef calculate_accuracy(df, tree):\n    df  = df.copy()\n\n    # df[\"classification\"] = df.apply(classify_example, axis=1, args=(tree,))\n    # df[\"classification_correct\"] = df[\"classification\"] == df[\"target\"]\n\n    df.loc[:, \"classification\"] = df.apply(classify_example, axis=1, args=(tree,))\n    df.loc[:, \"classification_correct\"] = df[\"classification\"] == df[\"target\"]\n    \n    accuracy = df[\"classification_correct\"].mean()\n    \n    return accuracy\n\nSince, we created sub_trees in an uninhibited manner, it will result in perfect train accuracy.\n\ncalculate_accuracy(train_df, my_tree)\n\n1.0\n\n\nLet’s check the test accuracy\n\ncalculate_accuracy(test_df, my_tree)\n\n1.0\n\n\nSurprisingly! It also resulted in the perfect test accuracy\n\nControl the depth and min_samples in a decision tree\nIn the above case, we got test accuracy of 100%. But usually this is not the case because datasets are more complex.\nIf we allow the decision tree to grow unhibited manner then it overfits where it is possible that every leaf node would only have one data point. This is not a good decision tree because then prediction from such a tree becomes highly sensitive to small fluctuations in the data.\nHence, now we modify our code to pre-prune the tree i.e. limit its growth using max_depth and min_samples as the hyperparamters\n\n\n\nDecision Tree code (for case 2. i.e. splitting fo a fixed number of steps)\nBefore we write the actual code, we need a function to get the majority class label of the set if the subset is not pure but further splitting is not possible because stopping condition is reached.\n\ndef get_majority_class(df: pd.DataFrame) -&gt; int:\n    \"\"\"\n    Classify the data.\n    \"\"\"\n    y = df.iloc[:, -1]\n    return y.mode()[0]\n\n\ndef decision_tree_improved(df: pd.DataFrame, counter = 0,  min_samples=2, max_depth=5) -&gt; dict:\n\n    data = df\n    # base case: If data is pure or we hit max_depth or min_sample condition violates-&gt; stop and return the class of the child node\n    if check_purity(data) or (len(data) &lt; min_samples) or (counter == max_depth):\n        predicted_class = get_majority_class(data) \n        return predicted_class\n    \n    # else: keep on splitting \n    # Recursive\n    else:\n        counter+=1\n        # for splitting: get_potential_splits -&gt; determine_best_split -&gt; split_data based on best_split_column and best_split_value\n        potential_splits = get_potential_splits(data)\n        best_split_column, best_split_value = determine_best_split(data, potential_splits)\n        left_branch, right_branch = split_data(data, best_split_column, best_split_value)\n\n        \n        condition = \"{} &lt;= {}\".format(list(data.columns)[best_split_column], best_split_value)\n        # create the sub-tree as a dictionary storing the condition as key and a list as the value. This list for a \n        # condition has either the `predicted_class` if the child node is pure or another condition that will further split the \n        # impure child node.\n        sub_tree = {condition: []}\n\n        # get the answer for the 2 child nodes we just created (Step-1) and append them to the sub-tree\n        # (Step-1): get answers\n        left_branch_answer = decision_tree_improved(left_branch,  counter, min_samples, max_depth)\n        right_branch_answer = decision_tree_improved(right_branch, counter, min_samples, max_depth)\n\n        if left_branch_answer == right_branch_answer: # Example: Instead of {'petal length &lt;= 2.5': ['setosa', 'setosa']} just return 'setosa'\n            sub_tree = left_branch_answer\n\n        else:\n            sub_tree[condition].append(left_branch_answer)\n            sub_tree[condition].append(right_branch_answer)\n\n    return sub_tree\n\n\nmy_tree = decision_tree_improved(train_df, max_depth=3)\npprint(my_tree)\n\n{'petal length (cm) &lt;= 2.45': ['setosa',\n                               {'petal width (cm) &lt;= 1.75': [{'petal length (cm) &lt;= 4.95': ['versicolor',\n                                                                                            'virginica']},\n                                                             {'petal length (cm) &lt;= 4.85': ['versicolor',\n                                                                                            'virginica']}]}]}\n\n\n\n\ncalculate_accuracy(test_df, my_tree) # test accuracy with pruned tree\n\n0.95"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Mohit Gupta is a reseacher with a Ph.D. in Computer Vision from Arizona State University. Check out his website to know more about him."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI Blogs",
    "section": "",
    "text": "Linked List Made Easy\n\n\n\nDSA\n\n\n\nCode & Debug Linked Lists.\n\n\n\n\n\nMay 19, 2025\n\n\nMohit Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression Made Easy\n\n\n\nCore ML\n\n\n\nUnderstanding and implementing core concepts related to Linear Regression.\n\n\n\n\n\nMay 18, 2025\n\n\nMohit Gupta\n\n\n\n\n\n\n\n\n\n\n\n\nDecision Trees Made Easy\n\n\n\nCore ML\n\n\n\nUnderstanding and implementing core concepts related to decision trees.\n\n\n\n\n\nMay 1, 2025\n\n\nMohit Gupta\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025-05-18-linear-regression-basics/2_linear_regression.html",
    "href": "posts/2025-05-18-linear-regression-basics/2_linear_regression.html",
    "title": "Linear Regression Made Easy",
    "section": "",
    "text": "In this notebook, we will be building a Linear Regression model from scratch to learn and familiarize ourselves with various governing foundational concepts about it. For this, we will use a sklearn.datasets.make_regression function to create a simple synthetic dataset in one variable. We can extend the concepts learned here to build multi-variate linear regression models.\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_regression\nfrom pprint import pprint\n\n\n# Generate the data\nX,y = make_regression(n_samples=100, n_features=1, noise=7, random_state=42) \n# Here, \n# X.shape: (n_samples, n_features) = (100, 1)\n# y.shape: (n_samples, ) = (100, )\n\nfig, ax = plt.subplots(figsize = (5,3))\nax.scatter(X,y)\nplt.show()\n\n\n\n\n\n\n\n\nWe can see that X axis varies from -2 to 2. It implies that our feature space is already normalized.😄\nHowever, if we are dealing with multiple features, then it is common to have different features in different range. For ex. in a dataset of house price prediction, the house area can range from 100-5000 while number of bedrooms typically range from 1-5. To make the linear regression model to give equal importance to all the features, it is a good practise to bring all the features in same range. So, we normalize the features. There are multiple ways to normalize:\n\nX/max: new feature range is 0 to 1\nX-mean/std: new feature range is -3 to 3 mostly (except outliers)\n(X-min)/(max-min): new feature range is 0 to 1\n\nIn Machine Learning, whenever, we want to build any model, we usually split it into 2 sets - train and val. We build algo on train and finetune its hyperparameters to optimise the loss/error function on val set. This step is mandatory. So, let’s build a helper function\n\ndef train_test_split(X, y, test_size=0.2, random_state=42):\n    \"\"\" splits the data into train and test sets\"\"\"\n\n    np.random.seed(random_state)\n    n = X.shape[0]\n\n    if isinstance(test_size, float) and test_size&lt;1:\n        test_size = int(n*test_size)\n    elif isinstance(test_size, int):\n        pass\n    else:\n        raise ValueError(\"test size must be a float/ int\")\n    \n    shuffled_indices = np.random.permutation(n)\n    test_indices = shuffled_indices[:test_size]\n    train_indices = shuffled_indices[test_size:]\n    X_train = X[train_indices]\n    X_test = X[test_indices]\n    y_train = y[train_indices]\n    y_test = y[test_indices]\n    return X_train, X_test, y_train, y_test\n\n\nX_train, X_val, y_train, y_val = train_test_split(X,y, test_size=20, random_state=0)\nprint(f'Train size: {X_train.shape[0]}, Test size: {X_val.shape[0]}')\n\n# plot the train and val data to see the split\nfig, ax = plt.subplots(figsize = (5,3))\nax.scatter(X_train,y_train, c='blue', label='train')\nax.scatter(X_val,y_val, c='red', label='val')\nax.legend()\nplt.show()\n\nTrain size: 80, Test size: 20\n\n\n\n\n\n\n\n\n\nNow, the question is how do we decide the size of val set and how we should create it?\nAnswer: There is no strict metric on how to decide the size of val set. The most important aspect to consider while creating val set is - it should be a representative of train set. This means, the points in the val set should have a good spread throughout the training data. What does this mean ? - Suppose, all val points (red) occur together and not separated from one another, then it is not a good split. Because this val data does not capture the distribution of train data and since the ultimate use of val data is to optimize the model, then it means we will end up optimizing the model only for a short spread of the data and not the entirety of it. Hence, we mostly perform random sampling to make sure that we get different and spread-out points in the hope that they would be a representative set of the entire training data. To achieve this, we can choose 20% of the data or 30% or 5% depending upon the distrubution of the data we are dealing with. Typical value is 15-25% sampled randomly. (But both the val data percentage and sampling method will vary depending upon the nature of the problem. Read more here )\nAfter creating the splits of train and val data, we can now write code to build a regression model.\n(FYI, if normalization of features is required as mentioned above, then normalization is performed after the data splitting. Various normalization constants are calculated from the train split of the data and stored for processing val and real test data that comes during the production stage.)\nWe are now writing a code for a simplistic model: yhat = wx + b\nwhere w and b are randomly initialized and\noptimized by Gradient Descent\nIn Linear regression, we use mean square error as loss function for optimiztion via Gradient Descent\n\n# hyperparameter\nlr = 0.001\nepochs = 101\n\n# initialize weights and biases randomly\nw = np.random.rand()\nb = np.random.rand()\n\n# Gradient Descent \ntrain_losses = []\nval_losses = []\nfor epoch in range(epochs):\n    # Go through TRAIN data\n    yhat = w*X_train + b\n    yhat = np.squeeze(yhat)\n    # mean square error loss\n    mse_loss = np.mean((y_train-yhat)**2)\n    if (epoch)%20==0:\n        print(f\"EPOCH: {epoch}, Train LOSS:{round(mse_loss, 3)}\")\n    train_losses.append(mse_loss)\n    # step of Gradient Descent\n    w = w - lr*(yhat-y_train)@X_train\n    x0 = np.ones((X_train.shape[0],1))\n    b = b - lr*(yhat-y_train)@x0 # or : b = b - lr * np.sum(yhat - y_train)\n    ## Monitor performance on VAL data\n    yhat = w*X_val + b\n    yhat = np.squeeze(yhat)\n    mse_loss = np.mean((y_val-yhat)**2)\n    if (epoch)%20==0:\n        print(f\"EPOCH: {epoch}, Val LOSS:{round(mse_loss, 3)}\")\n    val_losses.append(mse_loss)\n\nEPOCH: 0, Train LOSS:1567.608\nEPOCH: 0, Val LOSS:1552.406\nEPOCH: 20, Train LOSS:145.819\nEPOCH: 20, Val LOSS:168.799\nEPOCH: 40, Train LOSS:40.83\nEPOCH: 40, Val LOSS:73.231\nEPOCH: 60, Train LOSS:31.699\nEPOCH: 60, Val LOSS:68.759\nEPOCH: 80, Train LOSS:30.857\nEPOCH: 80, Val LOSS:69.459\nEPOCH: 100, Train LOSS:30.777\nEPOCH: 100, Val LOSS:69.843\n\n\n\nfig, ax = plt.subplots(figsize = (5,3))\nax.plot(range(epochs),train_losses, c=\"b\", label = 'train loss')\nax.plot(range(epochs),val_losses, c=\"red\", label = 'val loss')\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\nUsually, you should see decreasing error/loss values. If this does not happen, few things need to be checked: 1. Reduce learning rate, lr and retry 2. Always, check if the shapes of variables are correct. For ex: bias, b shape must be (1,); w: (1, ), mse_loss: (1,), yhat: (num_samples in train/val, ) and so on. I have seen many times, while working with numpy, if you are not careful of matrix multiplication and dot product rules, the shapes of your variables become incorrect causing weird model training. You could also see an up and down behaviour in train loss. For example: If you miss to account for x0 while calculating bias b -&gt; it will cause massive shape issues throughout.\nAnyhow, for our case, we did not encounter any such strange behavior. Let’s see the model we trained:\n\nprint(f\"Linear Regression Model: wx+b = {w}*x+{b}\")\n\nLinear Regression Model: wx+b = [44.0082033]*x+[0.38012626]\n\n\n\nxmin = min(X); yhat_min  = w*xmin+b\nxmax = max(X); yhat_max  = w*xmax+b\n\nfig, ax = plt.subplots(figsize = (5,3))\nax.scatter(X,y, label = 'data')\nax.plot([xmin, xmax], [yhat_min, yhat_max], c=\"red\", label='fitted model')\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe trained model fits really well visually. But how do we quantify the quality of fit?\nAnswer: r2-score\nI often used to forget the formula for r2-score, until I understood the reasoning behind it and then, I no longer needed to memorize it. I could reproduce the formula within seconds by following pure logic. In fact, I feel this is the best way to also sharpen your data understanding skills. Being able to reason about the data stuff and write it in terms of maths - this is the skill that will make you a data scientist with sharp eyes and mind.\n\nr2-score forumla\nLet’s understand the r2-score and derive its formula by asking just basic questions i.e. first principle thinking.\nQ. What is the simplest model we could use without any fancy math?\nA. use mean i.e. the average as the answer, yhat for any given x\nQ. Now, if we use the simplest model, what is the sum of squares of error?\nA. np.sum((y-mean)**2)\nQ. What is the sum of squares of error from our model?\nA. np.sum((y-yhat)**2)\nQ. If we have trained a good model, it should be better than baseline (simplest model, where we predict average no matter what X is). That is, sum of squares of error from trained model &lt; sum of squares of error from simplest model. But how much better?\nA. 1 - np.sum((y-yhat)^2) /np.sum((y-mean)^2)\n\n# simplest model \nymean = np.mean(y_train) # use train to calculate the model, where model = mean\nSST = np.sum((y_val - ymean)**2) # calculte sum of square of error on val data\nprint('SST: ', SST)\n\n# trained model\nyhat = w*X_val + b\nyhat = np.squeeze(yhat)\nRSS = np.sum((y_val-yhat)**2)\nprint('RSS: ', RSS)\n\n# R2-score\nr2_score = 1-RSS/SST\nprint('r2_score: ', r2_score)\n\nSST:  37726.247861497875\nRSS:  1396.867522387115\nr2_score:  0.9629735899653908\n\n\nFor simplest model,\nmodel = ymean, sum of square of error is called SST = Total sum of squares\nFor trained model,\nmodel = wx+b, sum of square of error is called RSS = Sum of square of Residuals\nr2_score close to 1 means the model explains the data well.\nr2_score ranges from 0 to 1. Can you think what what does r² = 0 mean?"
  }
]