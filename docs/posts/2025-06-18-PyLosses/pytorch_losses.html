<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.1">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Mohit Gupta">
<meta name="dcterms.date" content="2025-06-18">
<meta name="description" content="Most commonly used losses in Pytorch">

<title>Pytorch Losses Review – AI Blogs</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-443ac161e34e8c1eedb78f958e3d9213.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-5b8a677417396a6e7de0ff5891f330dc.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-YQ6KB81L7R"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-YQ6KB81L7R', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">AI Blogs</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About Me</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://mgupta70.github.io"> <i class="bi bi-person-circle" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/mgupta70"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Pytorch Losses Review</h1>
                  <div>
        <div class="description">
          Most commonly used losses in Pytorch
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">ML</div>
                <div class="quarto-category">DL</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Mohit Gupta </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">June 18, 2025</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">June 18, 2025</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<div id="cell-1" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="regression-losses" class="level3">
<h3 class="anchored" data-anchor-id="regression-losses">Regression Losses</h3>
<div id="cell-3" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>y_true <span class="op">=</span> torch.tensor([[<span class="fl">3.0</span>], [<span class="fl">0.0</span>], [<span class="fl">2.0</span>], [<span class="fl">8.0</span>]]) </span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> torch.tensor([[<span class="fl">1.5</span>], [<span class="fl">3.5</span>], [<span class="fl">4.5</span>], [<span class="fl">8.5</span>]]) <span class="co"># error: -1.5, 3.5, 2.5, 0.5</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># L1 Loss aka MAE</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.L1Loss(reduction<span class="op">=</span><span class="st">'sum'</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> criterion(y_pred, y_true) <span class="co"># error: |-1.5| + |3.5| + |2.5| + |0.5| = 8</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'L1 Loss: '</span>, loss)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co"># L2 Loss aka MSE</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.MSELoss(reduction<span class="op">=</span><span class="st">'sum'</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> criterion(y_pred, y_true) <span class="co"># error: (-1.5)^2 + (3.5)^2 + (2.5)^2 + (0.5)^2 = 21</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'L2 Loss: '</span>, loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>L1 Loss:  tensor(8.)
L2 Loss:  tensor(21.)</code></pre>
</div>
</div>
<section id="when-to-choose-l1-loss-and-when-to-choose-mse-loss" class="level4">
<h4 class="anchored" data-anchor-id="when-to-choose-l1-loss-and-when-to-choose-mse-loss"><span style="color: green"> <strong>When to choose L1 loss and when to choose MSE loss?</strong></span></h4>
<ul>
<li>Use L1 loss when feature selection is required</li>
<li>Use L1 loss when outliers are present because L2 loss will make outliers even more (because it squares the difference), making loss more sensitive to outliers</li>
<li>Use L2 loss generally for well-behaved datasets (i.e.&nbsp;less outliers)</li>
<li>L2 loss is easier to optimize than L1 loss</li>
</ul>
</section>
<section id="smooth-l1-loss" class="level4">
<h4 class="anchored" data-anchor-id="smooth-l1-loss"><span style="color: green"> <strong>Smooth L1 loss?</strong></span></h4>
<ul>
<li>Detail: Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise. It achieves this by transitioning from a quadratic (MSE-like) loss for small errors to a linear (MAE-like) loss for larger errors.</li>
<li>Primary Advantage: tries to solve the problem discussed above: making L2 loss less sensitive to outlier</li>
<li>Other Advantages:
<ul>
<li>it is less prone to exploding gradients.</li>
<li>L2 region provides smoothness over L1Loss near 0</li>
</ul></li>
</ul>
<p><img src="media/smoothl1.png" width="300"></p>
</section>
<section id="huber-loss" class="level4">
<h4 class="anchored" data-anchor-id="huber-loss"><span style="color: cyan"> <strong>Huber Loss?</strong></span></h4>
<ul>
<li>similar to <code>Smooth L1</code> loss i.e.&nbsp;it tries to make <code>MSE</code> less sensitive to outliers by transitioning from a quadratic (<code>MSE</code>-like) loss for small errors to a linear (<code>MAE</code>-like) loss for larger errors.</li>
<li>conceptually, it is similar to <code>Smooth L1</code>, it only differs in parametrization</li>
</ul>
<table>
<tbody><tr>
<th>
Smooth L1
</th>
<th>
Huber
</th>
</tr>
<tr>
<td>
<img src="media/smooth_l1.png" width="400">
</td>
<td>
<img src="media/huber.png" width="400">
</td>
</tr>
</tbody></table>
</section>
</section>
<section id="multi-class-classification-losses" class="level3">
<h3 class="anchored" data-anchor-id="multi-class-classification-losses">Multi-class classification losses</h3>
<div id="cell-6" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>target <span class="op">=</span> torch.tensor([<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">4</span>])</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> torch.randn(<span class="dv">3</span>, <span class="dv">5</span>, requires_grad<span class="op">=</span><span class="va">True</span>) <span class="co"># logits are the raw, unnormalized scores output by the model for each class</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Logits: '</span>, logits, <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Cross Entropy Loss</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> criterion(logits, target)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Cross Entropy Loss: '</span>, loss)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Cross Entropy Loss with label smoothing</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.CrossEntropyLoss(label_smoothing<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> criterion(logits, target)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Cross Entropy Loss with label smoothing: '</span>, loss)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co"># NLL Loss</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>log_softmax <span class="op">=</span> nn.LogSoftmax(dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>log_softmax_logits <span class="op">=</span> log_softmax(logits)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.NLLLoss()</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> criterion(log_softmax_logits, target)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'NLL Loss: '</span>, loss)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Logits:  tensor([[-0.5214,  0.4200, -0.5788, -0.2010, -0.8349],
        [ 1.4741, -1.7601, -1.3311, -1.6192, -1.0451],
        [ 0.2042,  0.6382,  0.4343,  0.7187, -0.8201]], requires_grad=True) 

Cross Entropy Loss:  tensor(1.3132, grad_fn=&lt;NllLossBackward0&gt;)
Cross Entropy Loss with label smoothing:  tensor(1.3812, grad_fn=&lt;AddBackward0&gt;)
NLL Loss:  tensor(1.3132, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
</div>
<section id="cross-entropy-loss-explained" class="level4">
<h4 class="anchored" data-anchor-id="cross-entropy-loss-explained"><span style="color: green"> <strong>Cross Entropy Loss explained…</strong></span></h4>
<p>Suppose data is:</p>
<p><img src="media/ce_loss_data.webp" width="750"></p>
<p>We used normalized logits i.e.&nbsp;after softmac to calculate loss w.r.t. GT</p>
<p><img src="media/ce_loss.webp" width="350"></p>
<p>Formula:</p>
<p><img src="media/ce_loss_2.webp" width="500"></p>
<p>Calculation:</p>
<p><img src="media/ce_loss_3.webp" width="500"></p>
<p><span style="color: red"><strong>NOTE:</strong></span> In Pytorch if one uses the <code>nn.CrossEntropyLoss</code> the input must be unnormalized raw value (aka logits), the target must be class index instead of one hot encoded vectors.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="bu">input</span> <span class="op">=</span> torch.tensor([[<span class="fl">3.2</span>, <span class="fl">1.3</span>,<span class="fl">0.2</span>, <span class="fl">0.8</span>]],dtype<span class="op">=</span>torch.<span class="bu">float</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>target <span class="op">=</span> torch.tensor([<span class="dv">0</span>], dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>criterion(<span class="bu">input</span>, target)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Output: <code>tensor(0.2547)</code></p>
</section>
<section id="what-is-input-to-nn.crossentropyloss" class="level4">
<h4 class="anchored" data-anchor-id="what-is-input-to-nn.crossentropyloss"><span style="color: green"> <strong>What is input to nn.CrossEntropyLoss()?</strong></span></h4>
<p>Logits not softmaxed logits</p>
<p>Logits are unnormalized raw outputs without activation in the last i.e.&nbsp;output layer</p>
</section>
<section id="what-is-label-smoothing" class="level4">
<h4 class="anchored" data-anchor-id="what-is-label-smoothing"><span style="color: green"> <strong>What is label smoothing?</strong></span></h4>
<p>Label smoothing replaces the hard label (e.g.,[1.0, 0.0, 0.0]) with a smoothed version (e.g., [0.9, 0.05, 0.05]).</p>
<p>The uniform distribution acts as a regularizer, preventing the model from becoming overly confident in its predictions for a single class.</p>
</section>
<section id="under-the-hood-softmax" class="level4">
<h4 class="anchored" data-anchor-id="under-the-hood-softmax"><span style="color: cyan"> <strong>Under the hood: Softmax?</strong></span></h4>
<ul>
<li><p>Formula: <img src="media/softmax_1.png" width="75"></p></li>
<li><p>Under the hood we do not want to compute this directly because if <span class="math inline">\(f_i\)</span>’s are large then computing their exponential causes numerical instability.</p></li>
<li><p>To solve this, we use a trick. Multiply numerator and denominator by a constant C</p>
<ul>
<li><img src="media/softmax_2.png" width="150"></li>
</ul></li>
<li><p>Use mathematical operations to show</p>
<ul>
<li><img src="media/softmax_3.png" width="200"></li>
</ul></li>
<li><p>We keep C = <img src="media/softmax_4.png" width="150"></p></li>
<li><p>Basically this means we make largest term = 0</p></li>
</ul>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> np.array([<span class="dv">123</span>, <span class="dv">456</span>, <span class="dv">789</span>]) <span class="co"># example with 3 classes and each having large scores</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> np.exp(f) <span class="op">/</span> np.<span class="bu">sum</span>(np.exp(f)) <span class="co"># Bad: Numeric problem, potential blowup</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co"># instead: first shift the values of f so that the highest number is 0:</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>f <span class="op">-=</span> np.<span class="bu">max</span>(f) <span class="co"># f becomes [-666, -333, 0]</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> np.exp(f) <span class="op">/</span> np.<span class="bu">sum</span>(np.exp(f)) <span class="co"># safe to do, gives the correct answer</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="logsoftmax" class="level4">
<h4 class="anchored" data-anchor-id="logsoftmax"><span style="color: green"> <strong>LogSoftmax?</strong></span></h4>
<p><span class="math inline">\(\text{LogSoftmax}(x_{i})=\log \left(\frac{\exp (x_{i})}{\sum _{j}\exp (x_{j})}\right)\)</span></p>
<p>Why we need Logsoftmax? Consider this:</p>
<p><img src="media/logsoftmax_1.webp" width="300"></p>
<p>As the numbers are too big the exponents will probably blow up (computer cannot handle such big numbers) giving Nan as output. Also, dividing large numbers for softmax calculation, can be numerically unstable.</p>
<p><strong>Another advantage of taking log of probabilities:</strong> Since the probabilities of independent events multiply, and logarithms convert multiplication to addition, log probabilities of independent events add. Log probabilities are thus practical for computations.</p>
</section>
<section id="under-the-hood-nn.crossentropyloss-combines-logsoftmax-and-nllloss." class="level4">
<h4 class="anchored" data-anchor-id="under-the-hood-nn.crossentropyloss-combines-logsoftmax-and-nllloss."><span style="color: green"> <strong>Under the hood: <code>nn.CrossEntropyLoss</code> combines <code>LogSoftmax</code> and <code>NLLLoss</code>.</strong></span></h4>
</section>
<section id="why-softmax-is-used-for-normalization-why-not-other-function" class="level4">
<h4 class="anchored" data-anchor-id="why-softmax-is-used-for-normalization-why-not-other-function"><span style="color: cyan"> <strong>Why softmax is used for normalization? Why not other function?</strong></span></h4>
<p>Why Softmax 1. <span class="math inline">\(e^x\)</span> is always positive no matter the sign of x. This is good because to read anything as probab or chances, it has to be + 2. training neural networks with <span class="math inline">\(e^x\)</span> is easy because derivative calculation is easy 3. quashes low values. For example: - softmax([1,2])<br>
- [0.26894142, 0.73105858] # it is a cat perhaps !? - softmax([10,20])<br>
- [0.0000453978687, 0.999954602] # it is definitely a CAT !</p>
<ol start="4" type="1">
<li>It applies a lower and upper bound so that they’re understandable.</li>
</ol>
<p>​Why not others:</p>
<ol type="1">
<li>Min-Max normalization OR Standard normalization</li>
</ol>
<ul>
<li><p>Softmax normalization reacts to small and large variation/change differently but standard normalization does not differentiate the stimulus by intensity so longest the proportion is the same, for example,</p></li>
<li><p>std_norm([1,2])</p>
<ul>
<li>[0.333, 0.667] # it is a cat perhaps !?</li>
</ul></li>
<li><p>std_norm([10,20])</p>
<ul>
<li>[0.333, 0.667] # it is a cat perhaps !?</li>
</ul>
<p>BUT</p></li>
<li><p>softmax([1,2])</p>
<ul>
<li>[0.26894142, 0.73105858] # it is a cat perhaps !?</li>
</ul></li>
<li><p>softmax([10,20])</p>
<ul>
<li>[0.0000453978687, 0.999954602] # it is definitely a CAT !</li>
</ul></li>
<li><p>Not differentiable at min/max boundaries.</p></li>
<li><p>Doesn’t sum to 1 → not valid for probability.</p></li>
<li><p>Another problem arises when there are negative values in the logits. In that case, you will end up with negative probabilities in the output (if using standard normalization). The Softmax is not affected with negative values because exponent of any value (positive or negative) is always a positive value.</p></li>
</ul>
<p>Other Benefits of Softmax:</p>
<ol type="1">
<li>Probabilistic Interpretation: Softmax ensures all outputs are in [0,1] and sum to 1 → valid probability vector.</li>
<li>Differentiability: It is smooth and differentiable → critical for backpropagation in neural nets.</li>
<li>Relative Comparison: It models relative confidence — a higher <span class="math inline">\(z_i\)</span> leads to much higher <span class="math inline">\(𝑝_𝑖\)</span></li>
</ol>
</section>
<section id="categorical-cross-entropy-vs-sparse-categorical-cross-entropy" class="level4">
<h4 class="anchored" data-anchor-id="categorical-cross-entropy-vs-sparse-categorical-cross-entropy"><span style="color: cyan"> <strong>Categorical cross-entropy vs sparse categorical cross-entropy ?</strong></span></h4>
<p>Basically they are same thing. It is just unncessary jargon which can cause confusion.</p>
<ul>
<li><strong>Categorical cross-entropy</strong>: is used when true labels are one-hot encoded, for example, we have the following true values for 3-class classification problem [1,0,0], [0,1,0] and [0,0,1].</li>
<li>In <strong>sparse categorical cross-entropy</strong> , truth labels are integer encoded, for example, [1], [2] and [3] for 3-class problem.</li>
</ul>
</section>
</section>
<section id="binary-classification-losses" class="level3">
<h3 class="anchored" data-anchor-id="binary-classification-losses">Binary classification losses</h3>
<div id="cell-12" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># binary classification</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>target <span class="op">=</span> torch.tensor([[<span class="dv">1</span>], [<span class="dv">0</span>], [<span class="dv">1</span>]])</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> torch.tensor([[<span class="op">-</span><span class="fl">2.5</span>], [<span class="fl">3.2</span>], [<span class="fl">4.8</span>]])</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co"># BCEWithLogitsLoss</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co">## used for binary classification</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.BCEWithLogitsLoss() <span class="co"># takes raw logits</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> criterion(logits, target.<span class="bu">float</span>())</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'BCEWithLogitsLoss: '</span>, loss)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="co"># BCELoss</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.BCELoss() <span class="co"># takes sigmoided logits</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> criterion(logits.sigmoid(), target.<span class="bu">float</span>())</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'BCELoss: '</span>, loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>BCEWithLogitsLoss:  tensor(1.9423)
BCELoss:  tensor(1.9423)</code></pre>
</div>
</div>
<section id="gradient-clipping-in-binary-cross-entropy" class="level4">
<h4 class="anchored" data-anchor-id="gradient-clipping-in-binary-cross-entropy"><span style="color: cyan"> <strong>Gradient clipping in Binary Cross Entropy?</strong></span></h4>
<ul>
<li>if predicted class probab = 0 -&gt; <span class="math inline">\(log(p) = -inf\)</span> : undesirable
<ul>
<li>Moreover, in gradient descent,and <span class="math inline">\(\frac{d log(p)}{d x} = \frac{1}{p} = \frac{1}{0} = inf\)</span> : also Undesirable</li>
</ul></li>
<li>Solution: Gradient clipping: Under the hood Pytorch’s <code>BCELoss</code> clamps its log function outputs to be greater than or equal to -100</li>
</ul>
</section>
<section id="how-to-use-bcewithlogitsloss-for-binary-segmentation" class="level4">
<h4 class="anchored" data-anchor-id="how-to-use-bcewithlogitsloss-for-binary-segmentation"><span style="color: cyan"> <strong>How to use BCEWithLogitsLoss for binary segmentation?</strong></span></h4>
<ul>
<li>good practise is to use <code>pos_weight</code> argument to weigh the positive or negative class more.</li>
<li>But note that since it is <code>BCEWithLogitsLoss</code> i.e.&nbsp;binary meaning there output neuron is 1 in this case. So, <code>pos_weight</code> is the weight for positive class (e.g.&nbsp;foreground) which can be calculated by counting the number of positive and negative pixels for complete training dataset and use the average of these counts to calculate the <code>pos_weight</code>.</li>
</ul>
</section>
<section id="strategies-for-class-imbalance" class="level4">
<h4 class="anchored" data-anchor-id="strategies-for-class-imbalance"><span style="color: green"> <strong>2 Strategies for class-imbalance?</strong></span></h4>
<ol type="1">
<li>use <code>class weights</code> while calculating loss/ training model to give weights to each class: inversely proportional to their frequency count</li>
<li>use an alternative loss like Focal loss</li>
</ol>
</section>
<section id="how-to-convert-multi-class-classification-into-multi-label-classification-in-pytorch" class="level4">
<h4 class="anchored" data-anchor-id="how-to-convert-multi-class-classification-into-multi-label-classification-in-pytorch"><span style="color: cyan"> <strong>How to convert multi-class classification into multi-label classification in Pytorch?</strong></span></h4>
<p><strong>1. Change Target Format</strong></p>
<ul>
<li><p>Multi-class: target is a single integer per sample (e.g., tensor([2, 0, 1]))</p></li>
<li><p>Multi-label: target is a binary vector per sample (e.g., tensor([[0,1,1], [1,1,0], [1,0,0]]))</p></li>
</ul>
<p><strong>2. Loss Function</strong></p>
<ul>
<li><p>Multi-class: <code>nn.CrossEntropyLoss()</code> (expects class index targets)</p></li>
<li><p>Multi-label: <code>nn.BCEWithLogitsLoss()</code> (expects multi-hot binary vectors)</p></li>
</ul>
<p><strong>3. Inference</strong></p>
<ul>
<li><p>Multi-class:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply softmax to get probabilities (optional)</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> torch.softmax(logits, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Get predicted class (index of highest probability)</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>preds <span class="op">=</span> torch.argmax(logits, dim<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Multi-label:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> torch.sigmoid(logits)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> (probs <span class="op">&gt;</span> <span class="fl">0.5</span>).<span class="bu">int</span>()  <span class="co"># thresholding</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ul>
<p>(conceptually: replace softmax with sigmoid)</p>
<ul>
<li><p>Multi-class: <code>nn.Softmax(dim=1)</code> (during inference)</p></li>
<li><p>Multi-label: <code>nn.Sigmoid()</code> (applies independently to each class)</p></li>
</ul>
</section>
</section>
<section id="contrastive-loss" class="level3">
<h3 class="anchored" data-anchor-id="contrastive-loss">Contrastive Loss</h3>
<ol type="1">
<li><code>nn.TripletMarginLoss</code>:
<ul>
<li>computes triplet loss = max(d(a,p)-d(a,n)+margin, 0)
<ul>
<li>here d(a,p) is distance b/w a and p using norm. Default is L2 norm</li>
</ul></li>
</ul></li>
<li><code>nn.TripletMarginWithDistanceLoss</code>:
<ul>
<li>computes triplet loss with a custom distance function = max(d(a,p)-d(a,n)+margin, 0)
<ul>
<li>here d(a,p) is a custom function which user can define. Default is pairwise distance</li>
<li>it can be easily modified to 1-<code>cosine_similarity</code></li>
</ul></li>
</ul></li>
</ol>
<section id="when-to-choose-euclidean-loss-over-cosine-similarity-loss-or-vice-versa" class="level4">
<h4 class="anchored" data-anchor-id="when-to-choose-euclidean-loss-over-cosine-similarity-loss-or-vice-versa"><span style="color: cyan"> <strong>When to choose euclidean loss over cosine similarity loss or vice-versa?</strong></span></h4>
<p>Euclidean Distance - reconstruction (AutoEncoders) - regression - small-dimension size</p>
<p>Cosine Distance - Large dimensional - textual data - NLP, Retrieval</p>
</section>
</section>
<section id="kl-divergence-loss" class="level3">
<h3 class="anchored" data-anchor-id="kl-divergence-loss">KL divergence loss</h3>
<ul>
<li>is a measure of how one probability distribution differs from another</li>
<li>KL Divergence helps you understand how much one set of things (like candies in a bag) is different from another set (like another bag of candies)</li>
<li>used in
<ul>
<li>Training VAE</li>
<li>Knowledge Distillation</li>
</ul></li>
<li>The key idea behind KL Divergence Loss is to quantify how much information is lost when we try to approximate one distribution (the “true” distribution) with another (the “predicted” distribution).</li>
</ul>
<p>Mathematically, the KL Divergence Loss is defined as:</p>
<p>KL(P||Q) = Σ P(x) log(P(x) / Q(x))</p>
<p>Where P is the true distribution and Q is the predicted/approximated distribution. The KL Divergence is a non-symmetric measure — it tells us how much information is lost when using Q to approximate P, but not vice versa.</p>
<section id="relation-bw-crossentropyloss-and-kl-divergence-loss" class="level4">
<h4 class="anchored" data-anchor-id="relation-bw-crossentropyloss-and-kl-divergence-loss"><span style="color: cyan"> <strong>Relation b/w CrossEntropyLoss and KL Divergence Loss?</strong></span></h4>
<ul>
<li><p>both cross entropy and KL divergence measure the difference between two probability distributions.</p></li>
<li><p>The difference is that it calculates the the total entropy between the distributions, while KL divergence represents relative entropy.</p></li>
<li><p>KL divergence measures the information loss when one distribution is used to approximate another</p></li>
<li><p>Cross entropy is typically used in supervised learning tasks where there is a fixed target distribution, while KL divergence is more suitable for unsupervised learning tasks and applications involving the comparison or approximation of two probability distributions.</p></li>
<li><p>Mathematically, H(P, Q) = H(P) + D_KL(P || Q)</p>
<ul>
<li>where H(P, Q) : Cross Entropy; H(P): Entropy of P</li>
</ul></li>
<li><p>H(P, Q) = - ∑ P(x) * log(Q(x))</p></li>
<li><p>D_KL(P || Q) = ∑ P(x) * log(P(x) / Q(x))</p></li>
<li><p>=&gt; H(P, Q) = H(P) + D_KL(P || Q)</p></li>
</ul>
</section>
<section id="entropy-explained-also-why-is-there-a-negative-sign-in-entropy" class="level4">
<h4 class="anchored" data-anchor-id="entropy-explained-also-why-is-there-a-negative-sign-in-entropy"><span style="color: cyan"> <strong>Entropy explained… Also why is there a negative sign in Entropy?</strong></span></h4>
<p>Formula:</p>
<p><img src="media/entropy.webp" width="300"></p>
<p><u><strong>Why -ve sign?</strong></u></p>
<p>Look at the graph of log(x)</p>
<div style="background-color:rgb(255, 255, 255); display: flex; gap: 20px; padding: 10px; justify-content: center;">
<figure style="margin: 0; text-align: center;" class="figure">
<figcaption>
<b>Graph of log(x)</b>
</figcaption>
<img src="media/entropy2.png" width="300" class="figure-img">
</figure>
</div>
<p>We can see that - log(0.00001) -&gt; it is close to -infinity - log(0.99999) -&gt; it is close to 0 (like -0.0000001)</p>
<p>Now, Entropy reflects Randomness.</p>
<p>Suppose <span class="math inline">\(p = probability of us being correct\)</span></p>
<p>If p is low =&gt; less confidence =&gt; more Entropy (aka randomness) - if -ve sign is not there then, log(0.00001) = -inf =&gt; meaning very less randomness: which is contradictory.</p>
<p>Similarly, if p is high =&gt; less confidence =&gt; more Entropy (aka randomness) - if -ve sign is not there then, log(0.999999) = -0.000001 =&gt; meaning high randomness on full Real scale ranging from -inf to +inf.</p>
<p>That is why we use, -ve sign.</p>
<p>Refer this <a href="https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e/">article</a> for more insights about Entropy and understanding it better!</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/mgupta70\.github\.io");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>