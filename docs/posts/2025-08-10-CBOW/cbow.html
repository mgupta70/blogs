<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.1">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Mohit Gupta">
<meta name="dcterms.date" content="2025-08-03">
<meta name="description" content="Implementing Word2Vec Paper">

<title>Comprehensive deep dive and implementation of Continuous Bag-of-Words – AI Blogs</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-443ac161e34e8c1eedb78f958e3d9213.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-5b8a677417396a6e7de0ff5891f330dc.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-YQ6KB81L7R"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-YQ6KB81L7R', { 'anonymize_ip': true});
</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">AI Blogs</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About Me</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://mgupta70.github.io"> <i class="bi bi-person-circle" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/mgupta70"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Comprehensive deep dive and implementation of Continuous Bag-of-Words</h1>
                  <div>
        <div class="description">
          Implementing Word2Vec Paper
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">NLP</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Mohit Gupta </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">August 3, 2025</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">August 10, 2025</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<div id="656ef3ea" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># For text processing</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> stopwords</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">'stopwords'</span>) <span class="co"># download stopwords</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchtext.data <span class="im">import</span> get_tokenizer</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co"># For Neural Network</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.optim.lr_scheduler <span class="im">import</span> StepLR, LambdaLR</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(device)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># For time measurement</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co"># For visualization</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.manifold <span class="im">import</span> TSNE</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>[nltk_data] Downloading package stopwords to
[nltk_data]     C:\Users\mgupta70.ASURITE\AppData\Roaming\nltk_data...
[nltk_data]   Package stopwords is already up-to-date!</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>cuda</code></pre>
</div>
</div>
<p><strong>Paper title</strong>: Efficient Estimation of Word Representations in Vector Space</p>
<p>This paper proposed 4 methods to learn word vectors.</p>
<ol type="1">
<li>NLLM</li>
<li>RNNLM</li>
<li>CBOW</li>
<li>Skip-gram</li>
</ol>
<p>Some details about them as mentioned in the paper:</p>
<ol type="1">
<li>NLLM - a feedforward NN with following layers: i/p, projection, hidden and o/p</li>
<li>RNNLM - removes the need of projection layer. Thus, i/p, hidden and o/p</li>
<li>CBOW - predict current word from R previous and R future words (context words)</li>
<li>Skip-gram - predict context words from given current word (i.e.&nbsp;predict 2R words) (Multi label multi-class classification)</li>
</ol>
<p>Before going into details of CBOW method, let’s load some data.</p>
<div id="9f1b76dc" class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> load_dataset(<span class="st">"wikitext"</span>, <span class="st">"wikitext-2-raw-v1"</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>train_texts <span class="op">=</span> dataset[<span class="st">'train'</span>][<span class="st">'text'</span>]</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>val_texts <span class="op">=</span> dataset[<span class="st">'validation'</span>][<span class="st">'text'</span>]</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Train sample: "</span>, train_texts[<span class="dv">1</span>] )</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Val sample: "</span>, val_texts[<span class="dv">1</span>])</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Train: Val = "</span>, <span class="bu">len</span>(train_texts), <span class="st">":"</span>, <span class="bu">len</span>(val_texts))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><code>train_texts</code> and <code>val_texts</code> are basically <code>list</code>s of strings. Let’s join all the strings into a single large string called <code>train_text</code> and <code>val_text</code></p>
<div id="9171207c" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>train_text <span class="op">=</span> <span class="st">""</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> text_paragraph <span class="kw">in</span> train_texts:</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    train_text <span class="op">=</span> train_text <span class="op">+</span> <span class="st">" "</span> <span class="op">+</span> text_paragraph</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>val_text <span class="op">=</span> <span class="st">""</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> text_paragraph <span class="kw">in</span> val_texts:</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    val_text <span class="op">=</span> val_text <span class="op">+</span> <span class="st">" "</span> <span class="op">+</span> text_paragraph</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of words in the corpus (in Millions):"</span>, <span class="bu">len</span>(train_text.split())<span class="op">/</span><span class="dv">1000000</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of words in the validation corpus (in Millions):"</span>, <span class="bu">len</span>(val_text.split())<span class="op">/</span><span class="dv">1000000</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of words in the corpus (in Millions): 2.05191
Number of words in the validation corpus (in Millions): 0.213886</code></pre>
</div>
</div>
<div id="0dcea1ea" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># preprocess the text: tokenize, lowercase, remove punctuation, etc.</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize_text(text, make_lower<span class="op">=</span><span class="va">True</span>, remove_stopwords<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># lowercase</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> make_lower:</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> text.lower()</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># remove punctuation</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> re.sub(<span class="vs">r'</span><span class="pp">[^</span><span class="dv">\w\s</span><span class="pp">]</span><span class="vs">'</span>, <span class="st">''</span>, text)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># tokenize: assume that words are separated by spaces</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    tokenizer <span class="op">=</span> get_tokenizer(<span class="st">"basic_english"</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    words <span class="op">=</span> tokenizer(text)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># remove stopwords</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> remove_stopwords:</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>        stop_words <span class="op">=</span> <span class="bu">set</span>(stopwords.words(<span class="st">'english'</span>))</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>        words <span class="op">=</span> [word <span class="cf">for</span> word <span class="kw">in</span> words <span class="cf">if</span> word <span class="kw">not</span> <span class="kw">in</span> stop_words]</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> words</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="a54c4fb6" class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># preprocess the train text</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>train_words <span class="op">=</span> tokenize_text(train_text, remove_stopwords<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> <span class="bu">set</span>(train_words)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>V <span class="op">=</span> <span class="bu">len</span>(vocab)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"--------------------------------"</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of tokens in the corpus (in Millions):"</span>, <span class="bu">len</span>(train_words)<span class="op">/</span><span class="dv">1000000</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Vocabulary size, V:"</span>, V)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="co"># preprocess the validation text</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>val_words <span class="op">=</span> tokenize_text(val_text, remove_stopwords<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>--------------------------------
Number of tokens in the corpus (in Millions): 1.749369
Vocabulary size, V: 66102</code></pre>
</div>
</div>
<div id="d558ee2f" class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># let's see some of the train words by frequency in descending order</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>train_word_counts <span class="op">=</span> Counter(train_words)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>train_word_counts.most_common()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="52">
<pre><code>[('the', 130769),
 ('of', 57032),
 ('and', 50735),
 ('in', 45016),
 ('to', 39522),
 ('a', 36453),
 ('was', 21008),
 ('on', 15141),
 ('as', 15062),
 ('s', 14488),
 ('that', 14351),
 ('for', 13794),
 ('with', 13012),
 ('by', 12718),
 ('is', 11692),
 ('it', 9276),
 ('from', 9229),
 ('at', 9071),
 ('his', 9020),
 ('he', 8708),
 ('were', 7334),
 ('an', 6251),
 ('had', 5707),
 ('which', 5546),
 ('be', 4860),
 ('are', 4714),
 ('this', 4560),
 ('their', 4290),
 ('first', 4245),
 ('but', 4233),
 ('not', 4006),
 ('one', 3914),
 ('they', 3894),
 ('its', 3878),
 ('also', 3842),
 ('after', 3749),
 ('her', 3670),
 ('or', 3657),
 ('two', 3565),
 ('have', 3470),
 ('has', 3325),
 ('been', 3263),
 ('who', 3029),
 ('she', 2884),
 ('new', 2767),
 ('other', 2729),
 ('during', 2690),
 ('when', 2655),
 ('time', 2607),
 ('all', 2557),
 ('into', 2443),
 ('more', 2402),
 ('would', 2332),
 ('1', 2254),
 ('i', 2179),
 ('over', 2137),
 ('while', 2127),
 ('game', 2077),
 ('only', 2061),
 ('most', 2027),
 ('2', 1982),
 ('three', 1976),
 ('later', 1928),
 ('up', 1919),
 ('about', 1914),
 ('may', 1878),
 ('between', 1871),
 ('him', 1844),
 ('song', 1828),
 ('there', 1801),
 ('some', 1771),
 ('than', 1765),
 ('out', 1760),
 ('no', 1702),
 ('season', 1692),
 ('year', 1669),
 ('made', 1642),
 ('city', 1609),
 ('3', 1601),
 ('such', 1584),
 ('before', 1557),
 ('where', 1524),
 ('used', 1508),
 ('series', 1498),
 ('them', 1488),
 ('second', 1469),
 ('world', 1462),
 ('being', 1454),
 ('years', 1451),
 ('both', 1448),
 ('many', 1447),
 ('000', 1447),
 ('these', 1445),
 ('film', 1405),
 ('however', 1397),
 ('album', 1383),
 ('south', 1373),
 ('war', 1370),
 ('through', 1364),
 ('5', 1362),
 ('north', 1357),
 ('then', 1329),
 ('part', 1320),
 ('can', 1320),
 ('early', 1300),
 ('several', 1287),
 ('4', 1279),
 ('number', 1278),
 ('state', 1274),
 ('including', 1273),
 ('against', 1272),
 ('well', 1262),
 ('known', 1259),
 ('became', 1255),
 ('four', 1238),
 ('united', 1216),
 ('under', 1198),
 ('although', 1194),
 ('m', 1180),
 ('century', 1178),
 ('day', 1165),
 ('following', 1152),
 ('music', 1134),
 ('us', 1119),
 ('began', 1102),
 ('because', 1090),
 ('so', 1085),
 ('work', 1083),
 ('like', 1075),
 ('end', 1071),
 ('called', 1065),
 ('episode', 1064),
 ('until', 1040),
 ('found', 1039),
 ('said', 1038),
 ('area', 1034),
 ('could', 1033),
 ('states', 1019),
 ('american', 1018),
 ('people', 1015),
 ('6', 1011),
 ('since', 995),
 ('british', 989),
 ('each', 984),
 ('released', 979),
 ('same', 979),
 ('team', 974),
 ('church', 968),
 ('around', 966),
 ('10', 963),
 ('long', 960),
 ('did', 958),
 ('along', 957),
 ('million', 940),
 ('life', 939),
 ('five', 938),
 ('national', 916),
 ('0', 914),
 ('john', 910),
 ('back', 910),
 ('high', 904),
 ('company', 900),
 ('t', 897),
 ('another', 896),
 ('best', 891),
 ('use', 888),
 ('you', 878),
 ('if', 872),
 ('final', 871),
 ('september', 869),
 ('august', 865),
 ('river', 861),
 ('large', 856),
 ('what', 848),
 ('west', 843),
 ('8', 837),
 ('km', 836),
 ('off', 835),
 ('down', 831),
 ('7', 825),
 ('due', 819),
 ('games', 818),
 ('june', 810),
 ('history', 809),
 ('line', 809),
 ('will', 803),
 ('name', 799),
 ('now', 791),
 ('any', 786),
 ('storm', 786),
 ('received', 785),
 ('home', 784),
 ('9', 777),
 ('government', 774),
 ('described', 774),
 ('six', 773),
 ('within', 770),
 ('species', 770),
 ('much', 769),
 ('group', 764),
 ('family', 758),
 ('october', 757),
 ('played', 755),
 ('east', 748),
 ('league', 747),
 ('general', 744),
 ('took', 744),
 ('set', 744),
 ('major', 737),
 ('road', 737),
 ('late', 735),
 ('july', 735),
 ('wrote', 733),
 ('single', 731),
 ('won', 731),
 ('system', 729),
 ('play', 726),
 ('video', 725),
 ('times', 724),
 ('according', 723),
 ('record', 718),
 ('third', 716),
 ('based', 713),
 ('april', 711),
 ('man', 707),
 ('included', 702),
 ('just', 700),
 ('march', 699),
 ('book', 697),
 ('january', 695),
 ('those', 694),
 ('show', 693),
 ('named', 691),
 ('very', 690),
 ('even', 690),
 ('england', 689),
 ('main', 687),
 ('white', 687),
 ('left', 686),
 ('york', 685),
 ('men', 684),
 ('small', 681),
 ('school', 681),
 ('though', 678),
 ('division', 672),
 ('club', 671),
 ('way', 670),
 ('old', 670),
 ('original', 667),
 ('near', 666),
 ('last', 665),
 ('12', 663),
 ('november', 661),
 ('water', 657),
 ('death', 655),
 ('place', 654),
 ('20', 653),
 ('15', 651),
 ('tropical', 650),
 ('december', 647),
 ('built', 646),
 ('own', 644),
 ('we', 642),
 ('character', 641),
 ('songs', 639),
 ('top', 633),
 ('de', 632),
 ('form', 631),
 ('30', 631),
 ('player', 630),
 ('do', 626),
 ('black', 625),
 ('king', 625),
 ('public', 623),
 ('island', 619),
 ('german', 619),
 ('next', 617),
 ('2009', 616),
 ('make', 613),
 ('still', 612),
 ('2008', 612),
 ('2010', 611),
 ('role', 598),
 ('led', 598),
 ('again', 595),
 ('ii', 590),
 ('moved', 590),
 ('career', 589),
 ('university', 589),
 ('without', 587),
 ('love', 585),
 ('often', 584),
 ('among', 582),
 ('recorded', 581),
 ('further', 578),
 ('hurricane', 578),
 ('military', 576),
 ('period', 575),
 ('star', 575),
 ('local', 574),
 ('considered', 573),
 ('army', 570),
 ('production', 570),
 ('release', 569),
 ('side', 568),
 ('2007', 567),
 ('great', 565),
 ('house', 557),
 ('came', 556),
 ('published', 554),
 ('written', 552),
 ('100', 551),
 ('continued', 550),
 ('power', 549),
 ('english', 548),
 ('town', 547),
 ('story', 545),
 ('forces', 542),
 ('days', 542),
 ('run', 542),
 ('route', 540),
 ('held', 540),
 ('french', 539),
 ('support', 535),
 ('14', 535),
 ('16', 532),
 ('11', 530),
 ('18', 528),
 ('force', 527),
 ('half', 527),
 ('take', 526),
 ('few', 526),
 ('international', 525),
 ('having', 524),
 ('25', 524),
 ('county', 523),
 ('land', 522),
 ('throughout', 521),
 ('2011', 519),
 ('point', 518),
 ('become', 516),
 ('order', 515),
 ('children', 515),
 ('2006', 515),
 ('light', 513),
 ('version', 513),
 ('title', 511),
 ('former', 509),
 ('lost', 507),
 ('track', 507),
 ('different', 506),
 ('development', 505),
 ('field', 504),
 ('ship', 503),
 ('similar', 502),
 ('despite', 499),
 ('live', 498),
 ('common', 497),
 ('members', 496),
 ('park', 494),
 ('february', 492),
 ('13', 491),
 ('gave', 488),
 ('produced', 488),
 ('short', 487),
 ('southern', 487),
 ('little', 485),
 ('dylan', 485),
 ('site', 482),
 ('once', 480),
 ('2012', 480),
 ('television', 480),
 ('writing', 480),
 ('given', 479),
 ('central', 478),
 ('control', 476),
 ('total', 476),
 ('country', 475),
 ('band', 475),
 ('service', 472),
 ('northern', 471),
 ('re', 469),
 ('include', 466),
 ('young', 464),
 ('position', 464),
 ('fire', 463),
 ('battalion', 460),
 ('making', 457),
 ('never', 457),
 ('seven', 456),
 ('away', 456),
 ('tour', 455),
 ('lead', 454),
 ('air', 454),
 ('age', 454),
 ('2013', 452),
 ('how', 451),
 ('reported', 451),
 ('open', 450),
 ('seen', 450),
 ('battle', 449),
 ('highway', 449),
 ('western', 448),
 ('good', 448),
 ('eastern', 448),
 ('st', 447),
 ('stated', 446),
 ('attack', 445),
 ('red', 445),
 ('god', 445),
 ('match', 444),
 ('returned', 443),
 ('across', 443),
 ('body', 442),
 ('instead', 442),
 ('ships', 441),
 ('established', 440),
 ('using', 438),
 ('ft', 438),
 ('population', 435),
 ('modern', 434),
 ('construction', 434),
 ('week', 434),
 ('america', 434),
 ('noted', 432),
 ('my', 431),
 ('less', 431),
 ('royal', 431),
 ('head', 430),
 ('c', 430),
 ('reached', 430),
 ('developed', 429),
 ('building', 429),
 ('eight', 429),
 ('rock', 428),
 ('players', 427),
 ('h', 427),
 ('ireland', 427),
 ('brigade', 426),
 ('president', 424),
 ('result', 424),
 ('thought', 422),
 ('right', 421),
 ('performance', 420),
 ('miles', 418),
 ('london', 418),
 ('himself', 417),
 ('father', 416),
 ('per', 415),
 ('important', 415),
 ('style', 413),
 ('performed', 412),
 ('felt', 411),
 ('various', 410),
 ('australia', 410),
 ('full', 409),
 ('17', 409),
 ('feet', 408),
 ('areas', 408),
 ('previous', 407),
 ('win', 407),
 ('low', 406),
 ('events', 406),
 ('b', 405),
 ('died', 401),
 ('kingdom', 401),
 ('guitar', 400),
 ('football', 399),
 ('too', 398),
 ('art', 398),
 ('others', 398),
 ('went', 398),
 ('originally', 397),
 ('project', 397),
 ('mm', 397),
 ('human', 396),
 ('upon', 395),
 ('23', 395),
 ('level', 395),
 ('works', 394),
 ('range', 394),
 ('started', 392),
 ('formed', 391),
 ('characters', 390),
 ('james', 390),
 ('political', 390),
 ('women', 388),
 ('should', 387),
 ('cup', 386),
 ('50', 384),
 ('port', 384),
 ('caused', 383),
 ('eventually', 382),
 ('located', 382),
 ('21', 382),
 ('19', 380),
 ('28', 379),
 ('created', 378),
 ('24', 378),
 ('stars', 378),
 ('critics', 377),
 ('sent', 377),
 ('me', 377),
 ('ground', 377),
 ('able', 376),
 ('2004', 374),
 ('class', 373),
 ('2005', 373),
 ('chart', 371),
 ('night', 371),
 ('born', 369),
 ('region', 369),
 ('street', 367),
 ('together', 366),
 ('design', 366),
 ('center', 366),
 ('court', 366),
 ('director', 365),
 ('present', 364),
 ('popular', 364),
 ('strong', 364),
 ('every', 363),
 ('award', 363),
 ('return', 361),
 ('son', 361),
 ('remained', 360),
 ('hero', 360),
 ('see', 359),
 ('novel', 358),
 ('completed', 358),
 ('guns', 358),
 ('scored', 357),
 ('announced', 355),
 ('australian', 355),
 ('grand', 354),
 ('almost', 353),
 ('fourth', 353),
 ('22', 353),
 ('behind', 351),
 ('least', 350),
 ('damage', 350),
 ('26', 349),
 ('added', 348),
 ('brown', 348),
 ('ten', 348),
 ('party', 348),
 ('heavy', 347),
 ('killed', 345),
 ('months', 345),
 ('followed', 345),
 ('wife', 344),
 ('appeared', 344),
 ('addition', 343),
 ('d', 343),
 ('playing', 342),
 ('does', 342),
 ('success', 342),
 ('list', 340),
 ('awards', 340),
 ('features', 338),
 ('aircraft', 335),
 ('coast', 334),
 ('sea', 334),
 ('2003', 334),
 ('taken', 333),
 ('david', 331),
 ('2015', 331),
 ('leading', 330),
 ('action', 329),
 ('championship', 329),
 ('europe', 328),
 ('france', 328),
 ('either', 327),
 ('served', 327),
 ('front', 327),
 ('recording', 327),
 ('towards', 326),
 ('operations', 325),
 ('campaign', 325),
 ('gold', 324),
 ('mother', 323),
 ('put', 323),
 ('elements', 322),
 ('decided', 322),
 ('records', 320),
 ('close', 320),
 ('generally', 319),
 ('magazine', 319),
 ('believed', 319),
 ('fleet', 319),
 ('move', 316),
 ('female', 316),
 ('post', 316),
 ('ever', 316),
 ('carey', 316),
 ('sold', 315),
 ('soon', 315),
 ('example', 315),
 ('poem', 315),
 ('goal', 314),
 ('points', 313),
 ('infantry', 313),
 ('significant', 313),
 ('fort', 312),
 ('weeks', 312),
 ('rather', 311),
 ('study', 311),
 ('european', 310),
 ('outside', 309),
 ('federer', 309),
 ('robert', 308),
 ('opened', 308),
 ('help', 307),
 ('finished', 307),
 ('directed', 307),
 ('brought', 307),
 ('case', 307),
 ('non', 306),
 ('william', 306),
 ('law', 306),
 ('go', 305),
 ('wales', 305),
 ('27', 304),
 ('earlier', 304),
 ('o', 304),
 ('featured', 303),
 ('victory', 302),
 ('get', 302),
 ('act', 300),
 ('successful', 300),
 ('manager', 300),
 ('gun', 299),
 ('start', 298),
 ('mid', 298),
 ('stage', 298),
 ('member', 297),
 ('provided', 297),
 ('association', 297),
 ('opening', 296),
 ('working', 296),
 ('appearance', 296),
 ('village', 296),
 ('mi', 296),
 ('wanted', 295),
 ('council', 295),
 ('particularly', 293),
 ('roman', 293),
 ('jordan', 293),
 ('troops', 292),
 ('2014', 291),
 ('initially', 291),
 ('depression', 291),
 ('atlantic', 291),
 ('40', 291),
 ('tech', 291),
 ('evidence', 290),
 ('29', 290),
 ('yard', 290),
 ('far', 290),
 ('office', 289),
 ('largest', 289),
 ('find', 289),
 ('dam', 289),
 ('review', 288),
 ('blue', 288),
 ('george', 288),
 ('attempt', 288),
 ('saw', 288),
 ('possible', 287),
 ('19th', 287),
 ('special', 286),
 ('type', 286),
 ('summer', 285),
 ('month', 285),
 ('above', 284),
 ('union', 284),
 ('yards', 284),
 ('rest', 283),
 ('florida', 283),
 ('allowed', 282),
 ('saying', 282),
 ('event', 281),
 ('winds', 281),
 ('race', 281),
 ('hours', 280),
 ('critical', 280),
 ('creek', 279),
 ('cross', 279),
 ('whom', 278),
 ('nine', 277),
 ('worked', 276),
 ('2001', 276),
 ('police', 276),
 ('society', 276),
 ('plan', 276),
 ('missouri', 276),
 ('designed', 275),
 ('reception', 275),
 ('500', 275),
 ('middle', 275),
 ('community', 275),
 ('previously', 274),
 ('free', 273),
 ('process', 273),
 ('forced', 272),
 ('real', 271),
 ('praised', 271),
 ('remains', 271),
 ('operation', 271),
 ('era', 271),
 ('radio', 271),
 ('200', 270),
 ('official', 269),
 ('research', 269),
 ('increased', 269),
 ('hall', 268),
 ('lower', 267),
 ('station', 267),
 ('parliament', 267),
 ('come', 266),
 ('michael', 266),
 ('relationship', 266),
 ('units', 265),
 ('command', 265),
 ('commander', 265),
 ('regiment', 265),
 ('studio', 265),
 ('hill', 265),
 ('taking', 264),
 ('base', 264),
 ('replaced', 264),
 ('parts', 263),
 ('writer', 263),
 ('navy', 262),
 ('ball', 262),
 ('industry', 262),
 ('social', 261),
 ('food', 260),
 ('highest', 259),
 ('college', 259),
 ('co', 259),
 ('bay', 259),
 ('reviews', 259),
 ('media', 259),
 ('beginning', 258),
 ('claimed', 258),
 ('estimated', 258),
 ('don', 258),
 ('museum', 257),
 ('mph', 257),
 ('canada', 257),
 ('section', 257),
 ('goals', 256),
 ('stone', 256),
 ('joined', 256),
 ('japanese', 255),
 ('placed', 255),
 ('religious', 255),
 ('commercial', 255),
 ('average', 255),
 ('stories', 254),
 ('involved', 254),
 ('signed', 254),
 ('training', 254),
 ('suggested', 254),
 ('oldham', 254),
 ('met', 253),
 ('shot', 253),
 ('introduced', 253),
 ('lines', 252),
 ('sometimes', 252),
 ('itself', 252),
 ('face', 251),
 ('31', 251),
 ('today', 251),
 ('background', 251),
 ('paul', 251),
 ('business', 251),
 ('olivier', 251),
 ('scene', 250),
 ('henry', 250),
 ('going', 250),
 ('complete', 250),
 ('structure', 249),
 ('mexico', 249),
 ('additional', 248),
 ('available', 248),
 ('thus', 248),
 ('give', 248),
 ('cast', 247),
 ('term', 247),
 ('loss', 247),
 ('language', 247),
 ('date', 247),
 ('horse', 247),
 ('whose', 246),
 ('nearly', 246),
 ('sound', 246),
 ('india', 246),
 ('thomas', 245),
 ('fifth', 245),
 ('past', 245),
 ('approximately', 244),
 ('indian', 244),
 ('shows', 244),
 ('must', 244),
 ('program', 244),
 ('irish', 244),
 ('told', 243),
 ('entire', 243),
 ('capital', 243),
 ('damaged', 243),
 ('already', 243),
 ('friends', 243),
 ('appointed', 243),
 ('native', 242),
 ('prior', 242),
 ('sun', 242),
 ('britain', 242),
 ('names', 241),
 ('issue', 241),
 ('probably', 241),
 ('turned', 241),
 ('mark', 241),
 ('forest', 241),
 ('male', 240),
 ('r', 240),
 ('winning', 239),
 ('passed', 239),
 ('change', 238),
 ('students', 238),
 ('our', 238),
 ('length', 237),
 ('size', 237),
 ('earth', 237),
 ('civil', 236),
 ('child', 236),
 ('especially', 236),
 ('woman', 235),
 ('christian', 235),
 ('enough', 235),
 ('notes', 235),
 ('overall', 234),
 ('chinese', 234),
 ('forward', 234),
 ('failed', 234),
 ('better', 234),
 ('running', 234),
 ('mixed', 234),
 ('captain', 233),
 ('ny', 233),
 ('limited', 232),
 ('2002', 232),
 ('iii', 231),
 ('minor', 231),
 ('ordered', 231),
 ('network', 231),
 ('l', 231),
 ('future', 231),
 ('wheeler', 231),
 ('regular', 230),
 ('remaining', 230),
 ('pacific', 230),
 ('spent', 230),
 ('changes', 230),
 ('moving', 230),
 ('might', 230),
 ('arrived', 230),
 ('includes', 230),
 ('larger', 229),
 ('education', 229),
 ('usually', 229),
 ('san', 229),
 ('canadian', 229),
 ('cathedral', 229),
 ('hand', 228),
 ('department', 228),
 ('hit', 228),
 ('birds', 228),
 ('required', 227),
 ('uk', 227),
 ('latter', 227),
 ('lake', 227),
 ('decision', 226),
 ('plot', 225),
 ('response', 225),
 ('africa', 225),
 ('wide', 225),
 ('debut', 225),
 ('space', 224),
 ('musical', 224),
 ('round', 224),
 ('voice', 224),
 ('2000', 224),
 ('crew', 223),
 ('appear', 223),
 ('related', 222),
 ('mounted', 222),
 ('groups', 222),
 ('territory', 221),
 ('view', 221),
 ('centre', 221),
 ('films', 221),
 ('supported', 221),
 ('saint', 221),
 ('rachel', 221),
 ('jin', 221),
 ('positive', 220),
 ('feature', 220),
 ('science', 220),
 ('nature', 220),
 ('billboard', 220),
 ('pressure', 220),
 ('ended', 220),
 ('00', 220),
 ('finally', 219),
 ('score', 219),
 ('squadron', 219),
 ('flight', 219),
 ('culture', 219),
 ('money', 218),
 ('becoming', 218),
 ('always', 217),
 ('particular', 217),
 ('smaller', 217),
 ('books', 217),
 ('provide', 217),
 ('shown', 217),
 ('charles', 217),
 ('private', 216),
 ('1995', 216),
 ('anti', 216),
 ('discovered', 216),
 ('shortly', 215),
 ('board', 215),
 ('minutes', 215),
 ('difficult', 215),
 ('person', 214),
 ('experience', 214),
 ('nations', 214),
 ('peter', 214),
 ('trade', 214),
 ('defeated', 214),
 ('mass', 214),
 ('temple', 214),
 ('staff', 213),
 ('big', 213),
 ('surface', 213),
 ('subsequently', 213),
 ('lack', 213),
 ('japan', 212),
 ('upper', 212),
 ('press', 212),
 ('living', 212),
 ('effects', 212),
 ('zealand', 212),
 ('fact', 211),
 ('word', 211),
 ('professional', 211),
 ('material', 210),
 ('greater', 210),
 ('mostly', 209),
 ('idea', 209),
 ('problems', 209),
 ('self', 209),
 ('intended', 209),
 ('room', 209),
 ('teams', 209),
 ('bridge', 209),
 ('numbers', 208),
 ('collection', 208),
 ('captured', 208),
 ('largely', 208),
 ('cut', 208),
 ('interest', 208),
 ('key', 208),
 ('gods', 208),
 ('referred', 207),
 ('themselves', 207),
 ('cover', 207),
 ('cm', 207),
 ('likely', 207),
 ('metres', 207),
 ('primary', 207),
 ('tv', 207),
 ('haifa', 207),
 ('mountain', 206),
 ('got', 206),
 ('2016', 206),
 ('60', 206),
 ('machine', 206),
 ('scientology', 206),
 ('news', 205),
 ('yet', 205),
 ('traffic', 205),
 ('xenon', 205),
 ('location', 204),
 ('tower', 204),
 ('entered', 204),
 ('contract', 204),
 ...]</code></pre>
</div>
</div>
<p>The above approach for making vocabulary has following limitations: 1. size of vocabulary is very large. This means we need output layer with more number of neurons -&gt; more computation 2. Some words are less frequent occuring maybe only once or twice in the entire dataset. I found that in model training, when our dataset is small and we have large vocab and many words appear less frequently, it does not result in good model training. 3. Current vocabulary cannot handle out-of-vocabulary words that could appear in validation/ test settings. 4. Many stopwords like <code>the</code>, <code>a</code> are overrepresented in the dataset.</p>
<p>In order to solve 1 and 2, we filter out low frequency words by defining a hyperparameter <code>MIN_FREQ</code></p>
<p>(For 1 and 2, we can also use Heirarchical Softmax instead of just Softmax classifier.)</p>
<p>For 3, we assign token <code>"&lt;unk&gt;"</code> to new tokens (or words) and to less frequent ones which we filtered out using <code>MIN_FREQ</code></p>
<p>For 4, we can use <code>remove_stopwords = False</code> while tokenizing the text. But when I removed stopwords, I witnessed decline in performance. Perhaps because stopwords are high in frequency and are essential to understand context in order to learn the current word emebeddings. Hence, I am keeping the stopwords.</p>
<div id="bbe30c50" class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># filter low frequency words to create vocabulary (from train text)</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>MIN_FREQ <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>train_word_counts <span class="op">=</span> Counter(train_words)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>high_freq_words <span class="op">=</span> [word <span class="cf">for</span> word <span class="kw">in</span> train_words <span class="cf">if</span> train_word_counts[word] <span class="op">&gt;=</span> MIN_FREQ]</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="co"># create a vocabulary</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> <span class="bu">list</span>(<span class="bu">set</span>(high_freq_words))</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>vocab.append(<span class="st">"&lt;unk&gt;"</span>)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>V <span class="op">=</span> <span class="bu">len</span>(vocab)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Vocabulary size, V:"</span>, V)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Vocabulary size, V: 4089</code></pre>
</div>
</div>
<p>This has brought down vocab from over 66k to just about 4k.</p>
<section id="cbow" class="level3">
<h3 class="anchored" data-anchor-id="cbow">CBOW</h3>
<p>Model architecture as proposed</p>
<p><img src="media\word2vec/cbow_skip_gram.png" width="650"></p>
<p><strong>CBOW:</strong> Predict the middle word based on R previous words and R future words</p>
<p><strong>Skip-gram:</strong> Predict context words based on middle word</p>
<p><img src="media\word2vec\cbow1.png" width="650"></p>
<p>CBOW architecture has no hidden layer.</p>
<p>It is a feedforward NN with following layers: Input → Embedding → Output</p>
<ul>
<li>Input layer: one-hot encoded context words</li>
<li>Embedding layer: V x D matrix</li>
<li>Output layer: V x 1 vector</li>
</ul>
<p>“…where the non-linear hidden layer is removed…” -&gt; no hidden layer</p>
<p>“…and the projection layer is shared for all words (not just the projection matrix); thus, all words get projected into the same position (their vectors are averaged)” -&gt; <code>self.embeddings(context_indices).mean(dim=1)</code></p>
</section>
<section id="why-was-the-non-linear-hidden-layer-removed" class="level3">
<h3 class="anchored" data-anchor-id="why-was-the-non-linear-hidden-layer-removed">Why was the non-linear hidden layer removed?</h3>
<p>Ans: To make training faster. In other words, make model simpler. This was the main argument in the paper.</p>
<div id="a68d3499" class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CBOWModel(nn.Module):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, V, D):</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co">        V: vocabulary size</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co">        D: embedding size</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(CBOWModel, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.projection <span class="op">=</span> nn.Embedding(V, D)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output <span class="op">=</span> nn.Linear(D, V)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.init_weights()</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inputs):</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="co">        inputs: context indices of shape [batch_size, 2R]</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>        embeddings <span class="op">=</span> <span class="va">self</span>.projection(inputs).mean(dim<span class="op">=</span><span class="dv">1</span>) <span class="co"># Average across context words shape: [batch_size, D]</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> <span class="va">self</span>.output(embeddings)</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logits</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># optional: weights initialization</span></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> init_weights(<span class="va">self</span>):</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># xavier uniform initialization for embeddings and linear weights</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>        nn.init.xavier_uniform_(<span class="va">self</span>.projection.weight)</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>        nn.init.xavier_uniform_(<span class="va">self</span>.output.weight)</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># zero bias initialization (optional but recommended)- valid for linear layer not for embedding layer</span></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.output.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>            nn.init.zeros_(<span class="va">self</span>.output.bias)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As per original paper (refer above image), authors: - used 4 history and 4 future word to predict current word. (R=4)</p>
<div id="68e3eea1" class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>R <span class="op">=</span> <span class="dv">4</span> <span class="co"># context window size</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co"># create word to index and index to word mappings</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>word_to_idx <span class="op">=</span> {word: idx <span class="cf">for</span> idx, word <span class="kw">in</span> <span class="bu">enumerate</span>(vocab)}</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>idx_to_word <span class="op">=</span> {idx: word <span class="cf">for</span> word, idx <span class="kw">in</span> word_to_idx.items()}</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_context_target(words, R, word_to_idx):</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    contexts <span class="op">=</span> []</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    targets <span class="op">=</span> []</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(R, <span class="bu">len</span>(words) <span class="op">-</span> R):</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>        context_words <span class="op">=</span> words[i<span class="op">-</span>R:i] <span class="op">+</span> words[i<span class="op">+</span><span class="dv">1</span>:i<span class="op">+</span>R<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>        context_words <span class="op">=</span> torch.tensor([word_to_idx[w] <span class="cf">if</span> w <span class="kw">in</span> word_to_idx <span class="cf">else</span> word_to_idx[<span class="st">"&lt;unk&gt;"</span>] <span class="cf">for</span> w <span class="kw">in</span> context_words], dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>        target_word <span class="op">=</span> words[i]</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>        target_word <span class="op">=</span> torch.tensor(word_to_idx[target_word] <span class="cf">if</span> target_word <span class="kw">in</span> word_to_idx <span class="cf">else</span> word_to_idx[<span class="st">"&lt;unk&gt;"</span>], dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>        contexts.append(context_words)</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>        targets.append(target_word)</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> torch.stack(contexts)</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> torch.stack(targets)</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X, y</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>X_train, y_train <span class="op">=</span> get_context_target(train_words, R, word_to_idx)</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>X_val, y_val <span class="op">=</span> get_context_target(val_words, R, word_to_idx)</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Train data:"</span>)</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(X_train.shape, y_train.shape)</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Validation data:"</span>)</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(X_val.shape, y_val.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Train data:
torch.Size([1749361, 8]) torch.Size([1749361])
Validation data:
torch.Size([183475, 8]) torch.Size([183475])</code></pre>
</div>
</div>
<div id="9fffefe7" class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"X_train sample:"</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(X_train[:<span class="dv">2</span>,:])</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"y_train sample:"</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(y_train[:<span class="dv">2</span>])</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"--------------------------------"</span>)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"X_val sample:"</span>)</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(X_val[:<span class="dv">2</span>,:])</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"y_val sample:"</span>)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(y_val[:<span class="dv">2</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>X_train sample:
tensor([[ 581, 3372, 3297, 4088,  581,  274, 4088, 3372],
        [3372, 3297, 4088, 1405,  274, 4088, 3372, 1964]])
y_train sample:
tensor([1405,  581])
--------------------------------
X_val sample:
tensor([[4088, 4088, 4088, 4088,  240, 2041, 3147, 4088],
        [4088, 4088, 4088, 3169, 2041, 3147, 4088, 2358]])
y_val sample:
tensor([3169,  240])</code></pre>
</div>
</div>
</section>
<section id="how-cbow-is-different-from-stansard-bag-of-words-or-what-does-as-unlike-standard-bag-of-words-model-it-uses-continuous-distributed-representation-of-the-context-means" class="level3">
<h3 class="anchored" data-anchor-id="how-cbow-is-different-from-stansard-bag-of-words-or-what-does-as-unlike-standard-bag-of-words-model-it-uses-continuous-distributed-representation-of-the-context-means">How CBOW is different from stansard Bag-of-Words? (Or, What does “as unlike standard bag-of-words model, it uses continuous distributed representation of the context” means?)</h3>
<p>Ans:</p>
<p><strong>Standard Bag-of-Words:</strong> - Each word is represented as sparse vector containing 1’s at index corresponding to the context words and 0’s elsewhere. - Here, size of each word embedding vector = [1, V] where <code>V</code> is vocab size</p>
<p><strong>Continuous Bag-of-Words</strong> - Each word is represented via a projection matrix in which values are continuos and hence the vector does not necessarily consist of just 1’s and 0’s - Here, size of each word embedding vector = [1, D] where <code>D</code> is <code>embedding_dimension</code> a user chooses</p>
<div id="9b31a836" class="cell" data-execution_count="67">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> <span class="dv">300</span> <span class="co"># embedding size</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Context window size, 2R:'</span>, <span class="dv">2</span><span class="op">*</span>R)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Embedding size, D:'</span>, D)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Vocabulary size, V:'</span>, V)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="co"># create the model</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> CBOWModel(V, D).to(device)</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="co"># count the number of parameters</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>trainable_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters() <span class="cf">if</span> p.requires_grad)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Number of trainable parameters (Millions):'</span>, trainable_params<span class="op">/</span><span class="fl">1e6</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Context window size, 2R: 8
Embedding size, D: 300
Vocabulary size, V: 4089
Number of trainable parameters (Millions): 2.457489</code></pre>
</div>
</div>
<div id="6596d285" class="cell" data-execution_count="68">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CBOWDataset(Dataset):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, contexts, targets):</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.contexts <span class="op">=</span> contexts.to(device)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.targets <span class="op">=</span> targets.to(device)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.contexts)</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.contexts[idx], <span class="va">self</span>.targets[idx]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="bded79e9" class="cell" data-execution_count="69">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create the dataset</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>train_ds <span class="op">=</span> CBOWDataset(X_train, y_train)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>val_ds <span class="op">=</span> CBOWDataset(X_val, y_val)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="co"># create the dataloader</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>train_dl <span class="op">=</span> DataLoader(train_ds, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>val_dl <span class="op">=</span> DataLoader(val_ds, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Model training settings</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a><span class="co"># LambdaLR scheduler (start with lr=0.025 and linearly decay to 0.00005 (nearly 0) over user-defined epochs)</span></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>initial_lr <span class="op">=</span> <span class="fl">0.025</span></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>lr_lambda <span class="op">=</span> <span class="kw">lambda</span> epoch: <span class="dv">1</span> <span class="op">-</span> epoch <span class="op">/</span> epochs</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr<span class="op">=</span>initial_lr)</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> LambdaLR(optimizer, lr_lambda<span class="op">=</span>lr_lambda)</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>    total_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch_idx, (context_idx, target_idx) <span class="kw">in</span> <span class="bu">enumerate</span>(train_dl):</span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>        context_idx <span class="op">=</span> context_idx.to(device)</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>        target_idx <span class="op">=</span> target_idx.to(device)</span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> model(context_idx)</span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(logits, target_idx)</span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a>        total_loss <span class="op">+=</span> loss.item()</span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a>    current_lr <span class="op">=</span> optimizer.param_groups[<span class="dv">0</span>][<span class="st">'lr'</span>]</span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, Loss: </span><span class="sc">{</span>total_loss<span class="op">/</span><span class="bu">len</span>(train_dl)<span class="sc">:.4f}</span><span class="ss">, LR: </span><span class="sc">{</span>current_lr<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a>    scheduler.step()</span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb22-38"><a href="#cb22-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb22-39"><a href="#cb22-39" aria-hidden="true" tabindex="-1"></a>        total_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb22-40"><a href="#cb22-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> batch_idx, (context_idx, target_idx) <span class="kw">in</span> <span class="bu">enumerate</span>(val_dl):</span>
<span id="cb22-41"><a href="#cb22-41" aria-hidden="true" tabindex="-1"></a>            context_idx <span class="op">=</span> context_idx.to(device)</span>
<span id="cb22-42"><a href="#cb22-42" aria-hidden="true" tabindex="-1"></a>            target_idx <span class="op">=</span> target_idx.to(device)</span>
<span id="cb22-43"><a href="#cb22-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-44"><a href="#cb22-44" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> model(context_idx)</span>
<span id="cb22-45"><a href="#cb22-45" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> criterion(logits, target_idx)</span>
<span id="cb22-46"><a href="#cb22-46" aria-hidden="true" tabindex="-1"></a>            total_loss <span class="op">+=</span> loss.item()</span>
<span id="cb22-47"><a href="#cb22-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-48"><a href="#cb22-48" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Validation Loss: </span><span class="sc">{</span>total_loss<span class="op">/</span><span class="bu">len</span>(val_dl)<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1, Loss: 6.0743, LR: 0.0250
Validation Loss: 6.0751
Epoch 2, Loss: 5.9600, LR: 0.0238
Validation Loss: 5.9654
Epoch 3, Loss: 5.8597, LR: 0.0225
Validation Loss: 5.9073
Epoch 4, Loss: 5.7652, LR: 0.0213
Validation Loss: 5.8122
Epoch 5, Loss: 5.6731, LR: 0.0200
Validation Loss: 5.7198
Epoch 6, Loss: 5.5820, LR: 0.0188
Validation Loss: 5.6467
Epoch 7, Loss: 5.4927, LR: 0.0175
Validation Loss: 5.5951
Epoch 8, Loss: 5.4013, LR: 0.0163
Validation Loss: 5.5274
Epoch 9, Loss: 5.3099, LR: 0.0150
Validation Loss: 5.4637
Epoch 10, Loss: 5.2136, LR: 0.0138
Validation Loss: 5.4086
Epoch 11, Loss: 5.1117, LR: 0.0125
Validation Loss: 5.3469
Epoch 12, Loss: 5.0028, LR: 0.0112
Validation Loss: 5.3002
Epoch 13, Loss: 4.8867, LR: 0.0100
Validation Loss: 5.2706
Epoch 14, Loss: 4.7610, LR: 0.0087
Validation Loss: 5.2455
Epoch 15, Loss: 4.6259, LR: 0.0075
Validation Loss: 5.2198
Epoch 16, Loss: 4.4806, LR: 0.0063
Validation Loss: 5.2170
Epoch 17, Loss: 4.3247, LR: 0.0050
Validation Loss: 5.2252
Epoch 18, Loss: 4.1590, LR: 0.0038
Validation Loss: 5.2454
Epoch 19, Loss: 3.9821, LR: 0.0025
Validation Loss: 5.2643
Epoch 20, Loss: 3.7955, LR: 0.0013
Validation Loss: 5.2792</code></pre>
</div>
</div>
<p>Above, we used dynamic learning rate by using <code>lr_scheduler</code> from Pytorch. We reduced learning rate linearly because because original paper did this.</p>
<p><img src="media/word2vec/cbow_training2.png" width="650"></p>
<div id="17bd2224" class="cell" data-execution_count="70">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># embedding from first model layer</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>embeddings <span class="op">=</span> <span class="bu">list</span>(model.parameters())[<span class="dv">0</span>]</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>embeddings <span class="op">=</span> embeddings.cpu().detach().numpy()</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="co"># normalization</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>norms <span class="op">=</span> (embeddings <span class="op">**</span> <span class="dv">2</span>).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>) <span class="op">**</span> (<span class="dv">1</span> <span class="op">/</span> <span class="dv">2</span>)</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>norms <span class="op">=</span> np.reshape(norms, (<span class="bu">len</span>(norms), <span class="dv">1</span>))</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>embeddings_norm <span class="op">=</span> embeddings <span class="op">/</span> norms</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>embeddings_norm.shape</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_top_similar(word: <span class="bu">str</span>, topN: <span class="bu">int</span> <span class="op">=</span> <span class="dv">10</span>):</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>    word_id <span class="op">=</span> word_to_idx[word]</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>    word_vec <span class="op">=</span> embeddings_norm[word_id]</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>    word_vec <span class="op">=</span> np.reshape(word_vec, (<span class="bu">len</span>(word_vec), <span class="dv">1</span>))</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>    dists <span class="op">=</span> np.matmul(embeddings_norm, word_vec).flatten()</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>    topN_ids <span class="op">=</span> np.argsort(<span class="op">-</span>dists)[<span class="dv">1</span> : topN <span class="op">+</span> <span class="dv">1</span>]</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>    topN_dict <span class="op">=</span> {}</span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> sim_word_id <span class="kw">in</span> topN_ids:</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a>        sim_word <span class="op">=</span> idx_to_word[sim_word_id]</span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>        topN_dict[sim_word] <span class="op">=</span> dists[sim_word_id]</span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> topN_dict</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="lets-see-some-semantic-capabilities-which-our-model-has-learned" class="level4">
<h4 class="anchored" data-anchor-id="lets-see-some-semantic-capabilities-which-our-model-has-learned">Let’s see some <strong>Semantic</strong> capabilities which our model has learned…</h4>
<div id="6f9a4eac" class="cell" data-execution_count="71">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Top 5 similar words to 'man':"</span>)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word, sim <span class="kw">in</span> get_top_similar(<span class="st">"man"</span>, topN<span class="op">=</span><span class="dv">5</span>).items():</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="sc">{}</span><span class="st">: </span><span class="sc">{:.3f}</span><span class="st">"</span>.<span class="bu">format</span>(word, sim))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Top 5 similar words to 'man':
person: 0.287
woman: 0.272
cougar: 0.270
girl: 0.246
pitcher: 0.224</code></pre>
</div>
</div>
</section>
<section id="lets-see-with-syntactic-capabilities-which-our-model-has-learned" class="level4">
<h4 class="anchored" data-anchor-id="lets-see-with-syntactic-capabilities-which-our-model-has-learned">Let’s see with <strong>Syntactic</strong> capabilities which our model has learned</h4>
<p><img src="media/word2vec/cbow_syntactic.png" width="650"></p>
<div id="43534493" class="cell" data-execution_count="76">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>emb1 <span class="op">=</span> embeddings[word_to_idx[<span class="st">"king"</span>]]</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>emb2 <span class="op">=</span> embeddings[word_to_idx[<span class="st">"man"</span>]]</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>emb3 <span class="op">=</span> embeddings[word_to_idx[<span class="st">"woman"</span>]]</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>emb4 <span class="op">=</span> emb1 <span class="op">-</span> emb2 <span class="op">+</span> emb3</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>emb4_norm <span class="op">=</span> (emb4 <span class="op">**</span> <span class="dv">2</span>).<span class="bu">sum</span>() <span class="op">**</span> (<span class="dv">1</span> <span class="op">/</span> <span class="dv">2</span>)</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>emb4 <span class="op">=</span> emb4 <span class="op">/</span> emb4_norm</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>emb4 <span class="op">=</span> np.reshape(emb4, (<span class="bu">len</span>(emb4), <span class="dv">1</span>))</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>dists <span class="op">=</span> np.matmul(embeddings_norm, emb4).flatten()</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>top5 <span class="op">=</span> np.argsort(<span class="op">-</span>dists)[:<span class="dv">15</span>]</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Top 15 similar words to 'king - man + woman':"</span>)</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i,word_id <span class="kw">in</span> <span class="bu">enumerate</span>(top5):</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"</span><span class="sc">{}</span><span class="st">. </span><span class="sc">{}</span><span class="st">: </span><span class="sc">{:.3f}</span><span class="st">"</span>.<span class="bu">format</span>(i<span class="op">+</span><span class="dv">1</span>, idx_to_word[word_id], dists[word_id]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Top 15 similar words to 'king - man + woman':
1. woman: 0.669
2. king: 0.600
3. emperor: 0.256
4. lord: 0.245
5. treaty: 0.234
6. church: 0.224
7. archbishop: 0.224
8. monarch: 0.215
9. scene: 0.212
10. royal: 0.211
11. regime: 0.207
12. anglo: 0.202
13. 10th: 0.200
14. surviving: 0.200
15. queen: 0.198</code></pre>
</div>
</div>
<p>We see <strong>queen</strong> in top 15 words. To get better results: 1. train with larger sized datasets 2. Increase <code>D</code> 3. Train longer</p>
<p><strong>Why am I saying this?</strong></p>
<p>As per original paper trained the CBOW model on a large dataset of <strong>6 Billion</strong> words. Even then their performance were not super-good until they started to increase the <code>D</code>.</p>
<p><img src="media/word2vec/cbow_training1.png" width="650"></p>
<p>Their results:</p>
<p><img src="media/word2vec/cbow_result.png" width="650"></p>
<p>Further improvements: 1. Implement Heirarchical Softmax to speed up the computation 2. Implement Negative sampling</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/mgupta70\.github\.io");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>